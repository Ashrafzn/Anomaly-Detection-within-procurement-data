{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9072b868-3a50-4894-a943-c0546d0df8e6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Robust exponential smoothing for anomaly detection in time series in the procurement context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c63e4493-a2e4-4ac0-988e-d6fbe012be30",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Data simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2f35312-e9b8-45f0-b992-a1e0994d5294",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_simulation_structure(Commodities_L1_number = 1,Departments_L1_number =1,Account_level_number=3,Suppliers_number = 4):\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from datetime import datetime, timedelta\n",
    "    import math\n",
    "    import random\n",
    "\n",
    "    if Commodities_L1_number <1 or Commodities_L1_number >7:\n",
    "        raise ValueError('Commodities_L1_number must be between 1 and 7')\n",
    "    if Departments_L1_number <1 or Departments_L1_number >7:\n",
    "        raise ValueError('Departments_L1_number must be between 1 and 7')\n",
    "    if Account_level_number <1 or Account_level_number >6:\n",
    "        raise ValueError('Account_level_number must be between 1 and 6')\n",
    "    if Suppliers_number <1 or Suppliers_number >15:\n",
    "        raise ValueError('Suppliers_number must be between 1 and 15')\n",
    "\n",
    "    ###########################################\n",
    "    #... Laying out the structure of data .....\n",
    "    ###########################################\n",
    "\n",
    "    # The columns of the desired dataframe.\n",
    "    columns = ['InvoiceID','Supplier Name','Date','Date(Quarter)',\n",
    "                'Date(Year)','Invoice Spend','Commodity (enriched) - Custom Category (L1)',\n",
    "                'Commodity (enriched) - Custom Category (L2)',\n",
    "                'Commodity (enriched) - Custom Category (L3)','Account',\n",
    "                'Department (D1)','Department (D2)','Department (D3)']\n",
    "\n",
    "    # Defining the divisions between the subsequent levels of the categories of ordered materials.\n",
    "    L1_log = ['L1_1','L1_2','L1_3','L1_4','L1_5','L1_6','L1_7']\n",
    "    L1_log = L1_log[:Commodities_L1_number]\n",
    "    L2_log = {item : [item+'_L2_1',item+'_L2_2',item+'_L2_3',item+'_L2_4',item+'_L2_5'] for item in L1_log}\n",
    "    L3_log = {}\n",
    "    for items in L2_log.values():\n",
    "        for item in items:\n",
    "            L3_log[item] = [item+'_L3_1',item+'_L3_2',item+'_L3_3',item+'_L3_4']\n",
    "\n",
    "    # Linking each final product to at most two suppliers.\n",
    "    suppliers = ['Supp1','Supp2','Supp3','Supp4','Supp5','Supp6','Supp7','Supp8','Supp9','Supp10','Supp11','Supp12','Supp13','Supp14','Supp15']\n",
    "    suppliers = suppliers[:Suppliers_number]\n",
    "    suppliers_log = {}\n",
    "    for items in L3_log.values():\n",
    "        for item in items:\n",
    "            suppliers_log[item]=[np.random.choice(suppliers),np.random.choice(suppliers)]\n",
    "\n",
    "    # Defining the divisions between grids and departments\n",
    "    Account = ['Budget_iterm_1','Budget_iterm_2','Budget_iterm_3','Budget_iterm_4','Budget_iterm_5','Budget_iterm_6']\n",
    "    Account = Account[:Account_level_number]\n",
    "\n",
    "    # Defining the divisions between the subsequent levels of the departments.\n",
    "    D1_log = ['D1_1','D1_2','D1_3','D1_4','D1_5','D1_6','D1_7']\n",
    "    D1_log = D1_log[:Departments_L1_number]\n",
    "    D2_log = {item : [item+'_D2_1',item+'_D2_2',item+'_D2_3',item+'_D2_4',item+'_D2_5'] for item in D1_log}\n",
    "    D3_log = {}\n",
    "    for items in D2_log.values():\n",
    "        for item in items:\n",
    "            D3_log[item] = [item+'_D3_1',item+'_D3_2',item+'_D3_3',item+'_D3_4']\n",
    "    return {'columns':columns,'L1_log':L1_log,'L2_log':L2_log,'L3_log':L3_log,'suppliers':suppliers,\n",
    "            'Account':Account,'D1_log':D1_log,'D2_log':D2_log,'D3_log':D3_log,'suppliers_log':suppliers_log}\n",
    "\n",
    "\n",
    "    \n",
    "def insert_entries(simulation_structure,seed,start_year = 2013,end_year = 2023,\n",
    "                   mu = [500000,1000000],sdv = [50000,100000],rtrend = [25000,50000]):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from datetime import datetime, timedelta\n",
    "    import math\n",
    "    import random\n",
    "    if start_year > end_year:\n",
    "        raise ValueError('end_year must be after the start year')\n",
    "\n",
    "    if start_year < 1900:\n",
    "        raise ValueError('please give a realistic number')\n",
    "    if end_year > 2025:\n",
    "        raise ValueError('you might be looking too far into the future')\n",
    "\n",
    "    columns = simulation_structure['columns']\n",
    "    L1_log = simulation_structure['L1_log']\n",
    "    L2_log = simulation_structure['L2_log']\n",
    "    L3_log = simulation_structure['L3_log']\n",
    "    suppliers = simulation_structure['suppliers']\n",
    "    Account = simulation_structure['Account']\n",
    "    D1_log = simulation_structure['D1_log']\n",
    "    D2_log = simulation_structure['D2_log']\n",
    "    D3_log = simulation_structure['D3_log']\n",
    "    suppliers_log = simulation_structure['suppliers_log']\n",
    "    # Setting random seeds for reproducibility\n",
    "    \n",
    "\n",
    "    ###################################################################\n",
    "    #.... Inserting the entries to the previously defined struchure ...\n",
    "    ###################################################################\n",
    "\n",
    "    # Calculate the number of days between start_date and end_date\n",
    "    start_date = datetime(start_year, 9, 1)\n",
    "    end_date = datetime(end_year, 12, 31)\n",
    "    delta = (end_date - start_date).days\n",
    "\n",
    "    # Creating an empty dataframe to fill it later with entries\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "    behavior = {}\n",
    "    # Looping through each department and inserting all of its entries at once\n",
    "\n",
    "    # Looping through each department level by level\n",
    "    for D1 in D1_log:\n",
    "        for D2 in D2_log[D1]:\n",
    "            for D3 in D3_log[D2]:\n",
    "                # Each department should have a distinct bahavior (distribution)\n",
    "                mean = np.abs(random.randint(mu[0],mu[1]))\n",
    "                std_dev = np.abs(random.randint(sdv[0],sdv[1]))\n",
    "                samples = np.abs(np.random.normal(mean, std_dev, 3000)) # not all of those will be used        \n",
    "\n",
    "                # Calculate the intervals between the invoices\n",
    "                # Assuming the time to event is exponential with random rate\n",
    "                #   time to event is reasonably between one day to one week\n",
    "                random_rate = np.abs(random.randint(1,7)) # pick a random parameter for this department\n",
    "                random_days = np.abs(np.random.exponential(random_rate,3000)) # use the parameter to draw samples\n",
    "                comulative_sum = np.abs(np.cumsum(random_days))\n",
    "\n",
    "                # Keeping only the dates that fall within the studied period (2013-2023)\n",
    "                selected_dates = [int(date) for date in comulative_sum if date < delta ]\n",
    "                \n",
    "                # Deciding on the number of entries for this department\n",
    "                entries = len(selected_dates)\n",
    "\n",
    "                # Deciding on a trend for this department's time series:\n",
    "\n",
    "                random_trend = random.randint(rtrend[0],rtrend[1])\n",
    "\n",
    "                # Having a list of 12 seasonal effects , one effect for each months.\n",
    "                highest_point_of_season = np.random.uniform(0.1,0.9)\n",
    "                lowest_point_of_season = np.random.uniform(0.1,highest_point_of_season)\n",
    "                first_half = [lowest_point_of_season + (i+1)*((highest_point_of_season - lowest_point_of_season)/6) for i in range(6)]\n",
    "                second_half = [highest_point_of_season - (i+1)*((highest_point_of_season - lowest_point_of_season)/6) for i in range(6)]\n",
    "                seasonal_effects = []\n",
    "                for x in range(12):\n",
    "                    if x in range(0,6):\n",
    "                        seasonal_effects.append(first_half[x])\n",
    "                    if x in range(6,12):\n",
    "                        seasonal_effects.append(second_half[x-6])\n",
    "                seasonal_effects = [round(x*(1/min(seasonal_effects)),4) for x in seasonal_effects]\n",
    "                \n",
    "\n",
    "                # Keeping log of each department's behavior\n",
    "                behavior[D3] = {'mean':mean,'std_dev':std_dev,'random_rate':random_rate,\n",
    "                                'random_trend':random_trend,'seasonal_effects':seasonal_effects}\n",
    "\n",
    "\n",
    "\n",
    "                # Entring one entry in each iteration\n",
    "                for i in range(entries):\n",
    "\n",
    "                    # Picking a L3 commodity\n",
    "                    random_L1 = np.random.choice(L1_log)\n",
    "                    random_L2 = np.random.choice(L2_log[random_L1])\n",
    "                    random_L3 = np.random.choice(L3_log[random_L2])\n",
    "\n",
    "                    # Picking a value to be the base level of this entry\n",
    "                    value = samples[i]\n",
    "\n",
    "                    # Selecting the trend\n",
    "                    trend = random_trend*i\n",
    "\n",
    "                    # Finalizing the value to be inserted after adding the trend\n",
    "                    value = value + trend\n",
    "\n",
    "                    \n",
    "\n",
    "                    # Picking a date\n",
    "                    random_date = start_date + timedelta(days=selected_dates[i])\n",
    "                    \n",
    "                    # Applying the seasonality\n",
    "                    month_index = random_date.date().month\n",
    "                    selected_seasonal_effect = seasonal_effects[month_index-1]\n",
    "                    value = value*selected_seasonal_effect\n",
    "\n",
    "                    # Inserting the entry\n",
    "                    df.loc[df.shape[0],:] = [df.shape[0],np.random.choice(suppliers_log[random_L3]),random_date.date(),math.ceil(random_date.date().month/3),\n",
    "                                            random_date.date().year,round(value,0),random_L1,random_L2,random_L3,np.random.choice(Account),D1,D2,D3]\n",
    "\n",
    "                \n",
    "    # Checking the dimensions of the resulting dataframe\n",
    "    print('The dimensions (n-rows , n-columns) of the simulated dataframe are:  ',df.shape)\n",
    "    D3_behavior = pd.DataFrame.from_dict(behavior,orient='index')\n",
    "\n",
    "    return {'behavior':behavior,'df':df,'simulation_structure':simulation_structure,'suppliers_log':suppliers_log}\n",
    "\n",
    "\n",
    "def insert_anomalies(entries_object,seed ,number_of_anomalies = 20):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from datetime import datetime, timedelta\n",
    "    import math\n",
    "    import random\n",
    "    simulation_structure = entries_object['simulation_structure']\n",
    "    columns = simulation_structure['columns']\n",
    "    L1_log = simulation_structure['L1_log']\n",
    "    L2_log = simulation_structure['L2_log']\n",
    "    L3_log = simulation_structure['L3_log']\n",
    "    suppliers = simulation_structure['suppliers']\n",
    "    Account = simulation_structure['Account']\n",
    "    D1_log = simulation_structure['D1_log']\n",
    "    D2_log = simulation_structure['D2_log']\n",
    "    D3_log = simulation_structure['D3_log']\n",
    "    suppliers_log = entries_object['suppliers_log']\n",
    "\n",
    "    df = entries_object['df']\n",
    "    if number_of_anomalies > df.shape[0]:\n",
    "        raise ValueError('There will be more anomalies than entries')\n",
    "    behavior = entries_object['behavior']\n",
    "    ##########################################\n",
    "    # ... Adding the anomalies to the data ...\n",
    "    ##########################################\n",
    "\n",
    "    # Keeping log of produced anomalies\n",
    "    anomalies_log = pd.DataFrame(columns=['invoice_ID','department_D3','anomaly type','old_value','new_value',\n",
    "                                        'old_rate','new_rate','affected entries','iso/patchy/shift'])\n",
    "    number_of_inserted_anomalies = 0\n",
    "    affected_entities = []\n",
    "    deleted_indecies = []\n",
    "    number_of_anomalies = number_of_anomalies\n",
    "    \n",
    "    '''\n",
    "    {'mean':mean,'std_dev':std_dev,'random_rate':random_rate,\n",
    "                                'random_trend':random_trend,'seasonal_effects':seasonal_effects}\n",
    "    '''\n",
    "\n",
    "    while number_of_inserted_anomalies != number_of_anomalies:\n",
    "        i = number_of_inserted_anomalies\n",
    "\n",
    "        # Anomaly could be produced either by manipulating value or frequency\n",
    "        pick = np.random.choice(['extremely higher value','higher value','lower value'])#,'patchy higher','patchy lower'])\n",
    "        #,'lower frequency','higher frequency'])\n",
    "        #'patchy higher','patchy lower','level shift lower','level shift higher']) \n",
    "        # 'aggregate increase' is taken out ,'lower frequency','higher frequency'\n",
    "\n",
    "        if pick == 'higher value':\n",
    "            #affected_entities = []\n",
    "            # Produce an anomaly by manipulating one value to be abnormally high\n",
    "            # getting random index\n",
    "            random_index = random.randint(0,df.shape[0]-1)\n",
    "            if int(random_index) in deleted_indecies:\n",
    "                continue\n",
    "            department = df.loc[random_index,'Department (D3)']\n",
    "            if department in affected_entities:\n",
    "                continue\n",
    "            affected_entities.append(department)\n",
    "            old_invoice = df.loc[random_index,'Invoice Spend']\n",
    "            selected_behavior = behavior[department]\n",
    "            std_dev = selected_behavior['std_dev']\n",
    "\n",
    "            # Modifying the value to become 7*sigma which is associated with an anomaly\n",
    "            anomalous_value = old_invoice + 7*std_dev\n",
    "            df.loc[random_index,'Invoice Spend'] = anomalous_value\n",
    "\n",
    "            # Keeping a log of all anomalies\n",
    "            anomalies_log.loc[anomalies_log.shape[0],:] = [random_index,department,'higher value',int(old_invoice),int(anomalous_value),\n",
    "                                                        int(selected_behavior['random_rate']),int(selected_behavior['random_rate']),\n",
    "                                                        '-','isolated outliers']\n",
    "            number_of_inserted_anomalies = number_of_inserted_anomalies + 1\n",
    "\n",
    "        elif pick == 'extremely higher value':\n",
    "            #affected_entities = []\n",
    "            # Produce an anomaly by manipulating one value to be abnormally high\n",
    "            # getting random index\n",
    "            random_index = random.randint(0,df.shape[0]-1)\n",
    "            if int(random_index) in deleted_indecies:\n",
    "                continue\n",
    "            department = df.loc[random_index,'Department (D3)']\n",
    "            if department in affected_entities:\n",
    "                continue\n",
    "            affected_entities.append(department)\n",
    "            old_invoice = df.loc[random_index,'Invoice Spend']\n",
    "            selected_behavior = behavior[department]\n",
    "            std_dev = selected_behavior['std_dev']\n",
    "\n",
    "            # Modifying the value to become 7*sigma which is associated with an anomaly\n",
    "            anomalous_value = old_invoice *15\n",
    "            df.loc[random_index,'Invoice Spend'] = anomalous_value\n",
    "\n",
    "            # Keeping a log of all anomalies\n",
    "            anomalies_log.loc[anomalies_log.shape[0],:] = [random_index,department,'extremely higher value',int(old_invoice),int(anomalous_value),\n",
    "                                                        int(selected_behavior['random_rate']),int(selected_behavior['random_rate']),\n",
    "                                                        '-','isolated outliers']\n",
    "            number_of_inserted_anomalies = number_of_inserted_anomalies + 1\n",
    "\n",
    "\n",
    "        elif pick == 'aggregate increase':\n",
    "            # Manipluating the frequency of invoices to be abnormally low\n",
    "            # getting random index\n",
    "            random_index = random.randint(0,df.shape[0]-1)\n",
    "            if int(random_index) in deleted_indecies:\n",
    "                continue\n",
    "            department = df.loc[random_index,'Department (D3)']\n",
    "            commodity = df.loc[random_index,'Commodity (enriched) - Custom Category (L3)']\n",
    "            if commodity in affected_entities:\n",
    "                continue\n",
    "            affected_entities.append(commodity)\n",
    "            old_invoice = df.loc[random_index,'Invoice Spend']\n",
    "            selected_behavior = behavior[department]\n",
    "            std_dev = selected_behavior['std_dev']\n",
    "\n",
    "            anomalous_interval = random.randint(30,90)\n",
    "            original_date = df.loc[random_index,'Date']\n",
    "            new_date = original_date + pd.Timedelta(days=anomalous_interval)\n",
    "\n",
    "            entries_to_increase = df[(df['Commodity (enriched) - Custom Category (L3)']== commodity) & (df['Date']> original_date) & (df['Date']< new_date)]\n",
    "            indecies_to_increase = entries_to_increase['InvoiceID']\n",
    "\n",
    "            # Shifting the values of the following entries\n",
    "            value_for_log = int()\n",
    "            for p in indecies_to_increase:\n",
    "                old_value = df.loc[p,'Invoice Spend']\n",
    "                new_value = 2*old_value\n",
    "                df.loc[p,'Invoice Spend'] = new_value\n",
    "                if p == random_index:\n",
    "                    value_for_log = new_value\n",
    "\n",
    "\n",
    "            # Keeping a log of all anomalies\n",
    "            anomalies_log.loc[anomalies_log.shape[0],:] = [random_index,commodity,'aggregate increase',int(old_invoice),int(old_invoice),\n",
    "                                                        int(selected_behavior['random_rate']),int(selected_behavior['random_rate']),\n",
    "                                                        f'slightly increasing {len(indecies_to_increase)} entries','shift']\n",
    "            number_of_inserted_anomalies = number_of_inserted_anomalies + 1\n",
    "\n",
    "\n",
    "        elif pick == 'lower value':\n",
    "            #affected_entities = []\n",
    "            # Produce an anomaly by manipulating the value to be abnormally low\n",
    "            # getting random index\n",
    "            random_index = random.randint(0,df.shape[0]-1)\n",
    "            if int(random_index) in deleted_indecies:\n",
    "                continue\n",
    "            department = df.loc[random_index,'Department (D3)']\n",
    "            if department in affected_entities:\n",
    "                continue\n",
    "            affected_entities.append(department)\n",
    "            old_invoice = df.loc[random_index,'Invoice Spend']\n",
    "            selected_behavior = behavior[department]\n",
    "            std_dev = selected_behavior['std_dev']\n",
    "\n",
    "            # Modifying the value to become |old_value - 7xsd| which is associated with an anomaly\n",
    "            #   this, however, might not be easily detected because there are no negative invoices (bounded from below)\n",
    "            anomalous_value = old_invoice*0.1\n",
    "            df.loc[random_index,'Invoice Spend'] = anomalous_value\n",
    "\n",
    "            # Keeping a log of all anomalies\n",
    "            anomalies_log.loc[anomalies_log.shape[0],:] = [random_index,department,'lower value',int(old_invoice),int(anomalous_value),\n",
    "                                                        int(selected_behavior['random_rate']),int(selected_behavior['random_rate']),\n",
    "                                                        '-','isolated outliers']\n",
    "            number_of_inserted_anomalies = number_of_inserted_anomalies + 1\n",
    "\n",
    "        elif pick == 'lower frequency':\n",
    "            # Manipluating the frequency of invoices to be abnormally low\n",
    "            # getting random index\n",
    "            random_index = random.randint(0,df.shape[0]-1)\n",
    "            if int(random_index) in deleted_indecies:\n",
    "                continue\n",
    "            department = df.loc[random_index,'Department (D3)']\n",
    "            if department in affected_entities:\n",
    "                continue\n",
    "            affected_entities.append(department)\n",
    "            old_invoice = df.loc[random_index,'Invoice Spend']\n",
    "            selected_behavior = behavior[department]\n",
    "            std_dev = selected_behavior['std_dev']\n",
    "\n",
    "            # Modifying the interval between invoices to become 5*sigma which is associated with an anomaly\n",
    "            anomalous_rate = selected_behavior['random_rate']+15*selected_behavior['random_rate']\n",
    "            original_date = df.loc[random_index,'Date']\n",
    "            new_date = original_date + pd.Timedelta(days=anomalous_rate)\n",
    "\n",
    "            # Selecting all entries that lie within the range that should be skipped\n",
    "            entries_to_delete = df[(df['Department (D3)']== department) & (df['Date']> original_date) & (df['Date']< new_date)]\n",
    "            indecies_to_delete = entries_to_delete['InvoiceID']\n",
    "            deleted_indecies.append([int(i) for i in indecies_to_delete])\n",
    "\n",
    "            # Deleting them leaving behind anomalous gap\n",
    "            df = df.drop(indecies_to_delete)\n",
    "\n",
    "            # Keeping a log of all anomalies\n",
    "            anomalies_log.loc[anomalies_log.shape[0],:] = [random_index,department,'lower frequency',int(old_invoice),int(old_invoice),\n",
    "                                                        int(selected_behavior['random_rate']),int(anomalous_rate),\n",
    "                                                        f'deleting {len(indecies_to_delete)} entries','-']\n",
    "            number_of_inserted_anomalies = number_of_inserted_anomalies + 1\n",
    "\n",
    "        elif pick == 'higher frequency':\n",
    "            # Manipluating the frequency of invoices to be abnormally high\n",
    "            # getting random index\n",
    "            random_index = random.randint(0,df.shape[0]-1)\n",
    "            if int(random_index) in deleted_indecies:\n",
    "                continue\n",
    "            department = df.loc[random_index,'Department (D3)']\n",
    "            if department in affected_entities:\n",
    "                continue\n",
    "            affected_entities.append(department)\n",
    "            old_invoice = df.loc[random_index,'Invoice Spend']\n",
    "            department_1 = df.loc[random_index,'Department (D1)']\n",
    "            department_2 = df.loc[random_index,'Department (D2)']\n",
    "\n",
    "            selected_behavior = behavior[department]\n",
    "            std_dev = selected_behavior['std_dev']\n",
    "\n",
    "            # Modifying the interval between invoices to become sigma/7 which is associated with an anomaly\n",
    "            anomalous_rate = selected_behavior['random_rate']/15\n",
    "            original_date = df.loc[random_index,'Date']\n",
    "            anomalous_range = random.randint(60,90)\n",
    "\n",
    "\n",
    "            # In terms of values, we keep the same bahavior.\n",
    "            mean = selected_behavior['mean']\n",
    "            std_dev = selected_behavior['std_dev']\n",
    "            trend = selected_behavior['random_trend']\n",
    "            seasonal_effects = selected_behavior['seasonal_effects']\n",
    "            samples = np.abs(np.random.normal(old_invoice, std_dev, 3000))\n",
    "\n",
    "            # Calculate the anomalous dates\n",
    "            # Assuming the time to event is exponential with random rate\n",
    "            random_days = np.random.exponential(anomalous_rate,3000)\n",
    "            comulative_sum = np.cumsum(random_days)\n",
    "\n",
    "            # Selecting only the dates within the anomalous range\n",
    "            selected_dates = [int(date) for date in comulative_sum if date < anomalous_range ]\n",
    "            \n",
    "            # Deciding on the number of added entries for this anlmalous department\n",
    "            entries = len(selected_dates)\n",
    "\n",
    "            # Entring one entry through each iteration\n",
    "            for j in range(entries):\n",
    "\n",
    "                # Picking an item\n",
    "                random_L1 = np.random.choice(L1_log)\n",
    "                random_L2 = np.random.choice(L2_log[random_L1])\n",
    "                random_L3 = np.random.choice(L3_log[random_L2])\n",
    "\n",
    "                # Picking a value\n",
    "                value = samples[j]\n",
    "\n",
    "                # Applying trend\n",
    "                value = value + trend*j\n",
    "\n",
    "                # Picking a date\n",
    "                start_date = original_date\n",
    "                random_date = start_date + timedelta(days=selected_dates[j])\n",
    "\n",
    "                month_index = random_date.month\n",
    "                selected_seasonal_effect = seasonal_effects[month_index-1]\n",
    "                value = value * selected_seasonal_effect\n",
    "\n",
    "                # Inserting the entry\n",
    "                df.loc[df.shape[0],:] = [df.shape[0],np.random.choice(suppliers_log[random_L3]),random_date,\n",
    "                                        math.ceil(random_date.month/3),random_date.year,round(value,0),random_L1,\n",
    "                                        random_L2,random_L3,np.random.choice(Account),department_1,department_2,department]\n",
    "                \n",
    "                \n",
    "            \n",
    "            # Keeping a log of all anomalies\n",
    "            anomalies_log.loc[anomalies_log.shape[0],:] = [random_index,department,'higher frequency',int(old_invoice),int(old_invoice),\n",
    "                                                        int(selected_behavior['random_rate']),int(anomalous_rate),f'adding {entries} entries','-']\n",
    "            number_of_inserted_anomalies = number_of_inserted_anomalies + 1\n",
    "\n",
    "\n",
    "        elif (pick == 'patchy higher') | (pick == 'patchy lower'):\n",
    "            #affected_entities = []\n",
    "            # getting random index to start the patchy outlier effect.\n",
    "            random_index = random.randint(0,df.shape[0]-1)     \n",
    "            if int(random_index) in deleted_indecies:\n",
    "                continue    \n",
    "            department = df.loc[random_index,'Department (D3)']\n",
    "            if department in affected_entities:\n",
    "                continue\n",
    "            affected_entities.append(department)\n",
    "            original_date = df.loc[random_index,'Date']\n",
    "            selected_behavior = behavior[department]\n",
    "            std_dev = selected_behavior['std_dev']\n",
    "            seasonal_effects = selected_behavior['seasonal_effects']\n",
    "            patchy_size = random.randint(2,10)\n",
    "\n",
    "            # Selecting indecies that are affected by the patchy effect.\n",
    "            entries_to_change = df[(df['Department (D3)']== department) & (df['Date']>= original_date)]\n",
    "            indecies_to_change = entries_to_change.index[0:patchy_size]\n",
    "\n",
    "            if pick == 'patchy higher':\n",
    "                for indx in indecies_to_change:\n",
    "                    dep = df.loc[indx,'Department (D3)']\n",
    "                    old_val = df.loc[indx,'Invoice Spend']\n",
    "                    anomalous_val = int(old_val+std_dev*7)\n",
    "                    error = np.random.uniform(anomalous_val/20,anomalous_val/18)\n",
    "                    anomalous_val = anomalous_val +error\n",
    "\n",
    "                    \n",
    "                    df.loc[indx,'Invoice Spend'] = anomalous_val\n",
    "                # Keeping a log of all anomalies\n",
    "                anomalies_log.loc[anomalies_log.shape[0],:] = [random_index,department,'patchy higher',int(old_val),int(anomalous_val),\n",
    "                                                            int(selected_behavior['random_rate']),int(selected_behavior['random_rate']),\n",
    "                                                            f'increased {patchy_size} consequtive entries','patchy outliers']\n",
    "                number_of_inserted_anomalies = number_of_inserted_anomalies + 1\n",
    "\n",
    "            if pick == 'patchy lower':\n",
    "                for indx in indecies_to_change:\n",
    "                    dep = df.loc[indx,'Department (D3)']\n",
    "                    old_val = df.loc[indx,'Invoice Spend']\n",
    "                    anomalous_val = int(0.1*old_val )\n",
    "                    error = np.random.uniform(anomalous_val/30,anomalous_val/28)\n",
    "                    anomalous_val = anomalous_val +error\n",
    "                    \n",
    "                    df.loc[indx,'Invoice Spend'] = anomalous_val\n",
    "                # Keeping a log of all anomalies\n",
    "                anomalies_log.loc[anomalies_log.shape[0],:] = [random_index,department,'patchy lower',int(old_val),int(anomalous_val),\n",
    "                                                            int(selected_behavior['random_rate']),int(selected_behavior['random_rate']),\n",
    "                                                            f'decreased {patchy_size} consequtive entries','patchy outliers']\n",
    "                number_of_inserted_anomalies = number_of_inserted_anomalies + 1\n",
    "\n",
    "            ''' \n",
    "            behavior[D3] = {'mean':mean,'std_dev':std_dev,'random_rate':random_rate,\n",
    "                    'random_trend':random_trend,'seasonal_effects':seasonal_effects}\n",
    "            '''\n",
    "\n",
    "        if pick == 'level shift lower':\n",
    "            #affected_entities = []\n",
    "            # Manipluating the level of invoices to be abnormally low and stay low for one department at a time\n",
    "            # getting random index\n",
    "            random_index = random.randint(0,df.shape[0]-1)\n",
    "            if int(random_index) in deleted_indecies:\n",
    "                continue\n",
    "            department = df.loc[random_index,'Department (D3)']\n",
    "            if department in affected_entities:\n",
    "                continue\n",
    "            affected_entities.append(department)\n",
    "            old_invoice = df.loc[random_index,'Invoice Spend']\n",
    "            selected_behavior = behavior[department]\n",
    "            std_dev = selected_behavior['std_dev']\n",
    "\n",
    "            # Modifying the value of following invoices to become shifted downwards which is associated with an anomaly\n",
    "            \n",
    "            original_date = df.loc[random_index,'Date']\n",
    "\n",
    "            # Selecting all entries follow the selected one for the same department\n",
    "            entries_to_shift = df[(df['Department (D3)']== department) & (df['Date']>= original_date)]\n",
    "            indecies_to_shift = entries_to_shift['InvoiceID']\n",
    "\n",
    "            # Shifting the values of the following entries\n",
    "            value_for_log = int()\n",
    "            for p in indecies_to_shift:\n",
    "                old_value = df.loc[p,'Invoice Spend']\n",
    "                new_value = 0.1*old_value\n",
    "                df.loc[p,'Invoice Spend'] = new_value\n",
    "                if p == random_index:\n",
    "                    value_for_log = new_value\n",
    "\n",
    "            # Keeping a log of all anomalies\n",
    "            anomalies_log.loc[anomalies_log.shape[0],:] = [random_index,department,'level shift lower',int(old_invoice),\n",
    "                                                        int(value_for_log),int(selected_behavior['random_rate']),\n",
    "                                                        int(selected_behavior['random_rate']),\n",
    "                                                        f'shifting {len(indecies_to_shift)} entries downwards','shift']\n",
    "            number_of_inserted_anomalies = number_of_inserted_anomalies + 1\n",
    "\n",
    "        if pick == 'level shift higher':\n",
    "            #affected_entities = []\n",
    "            # Manipluating the level of invoices to be abnormally high and stay low for one department at a time\n",
    "            # getting random index\n",
    "            random_index = random.randint(0,df.shape[0]-1)\n",
    "            if int(random_index) in deleted_indecies:\n",
    "                continue\n",
    "            department = df.loc[random_index,'Department (D3)']\n",
    "            if department in affected_entities:\n",
    "                continue\n",
    "            affected_entities.append(department)\n",
    "            old_invoice = df.loc[random_index,'Invoice Spend']\n",
    "            selected_behavior = behavior[department]\n",
    "            std_dev = selected_behavior['std_dev']\n",
    "\n",
    "            # Modifying the value of following invoices to become shifted upwards which is associated with an anomaly\n",
    "            \n",
    "            original_date = df.loc[random_index,'Date']\n",
    "\n",
    "            # Selecting all entries follow the selected one for the same department\n",
    "            entries_to_shift = df[(df['Department (D3)']== department) & (df['Date']>= original_date)]\n",
    "            indecies_to_shift = entries_to_shift['InvoiceID']\n",
    "\n",
    "            # Shifting the values of the following entries\n",
    "            value_for_log = int()\n",
    "            for p in indecies_to_shift:\n",
    "                old_value = df.loc[p,'Invoice Spend']\n",
    "                new_value = old_value + 7*std_dev\n",
    "                df.loc[p,'Invoice Spend'] = new_value\n",
    "                if p == random_index:\n",
    "                    value_for_log = new_value\n",
    "\n",
    "            # Keeping a log of all anomalies\n",
    "            anomalies_log.loc[anomalies_log.shape[0],:] = [random_index,department,'level shift higher',int(old_invoice),\n",
    "                                                        int(value_for_log),int(selected_behavior['random_rate']),\n",
    "                                                        int(selected_behavior['random_rate']),\n",
    "                                                        f'shifting {len(indecies_to_shift)} entries upwards','shift']\n",
    "            number_of_inserted_anomalies = number_of_inserted_anomalies + 1\n",
    "\n",
    "    # Reordering the data based on the date\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df_sorted = df.sort_values(by='Date')\n",
    "    \n",
    "    \n",
    "    return {'df_sorted':df_sorted,'anomalies_log':anomalies_log}\n",
    "\n",
    "\n",
    "\n",
    "def save_data_anomalies_to_file(anomalous_data_object ,\n",
    "                                path ='/Workspace/Users/ashraf.zedane@nl.abnamro.com/data_simulation/',\n",
    "                                data_name_str = 'dummy.xlsx',anomalies_log_str = 'anomalies_log.xlsx' ):\n",
    "    \n",
    "    if path[:62] != '/Workspace/Users/ashraf.zedane@nl.abnamro.com/data_simulation/':\n",
    "        raise ValueError('The path to save the data must be in \"/Workspace/Users/ashraf.zedane@nl.abnamro.com/data_simulation/\" ')\n",
    "\n",
    "    df_sorted = anomalous_data_object['df_sorted']\n",
    "    anomalies_log = anomalous_data_object['anomalies_log']\n",
    "    ###########################\n",
    "    # ... Exporting to excel...\n",
    "    ###########################\n",
    "\n",
    "    path_data  = str(path) + str(data_name_str)\n",
    "    df_sorted.to_excel(path_data, index=False)\n",
    "\n",
    "    print('files have been saved to the following location')\n",
    "    print(path_data)\n",
    "\n",
    "\n",
    "    path_anomalies_log  = str(path) + str(anomalies_log_str)\n",
    "    anomalies_log.to_excel(path_anomalies_log,index=True)\n",
    "\n",
    "    print('files have been saved to the following location')\n",
    "    print(path_anomalies_log)\n",
    "\n",
    "def simulate_data(seed,\n",
    "                  Commodities_L1_number = 1,\n",
    "                  Departments_L1_number = 1,\n",
    "                  Account_level_number = 3,\n",
    "                  Suppliers_number = 4,\n",
    "                  start_year = 2013,\n",
    "                  end_year = 2023,\n",
    "                  number_of_anomalies = 250,\n",
    "                  path ='/Workspace/Users/ashraf.zedane@nl.abnamro.com/data_simulation/Reduced_data/',\n",
    "                  data_name_str = 'dummy.xlsx',\n",
    "                  anomalies_log_str = 'anomalies_log.xlsx',\n",
    "                  mu = [500000,1000000],sdv = [50000,100000],\n",
    "                  rtrend = [25000,50000]):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from datetime import datetime, timedelta\n",
    "    import math\n",
    "    import random\n",
    "\n",
    "    simulation_structure = get_simulation_structure(Commodities_L1_number = Commodities_L1_number,\n",
    "                                                    Departments_L1_number =Departments_L1_number,\n",
    "                                                    Account_level_number=Account_level_number,\n",
    "                                                    Suppliers_number = Suppliers_number)\n",
    "\n",
    "    entries_object = insert_entries(simulation_structure=simulation_structure,\n",
    "                                    start_year = start_year,end_year = end_year,\n",
    "                                    mu = mu ,sdv = sdv,seed = seed)\n",
    "\n",
    "    anomalous_data_object = insert_anomalies(entries_object = entries_object,number_of_anomalies = number_of_anomalies,seed = seed)\n",
    "\n",
    "    save_data_anomalies_to_file(anomalous_data_object = anomalous_data_object,\n",
    "                                path =path,\n",
    "                                data_name_str = data_name_str, \n",
    "                                anomalies_log_str = anomalies_log_str)\n",
    "    print('done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88561dd4-5be7-4589-8e42-0f9476252c10",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## The robust anomaly detection algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34c868c7-606b-4e5a-8ced-d05502c40624",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import math\n",
    "import statistics\n",
    "from scipy.optimize import minimize\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "np.random.seed(1)\n",
    "# Return sign        \n",
    "def sign(x):\n",
    "   \"\"\"A function that returns the signal of a number.\n",
    "\n",
    "    Args:\n",
    "        x: the number whose signal is wanted\n",
    "\n",
    "    Returns:\n",
    "        int: either -1 or 1 depending on x's signal\n",
    "\n",
    "    Usage:\n",
    "        sign(15) = 1\n",
    "    \"\"\"\n",
    "   return int(math.copysign(1, x))\n",
    "\n",
    "\n",
    "# Huber function to limit the effect of extreme values:\n",
    "def huber(x,k_huber=3):\n",
    "    \"\"\"A function that prevents observations from having unbounded values.\n",
    "\n",
    "    This function imposes a ceiling on values, ensuring they do not exceed a specified threshold.\n",
    "\n",
    "    Args:\n",
    "    x: The number that is under study.\n",
    "    k_huber: The ceiling value.\n",
    "\n",
    "    Returns:\n",
    "    float: A value that follows the given number but never exceeds the ceiling.\n",
    "\n",
    "    Usage:\n",
    "    huber(5, k_huber=3) = 3\n",
    "    \"\"\"\n",
    "\n",
    "    k_huber = 10\n",
    "    if abs(x)<k_huber:\n",
    "        return x\n",
    "    else:\n",
    "        return sign(x)*k_huber\n",
    "\n",
    "\n",
    "# Biweight function\n",
    "def get_biweight(x,ck,k):\n",
    "    \"\"\"A function that is used to assign weights based on a distance measure.\n",
    "\n",
    "    The values of the constants ck and k describe how the assigned weight changes.\n",
    "\n",
    "    Args:\n",
    "        x: The number for which the weight is calculated.\n",
    "        ck: A constant that influences the weight.\n",
    "        k: A constant that determines the range of influence.\n",
    "\n",
    "    Returns:\n",
    "        float: The weight that the number should carry.\n",
    "\n",
    "    Usage:\n",
    "        get_biweight(-2, ck=5, k=1.75) = 5\n",
    "    \"\"\"\n",
    "    if abs(x)<k:\n",
    "        return ck*(1-(1-(x/k)**2)**3)\n",
    "    else:\n",
    "        return ck\n",
    "\n",
    "\n",
    "# Estimating sigma_hat from the privious sigma\n",
    "def get_sigma_hat(k,ck,previous_sigma,observed,predicted,lmbda,integers,min_dif,multiplicative_error=True):\n",
    "    \"\"\"Estimates sigma_hat on each step by taking previous sigma into account, in addition to the observed and predicted values.\n",
    "\n",
    "    The parameter lmbda is the smoothing parameter, estimated through the optimization process. \n",
    "    Based on whether the error is multiplicative or additive, calculations are different.\n",
    "\n",
    "    Args:\n",
    "        k: Constant affecting the get_biweight() function.\n",
    "        ck: Constant affecting the get_biweight() function.\n",
    "        previous_sigma: The previous value of sigma.\n",
    "        observed: The observed value.\n",
    "        predicted: The predicted value.\n",
    "        lmbda: The smoothing parameter.\n",
    "        integers: Boolean indicating if the data is integer-valued.\n",
    "        min_dif: Minimum difference to avoid division by zero.\n",
    "        multiplicative_error: Boolean indicating if the error is multiplicative.\n",
    "\n",
    "    Returns:\n",
    "        float: An estimate of the current sigma.\n",
    "\n",
    "    Usage:\n",
    "        get_sigma_hat(k=1.75, ck=5, previous_sigma=0.5, observed=1e4, predicted=9e3, lmbda=0.2, \n",
    "                      multiplicative_error=True, integers=False, mid_dif = 0.01) = 0.46032\n",
    "    \"\"\"\n",
    "    # To avoid division on zero, especially when dealing with anomalous values.\n",
    "    epsilon = 1e-8\n",
    "\n",
    "    # The possibility of setting a minimum value for sigma\n",
    "    if integers:\n",
    "      epsilon_sigma = min_dif\n",
    "    else:\n",
    "      epsilon_sigma = 0\n",
    "\n",
    "    # Overwrite min_dif value. The restriction will be set on the initial sigma_0\n",
    "    epsilon_sigma = 0\n",
    "\n",
    "    if multiplicative_error:\n",
    "      sigma_hat_squared = lmbda * get_biweight((observed-predicted)/(predicted * previous_sigma+epsilon),ck=ck,k=k) * (previous_sigma)**2 + (1-lmbda) * (previous_sigma)**2\n",
    "\n",
    "      # In case of an error, to understand the circumstances\n",
    "      if sigma_hat_squared<0:\n",
    "         print('Negative value: ',sigma_hat_squared,'\\n',\n",
    "               'lmbda: ',lmbda,' previous_sigma: ',previous_sigma,'\\n',\n",
    "               'observed: ',observed,' .predicted: ',predicted,'\\n',\n",
    "               'get_biweight: ',get_biweight((observed-predicted)/(predicted * previous_sigma+epsilon),ck=ck,k=k),'\\n')\n",
    "      sigma_hat = math.sqrt(sigma_hat_squared)\n",
    "\n",
    "      # Possibly setting a minimum sigma value if epsilon_sigma!=0\n",
    "      return max(sigma_hat,epsilon_sigma)\n",
    "    \n",
    "    # Additive error case   \n",
    "    else:\n",
    "      sigma_hat_squared = lmbda * get_biweight((observed-predicted)/(previous_sigma+epsilon),ck = ck,k=k) * (previous_sigma)**2 + (1-lmbda) * (previous_sigma)**2\n",
    "      \n",
    "      # in case of an error\n",
    "      if sigma_hat_squared<0:\n",
    "         print('Negative value: ',sigma_hat_squared,'\\n',\n",
    "               'lmbda: ',lmbda,' previous_sigma: ',previous_sigma,'\\n',\n",
    "               'observed: ',observed,' .predicted: ',predicted,'\\n',\n",
    "               'get_biweight: ',get_biweight((observed-predicted)/(predicted * previous_sigma+epsilon),ck=ck,k=k),'\\n')\n",
    "      sigma_hat = math.sqrt(sigma_hat_squared)\n",
    "\n",
    "      # Possibly setting a minimum sigma value if epsilon_sigma!=0\n",
    "      return max(sigma_hat,epsilon_sigma)\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Clean observations:\n",
    "def get_clean_observation(k,observed ,predicted, sigma_hat,multiplicative_error=True):\n",
    "    \"\"\"Clean observations are observations whose values are adjusted to limit the influence of extreme data points.\n",
    "\n",
    "    Observed values and predictions, in addition to sigma_hat, are necessary to set the threshold of where to start \"cleaning\".\n",
    "    The clean observations are the ones used to obtain the predicted values for the time series and not the actual observations.\n",
    "\n",
    "    Args:\n",
    "        k: Constant affecting the huber() function.\n",
    "        observed: The observed value.\n",
    "        predicted: The predicted value.\n",
    "        sigma_hat: The estimated sigma value.\n",
    "        multiplicative_error: Boolean indicating if the error is multiplicative.\n",
    "\n",
    "    Returns:\n",
    "        float: An adjusted observation if its value is extreme, otherwise the same as the actual observation.\n",
    "\n",
    "    Usage:\n",
    "        get_clean_observation(k=1.75, observed=1e4, predicted=9e3, sigma_hat=0.54, multiplicative_error=True) = 9999.9999\n",
    "    \"\"\"\n",
    "    # To avoid division on zero when dealing with anomalies\n",
    "    epsilon = 1e-8\n",
    "    if multiplicative_error:\n",
    "      clean_observation = (1+ huber((observed-predicted)/(predicted*sigma_hat+epsilon))*sigma_hat)*predicted\n",
    "      return clean_observation\n",
    "    else:\n",
    "      clean_observation = huber((observed-predicted)/(sigma_hat+epsilon))*sigma_hat + predicted\n",
    "      return clean_observation\n",
    "   \n",
    "\n",
    "\n",
    "# MAD Median Absolute Deviance\n",
    "def get_MAD(errors):\n",
    "  \"\"\"This is an alternative to the mean() for when doing robust calculations against outliers.\n",
    "\n",
    "  Because outliers can have a greater effect on means, medians represent a much more robust alternative.\n",
    "\n",
    "  Args:\n",
    "      errors: A list of errors. If errors are additive, it's the difference between observed and predicted values.\n",
    "              If errors are multiplicative, it's the ratio between observed and predicted values.\n",
    "\n",
    "  Returns:\n",
    "      float: The Median Absolute Deviation (MAD).\n",
    "\n",
    "  Usage:\n",
    "      get_MAD([3,6,3,1,34,2]) = 2.2239\n",
    "  \"\"\"\n",
    "\n",
    "  # Calculate the absolute deviations from the median\n",
    "  med = np.median(errors)\n",
    "  absolute_error = np.abs(errors-med)\n",
    "  # Calculate the MAD\n",
    "  MAD = 1.4826 * np.median(absolute_error)\n",
    "  # Trying possibly: setting a minimum on the MAD value which is not 0\n",
    "  #MAD = max(MAD,get_min_pairwise_differences(absolute_error))\n",
    "  return MAD\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_t_tilde(t, m):\n",
    "  \"\"\"To compute the right moment of the seasonal cycle.\n",
    "\n",
    "  This function calculates the position of an observation within the seasonal cycle.\n",
    "\n",
    "  Args:\n",
    "      t: The index of the observation.\n",
    "      m: The length of one season.\n",
    "\n",
    "  Returns:\n",
    "      int: The position of the observation in the seasonal cycle.\n",
    "\n",
    "  Usage:\n",
    "      get_t_tilde(401, 12) = 5\n",
    "  \"\"\"\n",
    "\n",
    "  t_tilde = (t % m)\n",
    "  return t_tilde\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def tau_squared(e,ck,k):\n",
    "   \"\"\"A robust alternative to the sum of squares.\n",
    "\n",
    "   This function is used in the target function of the optimization process to calculate how distant the observations \n",
    "   are from a supposed center, which is the Median Absolute Deviation (MAD).\n",
    "\n",
    "   Args:\n",
    "       e: A list of errors. If errors are additive, it's the difference between observed and predicted values.\n",
    "          If errors are multiplicative, it's the ratio between observed and predicted values.\n",
    "       ck: A constant influencing the weight calculation.\n",
    "       k: A constant determining the range of influence.\n",
    "\n",
    "   Returns:\n",
    "       float: The quantity of tau^2.\n",
    "\n",
    "   Usage:\n",
    "       tau_squared([3,6,3,1,34,2], ck=5, k=1.75) = 19.17745\n",
    "   \"\"\"\n",
    "\n",
    "   # To avoid division on zero especially when dealing with anomalous values\n",
    "   epsilon = 1e-6\n",
    "   s_T = get_MAD(e)\n",
    "   ck=5\n",
    "   k=1.75\n",
    "   TT = len(e)\n",
    "   ratio  = (s_T**2/TT)\n",
    "   summ = 0\n",
    "   \n",
    "   for i in range(len(e)):\n",
    "      x=e[i]/(s_T+epsilon)\n",
    "      summ = summ + get_biweight((e[i]/(s_T+epsilon)),ck=ck,k=k)\n",
    "   tau_2 = ratio * summ\n",
    "\n",
    "   # In case of an error\n",
    "   if tau_2<0:\n",
    "      print('Negative tau_squared value encountered', tau_2)\n",
    "      raise ValueError('Negative tau_squared value encountered')\n",
    "   # In case of an error\n",
    "   if tau_2 == 0:\n",
    "      print(tau_2, ' = 0 encountered','s_T: ',s_T,' | ck: ',ck,' | k:',k,' | TT:',TT,' | ratio: ',ratio,' | summ: ',summ)\n",
    "      print('The e vector that led to the error: ',e)\n",
    "      print('The np.median(errors) vector that led to the error: ',np.median(e))\n",
    "      print('The np.abs(errors-np.median(errors)) vector that led to the error: ',np.abs(e-np.median(e)))\n",
    "      print('The 1.4826 * np.median(np.abs(errors-np.median(errors))) vector that led to the error: ',1.4826 * np.median(np.abs(e-np.median(e))))\n",
    "      print('get_min_pairwise_differences(e): ',get_min_pairwise_differences(e))\n",
    "   return tau_2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def rob_lik_A (observed,predicted,ck,k):\n",
    "   \"\"\"Getting the robust likelihood for the additive error case.\n",
    "\n",
    "   This function utilizes the tau_squared() function to compute the robust likelihood.\n",
    "\n",
    "   Args:\n",
    "       observed: The observed values.\n",
    "       predicted: The predicted values.\n",
    "       ck: A constant influencing the weight calculation.\n",
    "       k: A constant determining the range of influence.\n",
    "\n",
    "   Returns:\n",
    "       float: The robust likelihood for the additive error case.\n",
    "\n",
    "   Usage:\n",
    "       rob_lik_A(observed=np.array([10, 19, 20]), predicted=np.array([1, 14, 19]), ck=5, k=1.75) = -6.77312\n",
    "   \"\"\"\n",
    "\n",
    "   e = observed - predicted\n",
    "   TT = len(e)\n",
    "   tau2 = tau_squared(e,ck=ck,k=k)\n",
    "   if tau2<0:\n",
    "      print('Encountered a negative value of tau_squared',tau2)\n",
    "   roblik_A = (-TT/2) * math.log(tau2)\n",
    "   return roblik_A\n",
    "\n",
    "\n",
    "def get_multiplicative_error(observed,predicted):\n",
    "   \"\"\"For the multiplicative case, errors are not the difference between observed and predicted.\n",
    "\n",
    "   Here, the error is the ratio between the difference and the predicted value.\n",
    "\n",
    "   Args:\n",
    "       observed: The observed values.\n",
    "       predicted: The predicted values.\n",
    "\n",
    "   Returns:\n",
    "       np.ndarray: The calculated multiplicative errors.\n",
    "\n",
    "   Usage:\n",
    "       get_multiplicative_error(observed=[10, 15, 20], predicted=[9, 14, 19]) = array([0.11111111, 0.07142857, 0.05263158])\n",
    "   \"\"\"\n",
    "\n",
    "   epsilon = 1e-8\n",
    "   e = np.zeros(len(observed))\n",
    "   for i in range(len(observed)):\n",
    "    e[i] = (observed[i]-predicted[i])/(predicted[i]+epsilon)\n",
    "   return e\n",
    "\n",
    "\n",
    "def rob_lik_M(observed,predicted,ck,k):\n",
    "   \"\"\"Calculating the robust likelihood for the multiplicative case.\n",
    "\n",
    "   This function utilizes the tau_squared() function to compute the robust likelihood for multiplicative errors.\n",
    "\n",
    "   Args:\n",
    "       observed: The observed values.\n",
    "       predicted: The predicted values.\n",
    "       ck: A constant influencing the weight calculation.\n",
    "       k: A constant determining the range of influence.\n",
    "\n",
    "   Returns:\n",
    "       float: The robust likelihood for the multiplicative error case.\n",
    "\n",
    "   Usage:\n",
    "       rob_lik_M(observed=[10, 15, 20], predicted=[9, 14, 19], ck=5, k=1.75) = 8.32662\n",
    "   \"\"\"\n",
    "\n",
    "   e = get_multiplicative_error(observed,predicted)\n",
    "   TT = len(e)\n",
    "   tau2 = tau_squared(e,ck=ck,k=k)\n",
    "   if tau2<0:\n",
    "      print('Encountered a negative value of tau_squared',tau2)\n",
    "   roblik_M = (-TT/2) * math.log(tau2)\n",
    "   return roblik_M\n",
    "\n",
    "\n",
    "def get_min_pairwise_differences(arr):\n",
    "   \"\"\"Returns the non-zero minimum pairwise difference between the values of an array.\n",
    "\n",
    "   This function calculates the absolute differences between all pairs of elements in the array and finds the minimum non-zero difference.\n",
    "\n",
    "   Args:\n",
    "       arr: An array of numerical values.\n",
    "\n",
    "   Returns:\n",
    "       float: The minimum non-zero pairwise difference. Returns None if no such difference exists.\n",
    "\n",
    "   Usage:\n",
    "       get_min_pairwise_differences([3, 6, 3, 1, 34, 2]) = 1\n",
    "   \"\"\"\n",
    "\n",
    "   differences =  [abs(arr[i]-arr[j]) for i in range(len(arr)) for j in range(i+1,len(arr))]\n",
    "   arr = np.array(differences)\n",
    "   filtered_arr = arr[arr != 0]\n",
    "   return filtered_arr.min() if filtered_arr.size > 0 else None\n",
    "\n",
    "\n",
    "def get_startup_period(seasonality,y,m):\n",
    "   \"\"\"Determines the startup period for obtaining initial values in a time series analysis.\n",
    "\n",
    "   The startup period is the number of observations at the beginning of the series used to obtain all initial values. \n",
    "   It is estimated differently based on whether there is a seasonal component.\n",
    "\n",
    "   Args:\n",
    "       seasonality: A boolean indicating the presence of a seasonal component.\n",
    "       y: The time series data.\n",
    "       m: The length of one season.\n",
    "\n",
    "   Returns:\n",
    "       int: The number of observations in the startup period.\n",
    "\n",
    "   Usage:\n",
    "       get_startup_period(seasonality=True, y=[i for i in range(50)], m=3) = 15\n",
    "   \"\"\"\n",
    "\n",
    "   if seasonality :\n",
    "      startup_period = max(5*m,10)\n",
    "   else:\n",
    "      startup_period = 10\n",
    "   if len(y)<startup_period:\n",
    "      warnings.warn(\"There are too few data points to obtain robust starting values.\", FutureWarning)\n",
    "      startup_period = len(y)\n",
    "   return startup_period\n",
    "\n",
    "\n",
    "def get_b_0(startup_period,y):\n",
    "    \"\"\"Calculates the initial slope estimate (b_0_hat) for a time series.\n",
    "\n",
    "    This function computes the median of the slopes between all pairs of points in the startup period.\n",
    "\n",
    "    Args:\n",
    "    startup_period: The number of observations used to calculate the initial values.\n",
    "    y: The time series data.\n",
    "\n",
    "    Returns:\n",
    "    float: The initial slope estimate (b_0_hat).\n",
    "\n",
    "    Usage:\n",
    "    get_b_0(startup_period=10, y=[i for i in range(0,50, 3)]) = 3.0\n",
    "    \"\"\"\n",
    "\n",
    "    # b_0_hat : Getting the median of i for the medians of j\n",
    "    amounts_i =[]\n",
    "    for i in range(startup_period):\n",
    "        amounts_j = []\n",
    "        for j in range(startup_period):\n",
    "            if i != j:\n",
    "                amounts_j.append((y[i]-y[j])/(i-j))\n",
    "        median_j = statistics.median(amounts_j)\n",
    "        amounts_i.append(median_j)\n",
    "    b_0_hat = statistics.median(amounts_i)\n",
    "    return b_0_hat\n",
    "\n",
    "\n",
    "def get_l_o(y,startup_period,b_0_hat):\n",
    "   \"\"\"Calculates the initial level estimate (L_0_hat) for a time series.\n",
    "\n",
    "   This function computes the median of the differences between the observed values and the initial slope-adjusted values \n",
    "   in the startup period.\n",
    "\n",
    "   Args:\n",
    "       y: The time series data.\n",
    "       startup_period: The number of observations used to calculate the initial values.\n",
    "       b_0_hat: The initial slope estimate.\n",
    "\n",
    "   Returns:\n",
    "       float: The initial level estimate (L_0_hat).\n",
    "\n",
    "   Usage:\n",
    "       get_l_o(y=[i for i in range(0,50, 3)], startup_period=10, b_0_hat=0.5) = 11.25\n",
    "   \"\"\"\n",
    "\n",
    "   # L_0_hat :\n",
    "   startup_observations = y[:startup_period]\n",
    "   startup_index = range(startup_period)\n",
    "   l_0_hat = statistics.median([startup_observations[i]-b_0_hat*i for i in startup_index])\n",
    "   return l_0_hat\n",
    "\n",
    "\n",
    "def get_s_0(y,multiplicative_seasonality,m,startup_period,b_0_hat,l_0_hat):\n",
    "   \"\"\"Calculates the initial seasonal components (S_0) for a time series.\n",
    "\n",
    "   This function computes the seasonal starting values for all seasons based on whether the seasonality is multiplicative or additive.\n",
    "\n",
    "   Args:\n",
    "       y: The time series data.\n",
    "       multiplicative_seasonality: Boolean indicating if the seasonality is multiplicative.\n",
    "       m: The length of one season.\n",
    "       startup_period: The number of observations used to calculate the initial values.\n",
    "       b_0_hat: The initial slope estimate.\n",
    "       l_0_hat: The initial level estimate.\n",
    "\n",
    "   Returns:\n",
    "       list: The initial seasonal components (S_0).\n",
    "\n",
    "   Usage:\n",
    "       get_s_0(y=[i for i in range(0,50, 3)], multiplicative_seasonality=True, m=3, startup_period=10, \n",
    "               b_0_hat=0.5, l_0_hat=2.0) = [2.7613 ,2.6666 ,2.999 ]\n",
    "   \"\"\"\n",
    "\n",
    "   # s_0_hat : Getting the seasonal starting values for all seasons\n",
    "   startup_observations = y[:startup_period]\n",
    "   startup_index = range(startup_period)\n",
    "   epsilon = 1e-6 # To avoid deviding by zero\n",
    "\n",
    "\n",
    "   if multiplicative_seasonality:\n",
    "      for q in range(m):\n",
    "         globals()[f'S_{q}'] = []\n",
    "         for i in range(q, startup_period, m):\n",
    "            ratio = startup_observations[i] /((startup_index[i]+1)*b_0_hat +l_0_hat+epsilon )\n",
    "            globals()[f'S_{q}'].append(ratio)\n",
    "         globals()[f'S_{q}'] = statistics.median(globals()[f'S_{q}'])\n",
    "\n",
    "\n",
    "   else:\n",
    "      for q in range(m):\n",
    "         globals()[f'S_{q}'] = []\n",
    "         for i in range(q, startup_period, m):\n",
    "            difference = startup_observations[i] - ((startup_index[i]+1)*b_0_hat +l_0_hat )\n",
    "            globals()[f'S_{q}'].append(difference)\n",
    "         globals()[f'S_{q}'] = statistics.median(globals()[f'S_{q}'])\n",
    "\n",
    "\n",
    "   seasonal_components = [globals()[f'S_{q}'] for q in range(m)]\n",
    "   return seasonal_components\n",
    "\n",
    "\n",
    "def get_sigma_0(y,seasonality,multiplicative_error,multiplicative_seasonality,startup_period,m,l_0_hat,b_0_hat,integers,min_dif):\n",
    "   \"\"\"Calculates the initial standard deviation estimate (sigma_0_hat) for a time series.\n",
    "\n",
    "   This function computes the initial sigma value based on the presence of seasonality, error type, and other parameters.\n",
    "\n",
    "   Args:\n",
    "       y: The time series data.\n",
    "       seasonality: Boolean indicating the presence of a seasonal component.\n",
    "       multiplicative_error: Boolean indicating if the error is multiplicative.\n",
    "       multiplicative_seasonality: Boolean indicating if the seasonality is multiplicative.\n",
    "       startup_period: The number of observations used to calculate the initial values.\n",
    "       m: The length of one season.\n",
    "       l_0_hat: The initial level estimate.\n",
    "       b_0_hat: The initial slope estimate.\n",
    "       integers: Boolean indicating if the data is integer-valued.\n",
    "       min_dif: Minimum difference to avoid division by zero.\n",
    "\n",
    "   Returns:\n",
    "       float: The initial standard deviation estimate (sigma_0_hat).\n",
    "\n",
    "   Usage:\n",
    "       get_sigma_0(y=[i for i in range(0,50, 3)], seasonality=True, multiplicative_error=True, \n",
    "                   multiplicative_seasonality=False, startup_period=10, m=3, \n",
    "                   l_0_hat=2.0, b_0_hat=0.5, integers=False, min_dif=0.01) = 1.43600\n",
    "   \"\"\"\n",
    "\n",
    "   # sigma_0_hat\n",
    "   startup_observations = y[:startup_period]\n",
    "   epsilon = 1e-6 # To avoid deviding by zero\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "   if seasonality:\n",
    "      if multiplicative_error:\n",
    "         if not multiplicative_seasonality:\n",
    "            error_components =[]\n",
    "            for t in range(startup_period):\n",
    "               \n",
    "               t_tilde = get_t_tilde(t=t,m=m)\n",
    "               called_seasonality = globals()[f'S_{t_tilde}']\n",
    "               error_components.append((startup_observations[t]-l_0_hat-b_0_hat*(t)-called_seasonality)/(l_0_hat-b_0_hat*(t)-called_seasonality+epsilon))\n",
    "            sigma_o_hat = get_MAD(error_components)\n",
    "         else:\n",
    "            error_components =[]\n",
    "            for t in range(startup_period):\n",
    "               \n",
    "               t_tilde = get_t_tilde(t=t,m=m)\n",
    "               called_seasonality = globals()[f'S_{t_tilde}']\n",
    "               error_components.append((startup_observations[t]-(l_0_hat-b_0_hat*(t))/called_seasonality+epsilon)/((l_0_hat-b_0_hat*(t))/called_seasonality+epsilon))\n",
    "            sigma_o_hat = get_MAD(error_components)\n",
    "\n",
    "\n",
    "      else:\n",
    "         if not multiplicative_seasonality:\n",
    "            error_components =[]\n",
    "            for t in range(startup_period):\n",
    "               \n",
    "               t_tilde = get_t_tilde(t=t,m=m)\n",
    "               called_seasonality = globals()[f'S_{t_tilde}']\n",
    "               error_components.append(startup_observations[t]-l_0_hat-b_0_hat*(t)-called_seasonality)\n",
    "            sigma_o_hat = get_MAD(error_components)\n",
    "         else:\n",
    "            error_components =[]\n",
    "            for t in range(startup_period):\n",
    "               \n",
    "               t_tilde = get_t_tilde(t=t,m=m)\n",
    "               called_seasonality = globals()[f'S_{t_tilde}']\n",
    "               error_components.append(startup_observations[t]-(l_0_hat-b_0_hat*(t))/called_seasonality+epsilon)\n",
    "            sigma_o_hat = get_MAD(error_components)\n",
    "   else :\n",
    "      error_components =[]\n",
    "      for t in range(startup_period):\n",
    "         error_components.append((startup_observations[t]-l_0_hat-b_0_hat*(t))/(l_0_hat-b_0_hat*(t)+epsilon))\n",
    "      sigma_o_hat = get_MAD(error_components)\n",
    "   \n",
    "   if integers:\n",
    "      epsilon_sigma = get_min_pairwise_differences(error_components)\n",
    "   else:\n",
    "      epsilon_sigma = 0\n",
    "   return max(sigma_o_hat,epsilon_sigma)\n",
    "   \n",
    "\n",
    "\n",
    "def get_starting_values(y,min_dif,integers,m=None,trend = True ,seasonality = True,multiplicative_seasonality=True,multiplicative_error=True):\n",
    "   \"\"\"Estimates the initial values for the parameters of a time series model.\n",
    "\n",
    "   Because the process is recursive, choosing the initial values of the parameters matters. This function estimates the initial values in a robust way for the effects (level, trend, seasonality) but not the smoothing parameters.\n",
    "\n",
    "   Args:\n",
    "       y: The time series data.\n",
    "       min_dif: Minimum difference to avoid division by zero.\n",
    "       integers: Boolean indicating if the data is integer-valued.\n",
    "       m: The length of one season.\n",
    "       trend: Boolean indicating if a trend component is present.\n",
    "       seasonality: Boolean indicating if a seasonal component is present.\n",
    "       multiplicative_seasonality: Boolean indicating if the seasonality is multiplicative.\n",
    "       multiplicative_error: Boolean indicating if the error is multiplicative.\n",
    "\n",
    "   Returns:\n",
    "       dict: A dictionary containing the initial parameter estimates:\n",
    "           - l_0_hat: The initial level estimate.\n",
    "           - b_0_hat: The initial slope estimate.\n",
    "           - seasonal_components: The initial seasonal components.\n",
    "           - sigma_o_hat: The initial standard deviation estimate.\n",
    "\n",
    "   Usage:\n",
    "       get_starting_values(y=[i for i in range(0,50, 3)], min_dif=0.01, integers=False, m=3, trend=True, \n",
    "                           seasonality=True, multiplicative_seasonality=True, multiplicative_error=True) = \n",
    "                           {'l_0_hat': 0.0,\n",
    "                            'b_0_hat': 3.0,\n",
    "                            'seasonal_components': [0.4615384497041424,\n",
    "                            0.7142856615646331,\n",
    "                            0.7999999525925969,\n",
    "                            0.8437499589843779,\n",
    "                            0.8705881994002327,\n",
    "                            0.8333332870370396,\n",
    "                            0.8571428163265326,\n",
    "                            0.8749999635416682,\n",
    "                            0.8888888559670793,\n",
    "                            0.899999970000001,\n",
    "                            0.9090908815427007,\n",
    "                            0.9166666412037044],\n",
    "                            'sigma_o_hat': 0.06486407566794782}\n",
    "   \"\"\"\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "   \n",
    "   if seasonality and m==None:\n",
    "      raise ValueError(\"Please specify the number of seasons, or set seasonality to False\")\n",
    "   if not seasonality and m!=None :\n",
    "      raise ValueError(\"Seasonality is set to False and m is provided. Either set seasonality to True or m to None\")\n",
    "\n",
    "\n",
    "   startup_period = get_startup_period(y=y,seasonality=seasonality,m=m)\n",
    "   if not trend:\n",
    "      b_0_hat = 0\n",
    "   else :\n",
    "      b_0_hat = get_b_0(startup_period=startup_period,y=y)\n",
    "\n",
    "\n",
    "   l_0_hat = get_l_o(y=y,startup_period=startup_period,b_0_hat=b_0_hat)\n",
    "\n",
    "\n",
    "   if seasonality:\n",
    "      seasonal_components = get_s_0(y=y,multiplicative_seasonality=multiplicative_seasonality,m=m,\n",
    "                              startup_period=startup_period,b_0_hat=b_0_hat,l_0_hat=l_0_hat)\n",
    "   else :\n",
    "      if multiplicative_seasonality:\n",
    "         seasonal_components = 1\n",
    "      else:\n",
    "         seasonal_components = 0\n",
    "\n",
    "\n",
    "   sigma_o_hat = get_sigma_0(y=y,seasonality=seasonality,multiplicative_error=multiplicative_error,multiplicative_seasonality=multiplicative_seasonality,\n",
    "                             startup_period=startup_period,m=m,l_0_hat=l_0_hat,b_0_hat=b_0_hat,integers=integers,min_dif= min_dif)\n",
    "\n",
    "\n",
    "   return {'l_0_hat':l_0_hat,'b_0_hat': b_0_hat,'seasonal_components': seasonal_components,'sigma_o_hat': sigma_o_hat}    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def forecast_equation(y,alpha,lmbda,integers,min_dif,beta = None, gamma = None,m = None,seasonality=True,trend=True,\n",
    "                      ck=4.12, k=3,multiplicative_error=True,multiplicative_seasonality=True):\n",
    "    \"\"\"Forecasts a time series using a robust method with options for trend and seasonality.\n",
    "\n",
    "    Takes smoothing parameters, the type of required model, and the time series data to return a dictionary \n",
    "    with the observations, initial predictions, and clean observations.\n",
    "\n",
    "    The process involves three stages:\n",
    "    1. Initializing variables to store the evolving parameters and getting the starting initial values.\n",
    "    2. Getting sigma_hat values and clean observations.\n",
    "    3. Forecasting using the clean observations and sigma_hats, and updating the records.\n",
    "\n",
    "    Args:\n",
    "        y: The time series data.\n",
    "        alpha: The smoothing parameter for the level.\n",
    "        lmbda: The smoothing parameter for the robust likelihood.\n",
    "        integers: Boolean indicating if the data is integer-valued.\n",
    "        min_dif: Minimum difference to avoid division by zero.\n",
    "        beta: The smoothing parameter for the trend component (if applicable).\n",
    "        gamma: The smoothing parameter for the seasonal component (if applicable).\n",
    "        m: The length of one season (if applicable).\n",
    "        seasonality: Boolean indicating if a seasonal component is present.\n",
    "        trend: Boolean indicating if a trend component is present.\n",
    "        ck: A constant influencing the weight calculation.\n",
    "        k: A constant determining the range of influence.\n",
    "        multiplicative_error: Boolean indicating if the error is multiplicative.\n",
    "        multiplicative_seasonality: Boolean indicating if the seasonality is multiplicative.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the predicted values, observed values, cleaned observations, sigma values, \n",
    "              level components, trend components, and seasonal components.\n",
    "\n",
    "    Usage:\n",
    "        forecast_equation(y=[i for i in range(0,50, 3)], alpha=0.1, lmbda=0.2, integers=False, min_dif=0.01, beta=0.2, \n",
    "                          gamma=0.3, m=3, seasonality=True, trend=True, ck=4.12, k=3, \n",
    "                          multiplicative_error=True, multiplicative_seasonality=True)\n",
    "    \"\"\"\n",
    "\n",
    "    if seasonality:\n",
    "        if gamma == None:\n",
    "            raise ValueError(\"If seasonality is specified, gamma value is required \")\n",
    "    if trend:\n",
    "        if beta == None:\n",
    "            raise ValueError(\"If trend is specified, beta value is required \")\n",
    " \n",
    "    ################\n",
    "    #..First step...\n",
    "    ################\n",
    "    # Get initial value to initiate the recursive algorithm and feed them to the value record\n",
    "    starting_values = get_starting_values(y=y,trend=trend,seasonality=seasonality, m=m, multiplicative_seasonality=multiplicative_seasonality,\n",
    "                                            multiplicative_error=multiplicative_error,integers=integers,min_dif=min_dif)\n",
    "    epsilon = 1e-8\n",
    "    #\n",
    "    # Initiating variables to keep the evolving parameters\n",
    "    l= np.zeros(len(y))\n",
    "    b= np.zeros(len(y))\n",
    "    sigma = np.zeros(len(y))\n",
    "    y_hat = np.zeros(len(y)+1)\n",
    "    if m != None :\n",
    "        s=np.zeros((m,len(y)//m+1))\n",
    "    clean_observations = np.zeros(len(y))\n",
    "    #\n",
    "    # Prepare starting values\n",
    "    if seasonality:\n",
    "        for season in range(m):\n",
    "            s[season,0]=starting_values['seasonal_components'][season]\n",
    "    else:\n",
    "        if multiplicative_seasonality:\n",
    "            s = 1\n",
    "        else:\n",
    "            s = 0\n",
    "    #\n",
    "    l[0] = starting_values['l_0_hat']\n",
    "    if trend:\n",
    "        b[0] = starting_values['b_0_hat']\n",
    "    else:\n",
    "        b = 0\n",
    "    sigma[0] = starting_values['sigma_o_hat']\n",
    "    h = 1   # one step ahead\n",
    "    #\n",
    "    ################\n",
    "    #..Second step..\n",
    "    ################\n",
    "    #\n",
    "    # Updating parameters and getting sigma_hat and clean_observations\n",
    "    if trend:\n",
    "        if seasonality:\n",
    "            if  not multiplicative_seasonality:\n",
    "                # separaing initial values from the values of observation 0\n",
    "                l_0 = l[0]\n",
    "                b_0 = b[0]\n",
    "                s_0 = s[:,0]\n",
    "                sigma_0 = sigma[0]\n",
    "\n",
    "\n",
    "                # Forecasting equation\n",
    "                y_hat[0] = l_0 +   b_0 +   s_0[0]\n",
    "               \n",
    "                sigma_hat = get_sigma_hat(previous_sigma = sigma_0,observed = y[0] ,\n",
    "                                  predicted = y_hat[0] ,  multiplicative_error=multiplicative_error,ck=ck,k=k,lmbda=lmbda,integers=integers,min_dif=min_dif)\n",
    "                sigma[0] = sigma_hat\n",
    "                clean_observation = get_clean_observation(observed = y[0] ,predicted = y_hat[0] ,\n",
    "                                                sigma_hat = sigma_hat , multiplicative_error=multiplicative_error,k=k)\n",
    "                clean_observations[0] = clean_observation\n",
    "\n",
    "\n",
    "                l[0] = alpha * (clean_observations[0] - s_0[0]) + (1-alpha)*(l_0 +   b_0)\n",
    "                b[0] = beta * (l[0] - l_0) + (1-beta) *   b_0\n",
    "                s[0,0] = gamma * (clean_observations[0] - l_0 -   b_0)+ (1-gamma) *s_0[0]\n",
    "                y_hat[1] = l[0] +   b[0] +   s[(1) % m ,max(0,((1) // m)-1)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                for t in range(1,len(y)):\n",
    "                    sigma_hat = get_sigma_hat(previous_sigma = sigma[max(0,t-1)],observed = y[t] ,\n",
    "                                  predicted = y_hat[t] ,  multiplicative_error=multiplicative_error,ck=ck,k=k,lmbda=lmbda,integers=integers,min_dif=min_dif)\n",
    "                    sigma[t] = sigma_hat\n",
    "                    clean_observation = get_clean_observation(observed = y[t] ,predicted = y_hat[t] ,\n",
    "                                                   sigma_hat = sigma_hat , multiplicative_error=multiplicative_error,k=k)\n",
    "                    clean_observations[t] = clean_observation\n",
    "                    season_step = t % m\n",
    "                    season_cycle = t // m\n",
    "                    l[t] = alpha * (clean_observations[t] - s[season_step,max(0,season_cycle-1)]) + (1-alpha)*(l[max(0,t-1)] +   b[max(0,t-1)])\n",
    "                    b[t] = beta * (l[t] - l[max(0,t-1)]) + (1-beta) *   b[max(0,t-1)]\n",
    "                    s[season_step,t // m ] = gamma * (clean_observations[t] - l[max(0,t-1)] -   b[max(0,t-1)])+ (1-gamma) *s[season_step,max(0,season_cycle-1)]\n",
    "                    y_hat[t+h] = l[t] +   b[t] +   s[(t+1) % m ,((t+1) // m)-1]\n",
    "                    #\n",
    "            elif   multiplicative_seasonality:\n",
    "                # separaing initial values from the values of observation 0\n",
    "                l_0 = l[0]\n",
    "                b_0 = b[0]\n",
    "                s_0 = s[:,0]\n",
    "                sigma_0 = sigma[0]\n",
    "\n",
    "\n",
    "                # Forecasting equation\n",
    "                y_hat[0] = (l_0 +   b_0) *   s_0[0]\n",
    "                sigma_hat = get_sigma_hat(previous_sigma = sigma_0,observed = y[0] ,\n",
    "                                  predicted = y_hat[0] ,  multiplicative_error=multiplicative_error,ck=ck,k=k,lmbda=lmbda,integers=integers,min_dif=min_dif)\n",
    "                sigma[0] = sigma_hat\n",
    "\n",
    "\n",
    "                clean_observation = get_clean_observation(observed = y[0] ,predicted = y_hat[0] ,\n",
    "                                                sigma_hat = sigma_hat , multiplicative_error=multiplicative_error,k=k)\n",
    "               \n",
    "                clean_observations[0] = clean_observation\n",
    "\n",
    "\n",
    "                l[0] = alpha * (clean_observations[0]/(s_0[0] + epsilon)) + (1-alpha)*(l_0 +   b_0)\n",
    "                b[0] = beta * (l[0] - l_0) + (1-beta) *   b_0\n",
    "                s[0,0] = gamma * (clean_observations[0]/(l_0 +   b_0+epsilon)) + (1-gamma) *s_0[0]\n",
    "                y_hat[1] = (l[0] +   b[0]) *s[(1) % m ,((1) // m)-1]\n",
    "\n",
    "\n",
    "                for t in range(1,len(y)):\n",
    "                    sigma_hat = get_sigma_hat(previous_sigma = sigma[max(0,t-1)],observed = y[t] ,\n",
    "                                  predicted = y_hat[t] ,  multiplicative_error=multiplicative_error,ck=ck,k=k,lmbda=lmbda,integers=integers,min_dif=min_dif)\n",
    "                    sigma[t] = sigma_hat\n",
    "                    clean_observation = get_clean_observation(observed = y[t] ,predicted = y_hat[t] ,\n",
    "                                                   sigma_hat = sigma_hat , multiplicative_error=multiplicative_error,k=k)\n",
    "                    clean_observations[t] = clean_observation\n",
    "                    season_step = t % m\n",
    "                    season_cycle = t // m\n",
    "                    l[t] = alpha * (clean_observations[t]/(s[season_step,max(0,season_cycle-1)]+epsilon)) + (1-alpha)*(l[max(0,t-1)] +   b[max(0,t-1)])\n",
    "                    b[t] = beta * (l[t] - l[max(0,t-1)]) + (1-beta) *   b[max(0,t-1)]\n",
    "                    s[season_step,t // m ] = gamma * (clean_observations[t]/(l[max(0,t-1)] +   b[max(0,t-1)]+epsilon)) + (1-gamma) *s[season_step,max(0,season_cycle-1)]\n",
    "                    y_hat[t+1] = (l[t] +   b[t]) *s[(t+1) % m ,max(0,((t+1) // m)-1)]\n",
    "        else:\n",
    "            # separaing initial values from the values of observation 0\n",
    "            l_0 = l[0]\n",
    "            b_0 = b[0]\n",
    "            sigma_0 = sigma[0]\n",
    "\n",
    "\n",
    "            # Forecasting equation\n",
    "            y_hat[0] = l_0 +   b_0\n",
    "\n",
    "\n",
    "            sigma_hat = get_sigma_hat(previous_sigma = sigma_0,observed = y[0] ,\n",
    "                                predicted = y_hat[0] ,  multiplicative_error=multiplicative_error,ck=ck,k=k,lmbda=lmbda,integers=integers,min_dif=min_dif)\n",
    "            sigma[0] = sigma_hat\n",
    "            clean_observation = get_clean_observation(observed = y[0] ,predicted = y_hat[0] ,\n",
    "                                            sigma_hat = sigma_hat , multiplicative_error=multiplicative_error,k=k)\n",
    "            clean_observations[0] = clean_observation\n",
    "\n",
    "\n",
    "            l[0] = alpha * (clean_observations[0] ) + (1-alpha)*(l_0 +   b_0)\n",
    "            b[0] = beta * (l[0] - l_0) + (1-beta) *   b_0\n",
    "\n",
    "\n",
    "            y_hat[1] = l[0] +   b[0]\n",
    "\n",
    "\n",
    "            for t in range(1,len(y)):\n",
    "                sigma_hat = get_sigma_hat(previous_sigma = sigma[max(0,t-1)],observed = y[t] ,\n",
    "                                  predicted = y_hat[t] ,  multiplicative_error=multiplicative_error,ck=ck,k=k,lmbda=lmbda,integers=integers,min_dif=min_dif)\n",
    "                sigma[t] = sigma_hat\n",
    "                clean_observation = get_clean_observation(observed = y[t] ,predicted = y_hat[t] ,\n",
    "                                                   sigma_hat = sigma_hat , multiplicative_error=multiplicative_error,k=k)\n",
    "                clean_observations[t] = clean_observation\n",
    "                l[t] = alpha * (clean_observations[t] ) + (1-alpha)*(l[max(0,t-1)] +   b[max(0,t-1)])\n",
    "                b[t] = beta * (l[t] - l[max(0,t-1)]) + (1-beta) *   b[max(0,t-1)]\n",
    "                y_hat[t+h] = l[t] +   b[t]\n",
    "    else:\n",
    "        if seasonality:\n",
    "            if  not multiplicative_seasonality:\n",
    "\n",
    "\n",
    "                # separaing initial values from the values of observation 0\n",
    "                l_0 = l[0]\n",
    "                s_0 = s[:,0]\n",
    "                sigma_0 = sigma[0]\n",
    "\n",
    "\n",
    "                # Forecasting equation\n",
    "                y_hat[0] = l_0 +   s_0[0]\n",
    "               \n",
    "                sigma_hat = get_sigma_hat(previous_sigma = sigma_0,observed = y[0] ,\n",
    "                                  predicted = y_hat[0] ,  multiplicative_error=multiplicative_error,ck=ck,k=k,lmbda=lmbda,integers=integers,min_dif=min_dif)\n",
    "                sigma[0] = sigma_hat\n",
    "                clean_observation = get_clean_observation(observed = y[0] ,predicted = y_hat[0] ,\n",
    "                                                sigma_hat = sigma_hat , multiplicative_error=multiplicative_error,k=k)\n",
    "                clean_observations[0] = clean_observation\n",
    "\n",
    "\n",
    "                l[0] = alpha * (clean_observations[0] - s_0[0]) + (1-alpha)*(l_0 )\n",
    "                s[0,0] = gamma * (clean_observations[0] - l_0 )+ (1-gamma) *s_0[0]\n",
    "                y_hat[1] = l[0] +  s[(1) % m ,max(0,((1) // m)-1)]\n",
    "\n",
    "\n",
    "                for t in range(1,len(y)):\n",
    "                    sigma_hat = get_sigma_hat(previous_sigma = sigma[max(0,t-1)],observed = y[t] ,\n",
    "                                  predicted = y_hat[t] ,  multiplicative_error=multiplicative_error,ck=ck,k=k,lmbda=lmbda,integers=integers,min_dif=min_dif)\n",
    "                    sigma[t] = sigma_hat\n",
    "                    clean_observation = get_clean_observation(observed = y[t] ,predicted = y_hat[t] ,\n",
    "                                                   sigma_hat = sigma_hat , multiplicative_error=multiplicative_error,k=k)\n",
    "                    clean_observations[t] = clean_observation\n",
    "                    season_step = t % m\n",
    "                    season_cycle = t // m\n",
    "                    l[t] = alpha * (clean_observations[t] - s[season_step,max(0,season_cycle-1)]) + (1-alpha)*(l[max(0,t-1)])\n",
    "                    s[season_step,t // m ] = gamma * (clean_observations[t] - l[max(0,t-1)] )+ (1-gamma)* s[season_step,max(0,season_cycle-1)]\n",
    "                    y_hat[t+h] = l[t] +    s[(t+1) % m ,max(0,((t+1) // m)-1)]\n",
    "                    #\n",
    "            elif   multiplicative_seasonality:\n",
    "                # separaing initial values from the values of observation 0\n",
    "                l_0 = l[0]\n",
    "                s_0 = s[:,0]\n",
    "                sigma_0 = sigma[0]\n",
    "\n",
    "\n",
    "                # Forecasting equation\n",
    "                y_hat[0] = l_0  *   s_0[0]\n",
    "                sigma_hat = get_sigma_hat(previous_sigma = sigma_0,observed = y[0] ,\n",
    "                                  predicted = y_hat[0] ,  multiplicative_error=multiplicative_error,ck=ck,k=k,lmbda=lmbda,integers=integers,min_dif=min_dif)\n",
    "                sigma[0] = sigma_hat\n",
    "\n",
    "\n",
    "                clean_observation = get_clean_observation(observed = y[0] ,predicted = y_hat[0] ,\n",
    "                                                sigma_hat = sigma_hat , multiplicative_error=multiplicative_error,k=k)\n",
    "               \n",
    "                clean_observations[0] = clean_observation\n",
    "\n",
    "\n",
    "                l[0] = alpha * (clean_observations[0]/(s_0[0] + epsilon)) + (1-alpha)*(l_0 )\n",
    "                s[0,0] = gamma * (clean_observations[0]/(l_0 +epsilon)) + (1-gamma) *s_0[0]\n",
    "                y_hat[1] = l[0]  *s[(1) % m ,((1) // m)-1]\n",
    "\n",
    "\n",
    "                for t in range(1,len(y)):\n",
    "                    sigma_hat = get_sigma_hat(previous_sigma = sigma[max(0,t-1)],observed = y[t] ,\n",
    "                                  predicted = y_hat[t] ,  multiplicative_error=multiplicative_error,ck=ck,k=k,lmbda=lmbda,integers=integers,min_dif=min_dif)\n",
    "                    sigma[t] = sigma_hat\n",
    "                    clean_observation = get_clean_observation(observed = y[t] ,predicted = y_hat[t] ,\n",
    "                                                   sigma_hat = sigma_hat , multiplicative_error=multiplicative_error,k=k)\n",
    "                    clean_observations[t] = clean_observation\n",
    "                    season_step = t % m\n",
    "                    season_cycle = t // m\n",
    "                    l[t] = alpha * (clean_observations[t]/(s[season_step,max(0,season_cycle-1)]+epsilon)) + (1-alpha)*(l[max(0,t-1)])\n",
    "                    s[season_step,t // m ] = gamma * (clean_observations[t]/(l[max(0,t-1)] + epsilon)) + (1-gamma)* s[season_step,max(0,season_cycle-1)]\n",
    "                    y_hat[t+1] = (l[t] ) * s[(t+1) % m ,max(0,((t+1) // m)-1)]\n",
    "        else:\n",
    "            # separaing initial values from the values of observation 0\n",
    "            l_0 = l[0]\n",
    "            sigma_0 = sigma[0]\n",
    "\n",
    "\n",
    "            # Forecasting equation\n",
    "            y_hat[0] = l_0\n",
    "\n",
    "\n",
    "            sigma_hat = get_sigma_hat(previous_sigma = sigma_0,observed = y[0] ,\n",
    "                                predicted = y_hat[0] ,  multiplicative_error=multiplicative_error,ck=ck,k=k,lmbda=lmbda,integers=integers,min_dif=min_dif)\n",
    "            sigma[0] = sigma_hat\n",
    "            clean_observation = get_clean_observation(observed = y[0] ,predicted = y_hat[0] ,\n",
    "                                            sigma_hat = sigma_hat , multiplicative_error=multiplicative_error,k=k)\n",
    "            clean_observations[0] = clean_observation\n",
    "\n",
    "\n",
    "            l[0] = alpha * (clean_observations[0] ) + (1-alpha)*(l_0 )\n",
    "\n",
    "\n",
    "            y_hat[1] = l[0]\n",
    "\n",
    "\n",
    "            for t in range(1,len(y)):\n",
    "                sigma_hat = get_sigma_hat(previous_sigma = sigma[max(0,t-1)],observed = y[t] ,\n",
    "                                  predicted = y_hat[t] ,  multiplicative_error=multiplicative_error,ck=ck,k=k,lmbda=lmbda,integers=integers,min_dif=min_dif)\n",
    "                sigma[t] = sigma_hat\n",
    "                clean_observation = get_clean_observation(observed = y[t] ,predicted = y_hat[t] ,\n",
    "                                                   sigma_hat = sigma_hat , multiplicative_error=multiplicative_error,k=k)\n",
    "                clean_observations[t] = clean_observation\n",
    "                l[t] = alpha * (clean_observations[t] ) + (1-alpha)*(l[max(0,t-1)] )\n",
    "                y_hat[t+h] = l[t]\n",
    "                #\n",
    "    y_hat = y_hat[:-1] # Taking out the last prediction (corresponds to non-existing observation)\n",
    "    #\n",
    "    ################\n",
    "    #..Third step...\n",
    "    ################\n",
    "    #\n",
    "    # Getting cleaned observations.\n",
    "    # Alternating between updating sigma_hat and a clean observation for every raw observation.\n",
    "    return {\"predicted\":y_hat,\"observed\" : y, \"cleaned\" :clean_observations,\"sigma\":sigma,\"l\":l,\"b\":b,\"s\":s}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_robust_likelihood(params, y, m,trend,seasonality,multiplicative_error, multiplicative_seasonality,integers,min_dif,ck=4.685, k=3):\n",
    "    \"\"\"Fits a smoothing model, obtains predictions, and calculates robust likelihood for optimization and information criterion.\n",
    "    packs the parameters for optimization in one object\n",
    "\n",
    "    Args:\n",
    "        params: The smoothing parameters to be optimized.\n",
    "        y: The time series data.\n",
    "        m: The length of one season.\n",
    "        trend: Boolean indicating if a trend component is present.\n",
    "        seasonality: Boolean indicating if a seasonal component is present.\n",
    "        multiplicative_error: Boolean indicating if the error is multiplicative.\n",
    "        multiplicative_seasonality: Boolean indicating if the seasonality is multiplicative.\n",
    "        integers: Boolean indicating if the data is integer-valued.\n",
    "        min_dif: Minimum difference to avoid division by zero.\n",
    "        ck: A constant influencing the weight calculation.\n",
    "        k: A constant determining the range of influence.\n",
    "\n",
    "    Returns:\n",
    "        float: The negative robust likelihood.\n",
    "\n",
    "    Usage:\n",
    "        get_robust_likelihood(params=[0.2, 0.1, 0.05, 0.1], y=[i for i in range(0,50, 3)], m=3, trend=True, \n",
    "                              seasonality=True, multiplicative_error=True, multiplicative_seasonality=True, \n",
    "                              lmbda=0.2, integers=False, min_dif=0.01, ck=4.685, k=3)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    #\n",
    "    if trend and seasonality:\n",
    "        lmbda, alpha, beta,  gamma  = params\n",
    "        forecasting = forecast_equation(multiplicative_error=multiplicative_error,multiplicative_seasonality=multiplicative_seasonality,\n",
    "                                        y=y,m=m,alpha=alpha,beta=beta, gamma=gamma ,lmbda = lmbda,ck=ck,k=k,seasonality=seasonality,\n",
    "                                        trend=trend,integers=integers,min_dif=min_dif)\n",
    "\n",
    "\n",
    "    #\n",
    "    if trend and not seasonality:\n",
    "        lmbda, alpha, beta  = params\n",
    "        forecasting = forecast_equation(multiplicative_error=multiplicative_error,multiplicative_seasonality=multiplicative_seasonality,\n",
    "                                        y=y,m=m,alpha=alpha,beta=beta, ck=ck,k=k,lmbda =lmbda ,seasonality=seasonality,\n",
    "                                        trend=trend,integers=integers,min_dif=min_dif)\n",
    "\n",
    "\n",
    "    #\n",
    "    if not trend and seasonality:\n",
    "        lmbda, alpha, gamma  = params\n",
    "        forecasting = forecast_equation(multiplicative_error=multiplicative_error,multiplicative_seasonality=multiplicative_seasonality,\n",
    "                                        y=y,m=m,alpha=alpha,ck=ck,k=k, lmbda = lmbda, seasonality=seasonality,gamma=gamma,\n",
    "                                        trend=trend,integers=integers,min_dif=min_dif)\n",
    "    #\n",
    "    if not trend and not seasonality:\n",
    "        lmbda, alpha = params\n",
    "        forecasting = forecast_equation(multiplicative_error=multiplicative_error,multiplicative_seasonality=multiplicative_seasonality,\n",
    "                                        y=y,m=m,alpha=alpha,ck=ck,k=k,lmbda = lmbda,seasonality=seasonality,\n",
    "                                        trend=trend,integers=integers,min_dif=min_dif)\n",
    "\n",
    "\n",
    "    predicted = forecasting['predicted']\n",
    "    observed = forecasting['observed']\n",
    "       \n",
    "    # Residuals are the difference between observed_1 and predicted_2 if error is additive\n",
    "    if not multiplicative_error:\n",
    "        robust_likelihood = rob_lik_A(observed=observed,predicted=predicted,ck=ck,k=k)\n",
    "    # Residuals are the ratio between the difference  and predicted_2 if error is multiplicative\n",
    "    else:\n",
    "        robust_likelihood = rob_lik_M(observed=observed,predicted=predicted,ck=ck,k=k)\n",
    "\n",
    "\n",
    "       \n",
    "    return - robust_likelihood\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_optimized_parameters(y, m,integers,min_dif,seasonality=True,trend=True,multiplicative_error = True,multiplicative_seasonality=True,ck=4.685,k=3,\n",
    "                              method='Nelder-Mead'):  # Ck = 4.685 in Tukey's function | and k = 3\n",
    "    \"\"\"Performs optimization to find the best values of the smoothing parameters.\n",
    "\n",
    "    This function returns the optimal parameters if successful, or raises an error if the optimization fails.\n",
    "\n",
    "    Args:\n",
    "        y: The time series data.\n",
    "        m: The length of one season.\n",
    "        integers: Boolean indicating if the data is integer-valued.\n",
    "        min_dif: Minimum difference to avoid division by zero.\n",
    "        seasonality: Boolean indicating if a seasonal component is present.\n",
    "        trend: Boolean indicating if a trend component is present.\n",
    "        multiplicative_error: Boolean indicating if the error is multiplicative.\n",
    "        multiplicative_seasonality: Boolean indicating if the seasonality is multiplicative.\n",
    "        ck: A constant influencing the weight calculation.\n",
    "        k: A constant determining the range of influence.\n",
    "        method: The optimization method to use.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the optimized smoothing parameters.\n",
    "\n",
    "    Usage:\n",
    "        get_optimized_parameters(y=[i for i in range(0,50, 3)], m=3, integers=False, min_dif=0.01, seasonality=True, \n",
    "                                 trend=True, multiplicative_error=True, multiplicative_seasonality=True, \n",
    "                                 ck=4.685, k=3, method='Nelder-Mead')\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    if trend and seasonality:\n",
    "        initial_guess = [0.5,0.5,0.5,0.5 ]\n",
    "        if len(y)<= len(initial_guess):\n",
    "            raise ValueError('Too few observations were given, model cannot be properly estimated')\n",
    "        up = 0.999\n",
    "        down = 0.001\n",
    "        bounds = [ (down, up), (down, up),(down, up),(down, up)]\n",
    "        args = (y, m,trend,seasonality,multiplicative_error,multiplicative_seasonality,ck,k,integers,min_dif)\n",
    "\n",
    "\n",
    "    if trend and not seasonality:\n",
    "        initial_guess = [0.5,0.5,0.5 ]\n",
    "        if len(y)<= len(initial_guess):\n",
    "            raise ValueError('Too few observations were given, model cannot be properly estimated')\n",
    "        up = 0.999\n",
    "        down = 0.001\n",
    "        bounds = [  (down, up), (down, up), (down, up)]\n",
    "        args = (y, m,trend,seasonality  ,multiplicative_error,multiplicative_seasonality,ck,k,integers,min_dif)\n",
    "\n",
    "\n",
    "    if not trend and seasonality:\n",
    "        initial_guess = [0.5,0.5,0.5]\n",
    "        if len(y)<= len(initial_guess):\n",
    "            raise ValueError('Too few observations were given, model cannot be properly estimated')\n",
    "        up = 0.999\n",
    "        down = 0.001\n",
    "        bounds = [(down, up), (down, up),(down, up)]\n",
    "        args = (y, m,trend,seasonality  ,multiplicative_error,multiplicative_seasonality,ck,k,integers,min_dif)\n",
    "\n",
    "\n",
    "    if not trend and not seasonality:\n",
    "        initial_guess = [0.5,0.5]\n",
    "        if len(y)<= len(initial_guess):\n",
    "            raise ValueError('Too few observations were given, model cannot be properly estimated')\n",
    "        up = 0.999\n",
    "        down = 0.001\n",
    "        bounds = [(down, up),(down, up)]\n",
    "        args = (y, m,trend,seasonality  ,multiplicative_error,multiplicative_seasonality,ck,k,integers,min_dif)\n",
    "\n",
    "\n",
    "    result = minimize(fun=get_robust_likelihood, args=args,\n",
    "                      x0=initial_guess, method=method, bounds = bounds ,options={'maxiter': 10000})\n",
    "    optimum = {\"lmbda\":None,\"alpha\":None,\"beta\":None,\n",
    "                       \"gamma\":None}\n",
    "    if result.success:\n",
    "        if trend and seasonality:\n",
    "            optimum['lmbda'] =result.x[0]\n",
    "            optimum['alpha'] = result.x[1]\n",
    "            optimum['beta'] = result.x[2]\n",
    " \n",
    "            optimum['gamma'] = result.x[3]\n",
    "        if trend and not seasonality:\n",
    "            optimum['lmbda'] =result.x[0]\n",
    "            optimum['alpha'] = result.x[1]\n",
    "            optimum['beta'] = result.x[2]\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if not trend and seasonality:\n",
    "            optimum['lmbda'] =result.x[0]\n",
    "            optimum['alpha'] = result.x[1]\n",
    "            optimum['gamma'] = result.x[2]\n",
    "\n",
    "\n",
    "        if not trend and not seasonality:\n",
    "            optimum['lmbda'] =result.x[0]\n",
    "            optimum['alpha'] = result.x[1]\n",
    "\n",
    "\n",
    "        return  optimum # Return the optimized parameters\n",
    "    else:\n",
    "        raise ValueError(\"Optimization failed: \" + result.message)\n",
    "   \n",
    "\n",
    "\n",
    "def fit_model(seasonality,trend,multiplicative_error,multiplicative_seasonality,y,m,integers,min_dif,ck=4.685, k=3,initial_guess = [0.8,0.4,0.8,0.4],method='Nelder-Mead'):\n",
    "    \"\"\"Runs the optimization based on the specified model type, \n",
    "    and uses those optimal parameters in the forecasting.\n",
    "\n",
    "    Args:\n",
    "        seasonality: Boolean indicating if a seasonal component is present.\n",
    "        trend: Boolean indicating if a trend component is present.\n",
    "        multiplicative_error: Boolean indicating if the error is multiplicative.\n",
    "        multiplicative_seasonality: Boolean indicating if the seasonality is multiplicative.\n",
    "        y: The time series data.\n",
    "        m: The length of one season.\n",
    "        integers: Boolean indicating if the data is integer-valued.\n",
    "        min_dif: Minimum difference to avoid division by zero.\n",
    "        ck: A constant influencing the weight calculation.\n",
    "        k: A constant determining the range of influence.\n",
    "        initial_guess: Initial guess for the smoothing parameters.\n",
    "        method: The optimization method to use.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the robust likelihood, observed values, cleaned observations, predicted values, \n",
    "              residuals, sigma values, level components, trend components, and seasonal components, along with optimized parameters.\n",
    "\n",
    "    Usage:\n",
    "        fit_model(seasonality=True, trend=True, multiplicative_error=True, multiplicative_seasonality=True, \n",
    "                  y=[i for i in range(0,50, 3)], m=3, integers=False, min_dif=0.01, ck=4.685, k=3, \n",
    "                  initial_guess=[0.8, 0.4, 0.8, 0.4], method='Nelder-Mead')\n",
    "    \"\"\"\n",
    "\n",
    "    if seasonality and trend:\n",
    "        initial_smoothing_parameters = get_optimized_parameters(y = y, m = m,trend=trend, seasonality=seasonality,multiplicative_error = multiplicative_error,\n",
    "                                                            multiplicative_seasonality=multiplicative_seasonality,ck=ck,k=k,\n",
    "                                                            method=method,integers=integers,min_dif=min_dif)\n",
    "        alpha = initial_smoothing_parameters['alpha']\n",
    "        beta = initial_smoothing_parameters['beta']\n",
    " \n",
    "        gamma = initial_smoothing_parameters['gamma']\n",
    "        lmbda = initial_smoothing_parameters['lmbda']\n",
    "        opt_parameters = {'alpha':alpha,'beta':beta, 'gamma':gamma,'lmbda':lmbda}\n",
    "\n",
    "\n",
    "        # First casting phase with raw observations to obtain clean observations\n",
    "        forecasting = forecast_equation(trend=trend, seasonality=seasonality,multiplicative_error=multiplicative_error,multiplicative_seasonality=multiplicative_seasonality,\n",
    "                                        y=y,m=m,alpha=alpha,beta=beta, lmbda = lmbda, gamma=gamma,ck=ck,k=k,integers=integers,min_dif=min_dif)\n",
    "        predicted = forecasting['predicted']\n",
    "        cleaned = forecasting['cleaned']\n",
    "        observed = forecasting['observed']\n",
    "       \n",
    "       \n",
    "    if not seasonality and trend:\n",
    "        initial_smoothing_parameters = get_optimized_parameters(y = y, m = m,trend=trend, seasonality=seasonality,multiplicative_error = multiplicative_error,\n",
    "                                                    multiplicative_seasonality=multiplicative_seasonality,ck=ck,k=k,\n",
    "                                                    method=method,integers=integers,min_dif=min_dif)\n",
    "        alpha = initial_smoothing_parameters['alpha']\n",
    "        beta = initial_smoothing_parameters['beta']\n",
    " \n",
    "        gamma = None\n",
    "        lmbda = initial_smoothing_parameters['lmbda']\n",
    "        opt_parameters = {'alpha':alpha,'beta':beta, 'gamma':gamma,'lmbda':lmbda}\n",
    "\n",
    "\n",
    "        # First casting phase with raw observations to obtain clean observations\n",
    "        forecasting = forecast_equation(trend=trend, seasonality=seasonality,multiplicative_error=multiplicative_error,multiplicative_seasonality=multiplicative_seasonality,\n",
    "                                        y=y,m=m,alpha=alpha,beta=beta, lmbda = lmbda , gamma=gamma,ck=ck,k=k,integers=integers,min_dif=min_dif)\n",
    "        predicted = forecasting['predicted']\n",
    "        cleaned = forecasting['cleaned']\n",
    "        observed = forecasting['observed']\n",
    "       \n",
    "    if seasonality and not trend:\n",
    "        initial_smoothing_parameters = get_optimized_parameters(y = y, m = m,trend=trend, seasonality=seasonality,multiplicative_error = multiplicative_error,\n",
    "                                                    multiplicative_seasonality=multiplicative_seasonality,ck=ck,k=k,\n",
    "                                                    method=method,integers=integers,min_dif=min_dif)\n",
    "        alpha = initial_smoothing_parameters['alpha']\n",
    "        beta = None\n",
    " \n",
    "        gamma = initial_smoothing_parameters['gamma']\n",
    "        lmbda = initial_smoothing_parameters['lmbda']\n",
    "\n",
    "\n",
    "        opt_parameters = {'alpha':alpha,'beta':beta, 'gamma':gamma,'lmbda':lmbda}\n",
    "\n",
    "\n",
    "        # First casting phase with raw observations to obtain clean observations\n",
    "        forecasting = forecast_equation(trend=trend, seasonality=seasonality,multiplicative_error=multiplicative_error,multiplicative_seasonality=multiplicative_seasonality,\n",
    "                                        y=y,m=m,alpha=alpha,beta=beta, lmbda = lmbda, gamma=gamma,ck=ck,k=k,integers=integers,min_dif=min_dif)\n",
    "        predicted = forecasting['predicted']\n",
    "        cleaned = forecasting['cleaned']\n",
    "        observed = forecasting['observed']\n",
    "       \n",
    "    if not seasonality and not trend:\n",
    "        initial_smoothing_parameters = get_optimized_parameters(y = y, m = m,trend=trend, seasonality=seasonality,multiplicative_error = multiplicative_error,\n",
    "                                                    multiplicative_seasonality=multiplicative_seasonality,ck=ck,k=k,\n",
    "                                                    method=method,integers=integers,min_dif=min_dif)\n",
    "        alpha = initial_smoothing_parameters['alpha']\n",
    "        beta = None\n",
    " \n",
    "        gamma = None\n",
    "        lmbda = initial_smoothing_parameters['lmbda']\n",
    "\n",
    "\n",
    "        opt_parameters = {'alpha':alpha,'beta':beta, 'gamma':gamma,'lmbda':lmbda}\n",
    "\n",
    "\n",
    "        # First casting phase with raw observations to obtain clean observations\n",
    "        forecasting = forecast_equation(trend=trend, seasonality=seasonality,multiplicative_error=multiplicative_error,multiplicative_seasonality=multiplicative_seasonality,\n",
    "                                        y=y,m=m,alpha=alpha,beta=beta,  lmbda = lmbda , gamma=gamma,ck=ck,k=k,integers=integers,min_dif=min_dif)\n",
    "        predicted = forecasting['predicted']\n",
    "        cleaned = forecasting['cleaned']\n",
    "        observed = forecasting['observed']\n",
    "\n",
    "\n",
    "    sigma = forecasting['sigma']\n",
    "    l = forecasting['l']\n",
    "    b = forecasting['b']\n",
    "    s = forecasting['s']\n",
    "    # Residuals are the difference between observed_1 and predicted_2 if error is additive\n",
    "    if not multiplicative_error:\n",
    "        residuals = observed - predicted\n",
    "        robust_likelihood = rob_lik_A(observed=observed,predicted=predicted,ck=ck,k=k)\n",
    "    # Residuals are the ratio between the difference  and predicted_2 if error is multiplicative\n",
    "    else:\n",
    "        residuals = get_multiplicative_error(observed=observed,predicted=predicted)\n",
    "        robust_likelihood = rob_lik_M(observed=observed,predicted=predicted,ck=ck,k=k)\n",
    "       \n",
    "    return {\"robust_likelihood\": robust_likelihood,\"observed\":observed,\"cleaned\":cleaned,\n",
    "            \"predicted\":predicted,\"residuals\":residuals,\"sigma\":sigma,\n",
    "            \"l\":l,\"b\":b,\"s\":s,'opt_parameters':opt_parameters}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_rob_IC(y,m,trend,seasonality,multiplicative_error,multiplicative_seasonality,min_dif,integers,method='Nelder-Mead',ck=4.685, k=3):\n",
    "    \"\"\"Computes robust information criteria for a time series model.\n",
    "\n",
    "    This function takes model specifications, runs an optimization process to obtain optimal parameters using the specified method,\n",
    "    and then computes the robust likelihood based on the model with optimal parameters. From the obtained robust likelihood, it \n",
    "    computes the robust information criteria.\n",
    "\n",
    "    Args:\n",
    "        y: The time series data.\n",
    "        m: The length of one season.\n",
    "        trend: Boolean indicating if a trend component is present.\n",
    "        seasonality: Boolean indicating if a seasonal component is present.\n",
    "        multiplicative_error: Boolean indicating if the error is multiplicative.\n",
    "        multiplicative_seasonality: Boolean indicating if the seasonality is multiplicative.\n",
    "        min_dif: Minimum difference to avoid division by zero.\n",
    "        integers: Boolean indicating if the data is integer-valued.\n",
    "        method: The optimization method to use.\n",
    "        ck: A constant influencing the weight calculation.\n",
    "        k: A constant determining the range of influence.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the number of parameters, number of observations, robust likelihood, robust AIC, \n",
    "              robust BIC, robust AICc, optimized parameters, and the optimization method used.\n",
    "\n",
    "    Usage:\n",
    "        get_rob_IC(y=[i for i in range(0,50, 3)], m=3, trend=True, seasonality=True, multiplicative_error=True, \n",
    "                   multiplicative_seasonality=True, min_dif=0.01, integers=False, method='Nelder-Mead', \n",
    "                   ck=4.685, k=3)\n",
    "    \"\"\"\n",
    "\n",
    "    robust_likelihood_model = fit_model(multiplicative_error = multiplicative_error,multiplicative_seasonality = multiplicative_seasonality,\n",
    "                                  y = y,m = m,ck = ck,k =k,trend=trend,seasonality=seasonality,integers=integers,min_dif=min_dif)\n",
    "    opt_parameters = robust_likelihood_model['opt_parameters']\n",
    "    robust_likelihood = robust_likelihood_model['robust_likelihood']\n",
    "    p = len([ i for i in opt_parameters.keys() if opt_parameters[i] != None])\n",
    "    n = len(y)\n",
    "    robAIC = -2 * robust_likelihood + 2*p\n",
    "    robBIC = -2 * robust_likelihood + math.log(n) * p\n",
    "    robAICc = -2 * robust_likelihood + 2 * ((p * n)/(n - p -1))\n",
    "    return {\"n_parameters\":p,\"n_observations\":n,'robust_likelihood':robust_likelihood,\"robAIC\":robAIC,\"robBIC\":robBIC,\"robAICc\":robAICc,\"opt_parameters\":opt_parameters,'method':method}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def find_best_model(y,m,integers,min_dif,method='Nelder-Mead',ck=4.685, k=3,IC='robAICc'):\n",
    "    \"\"\"Finds the best time series model based on the robust information criterion.\n",
    "\n",
    "    This function evaluates various combinations of additive and multiplicative error and seasonality models,\n",
    "    obtains the robust Information Criterion (IC) for each model, and picks the model with the best (lowest) IC.\n",
    "\n",
    "    Args:\n",
    "        y: The time series data.\n",
    "        m: The length of one season.\n",
    "        integers: Boolean indicating if the data is integer-valued.\n",
    "        min_dif: Minimum difference to avoid division by zero.\n",
    "        method: The optimization method to use.\n",
    "        ck: A constant influencing the weight calculation.\n",
    "        k: A constant determining the range of influence.\n",
    "        IC: The information criterion to use for model selection ('robAICc' by default).\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing information about the best model, including robust likelihood, number of parameters,\n",
    "              number of observations, robust AIC, robust BIC, robust AICc, optimal parameters, method used, and model components.\n",
    "\n",
    "    Usage:\n",
    "        find_best_model(y=[i for i in range(0,50, 3)], m=3, integers=False, min_dif=0.01, method='Nelder-Mead', \n",
    "                        ck=4.685, k=3, IC='robAICc')\n",
    "    \"\"\"\n",
    "\n",
    "    multiplicative_error_multiplicative_seasonality_trend = get_rob_IC(trend=True,seasonality=True,y =y,m =m,multiplicative_error=True,\n",
    "                                                                 multiplicative_seasonality=True , method=method,ck=ck,k=k,integers=integers,min_dif=min_dif)\n",
    "    multiplicative_error_additive_seasonality_trend = get_rob_IC(trend=True,seasonality=True,y =y,m =m,multiplicative_error=True,\n",
    "                                                           multiplicative_seasonality=False, method=method,ck=ck,k=k,integers=integers,min_dif=min_dif)\n",
    "    additive_error_multiplicative_seasonality_trend = get_rob_IC(trend=True,seasonality=True,y =y,m =m,multiplicative_error=False,\n",
    "                                                           multiplicative_seasonality=True, method=method,ck=ck,k=k,integers=integers,min_dif=min_dif)\n",
    "    additive_error_additive_seasonality_trend = get_rob_IC(trend=True,seasonality=True,y =y,m =m,multiplicative_error=False,\n",
    "                                                     multiplicative_seasonality=False ,method=method,ck=ck,k=k,integers=integers,min_dif=min_dif)\n",
    "    multiplicative_error_multiplicative_seasonality = get_rob_IC(trend=False,seasonality=True,y =y,m =m,multiplicative_error=True,\n",
    "                                                                multiplicative_seasonality=True , method=method,ck=ck,k=k,\n",
    "                                                                integers=integers,min_dif=min_dif)\n",
    "    multiplicative_error_additive_seasonality = get_rob_IC(trend=False,seasonality=True,y =y,m =m,multiplicative_error=True,\n",
    "                                                           multiplicative_seasonality=False, method=method,ck=ck,k=k,integers=integers,min_dif=min_dif)\n",
    "    additive_error_multiplicative_seasonality = get_rob_IC(trend=False,seasonality=True,y =y,m =m,multiplicative_error=False,\n",
    "                                                           multiplicative_seasonality=True, method=method,ck=ck,k=k,integers=integers,min_dif=min_dif)\n",
    "    additive_error_additive_seasonality = get_rob_IC(trend=False,seasonality=True,y =y,m =m,multiplicative_error=False,\n",
    "                                                     multiplicative_seasonality=False ,method=method,ck=ck,k=k,integers=integers,min_dif=min_dif)\n",
    "   \n",
    "    additive_error_trend = get_rob_IC(trend=True,seasonality=False,y =y,m =None,multiplicative_error=False,\n",
    "                                                     multiplicative_seasonality=False ,method=method,ck=ck,k=k,integers=integers,min_dif=min_dif)\n",
    "    multiplicative_error_trend = get_rob_IC(trend=True,seasonality=False,y =y,m =None,multiplicative_error=True,\n",
    "                                                     multiplicative_seasonality=False ,method=method,ck=ck,k=k,integers=integers,min_dif=min_dif)\n",
    "    additive_error = get_rob_IC(trend=False,seasonality=False,y =y,m =None,multiplicative_error=False,\n",
    "                                                     multiplicative_seasonality=False ,method=method,ck=ck,k=k,integers=integers,min_dif=min_dif)\n",
    "    mmultiplicative_error = get_rob_IC(trend=False,seasonality=False,y =y,m =None,multiplicative_error=True,\n",
    "                                                     multiplicative_seasonality=False ,method=method,ck=ck,k=k,integers=integers,min_dif=min_dif)\n",
    "   \n",
    "    compare_robAICc = {\"multiplicative_error_multiplicative_seasonality_trend\":multiplicative_error_multiplicative_seasonality_trend[IC],\n",
    "         \"multiplicative_error_additive_seasonality_trend\":multiplicative_error_additive_seasonality_trend[IC],\n",
    "         \"additive_error_multiplicative_seasonality_trend\":additive_error_multiplicative_seasonality_trend[IC],\n",
    "         \"additive_error_additive_seasonality_trend\":additive_error_additive_seasonality_trend[IC],\n",
    "         \"multiplicative_error_multiplicative_seasonality\":multiplicative_error_multiplicative_seasonality[IC],\n",
    "         \"multiplicative_error_additive_seasonality\":multiplicative_error_additive_seasonality[IC],\n",
    "         \"additive_error_multiplicative_seasonality\":additive_error_multiplicative_seasonality[IC],\n",
    "         \"additive_error_additive_seasonality\":additive_error_additive_seasonality[IC],\n",
    "         \"additive_error_trend\":additive_error_trend[IC],\n",
    "         \"multiplicative_error_trend\":multiplicative_error_trend[IC],\n",
    "         \"additive_error\":additive_error[IC],\n",
    "         \"mmultiplicative_error\":mmultiplicative_error[IC]}\n",
    "   \n",
    "    compare_model = {\"multiplicative_error_multiplicative_seasonality_trend\":multiplicative_error_multiplicative_seasonality_trend,\n",
    "         \"multiplicative_error_additive_seasonality_trend\":multiplicative_error_additive_seasonality_trend,\n",
    "         \"additive_error_multiplicative_seasonality_trend\":additive_error_multiplicative_seasonality_trend,\n",
    "         \"additive_error_additive_seasonality_trend\":additive_error_additive_seasonality_trend,\n",
    "         \"multiplicative_error_multiplicative_seasonality\":multiplicative_error_multiplicative_seasonality,\n",
    "         \"multiplicative_error_additive_seasonality\":multiplicative_error_additive_seasonality,\n",
    "         \"additive_error_multiplicative_seasonality\":additive_error_multiplicative_seasonality,\n",
    "         \"additive_error_additive_seasonality\":additive_error_additive_seasonality,\n",
    "         \"additive_error_trend\":additive_error_trend,\n",
    "         \"multiplicative_error_trend\":multiplicative_error_trend,\n",
    "         \"additive_error\":additive_error,\n",
    "         \"mmultiplicative_error\":mmultiplicative_error}\n",
    "    best_model_name = str(min(compare_robAICc,key=compare_robAICc.get))\n",
    "    best_model = compare_model[best_model_name]\n",
    "\n",
    "\n",
    "    if best_model_name.find(\"multiplicative_error\") == -1:\n",
    "        multiplicative_error = False\n",
    "    else:\n",
    "        multiplicative_error = True\n",
    "   \n",
    "    if best_model_name.find(\"multiplicative_seasonality\") == -1:\n",
    "        multiplicative_seasonality = False\n",
    "    else:\n",
    "        multiplicative_seasonality = True\n",
    "    if best_model_name.find(\"seasonality\") == -1:\n",
    "        seasonality = False\n",
    "    else:\n",
    "        seasonality = True\n",
    "\n",
    "\n",
    "    if best_model_name.find(\"trend\")== -1:\n",
    "        trend = False\n",
    "    else:\n",
    "        trend = True\n",
    "\n",
    "\n",
    "    optimum = best_model['opt_parameters']\n",
    "\n",
    "\n",
    "    robAIC = best_model['robAIC']\n",
    "    robBIC = best_model['robBIC']\n",
    "    robAICc = best_model['robAICc']\n",
    "    p = best_model['n_parameters']\n",
    "    n = best_model['n_observations']\n",
    "    robust_likelihood = best_model['robust_likelihood']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return {\"best_model\":best_model_name,'robust_likelihood':robust_likelihood,\n",
    "            \"n_parameters\":p,'n_observations':n,\"robAIC\":robAIC,\"robBIC\":robBIC,\n",
    "            \"robAICc\":robAICc,\"optimal_parameters\":optimum,\"method\":method,\n",
    "            'multiplicative_error':multiplicative_error,'multiplicative_seasonality':multiplicative_seasonality,\n",
    "            \"seasonality\":seasonality,\"trend\":trend}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_confidence_interval(observed,predicted,sigma,multiplicative_error,a):\n",
    "    \"\"\"Computes the confidence interval around expected values.\n",
    "\n",
    "    This function calculates confidence intervals for each predicted value in a time series, taking into account whether \n",
    "    the error is multiplicative or additive. It also identifies anomalies that fall outside the computed confidence interval.\n",
    "\n",
    "    Args:\n",
    "        observed: The observed values.\n",
    "        predicted: The predicted values.\n",
    "        sigma: The standard deviation values.\n",
    "        multiplicative_error: Boolean indicating if the error is multiplicative.\n",
    "        a: The confidence level.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the prediction interval, lower bound, upper bound, anomalies indices, and certainty.\n",
    "\n",
    "    Usage:\n",
    "        get_confidence_interval(observed=[10, 15, 20], predicted=[9, 14, 19], sigma=[0.5, 0.5, 0.5], \n",
    "                                multiplicative_error=True, a=0.05)\n",
    "    \"\"\"\n",
    "\n",
    "    epsilon = 1e-8\n",
    "    # Calculate the cumulative probability\n",
    "    cumulative_probability = 1 - a / 2\n",
    "    # Get the z-score corresponding to the cumulative probability\n",
    "    q = stats.norm.ppf(cumulative_probability)\n",
    "    prediction_interval = []\n",
    "    lower_bound = []\n",
    "    upper_bound = []\n",
    "    each_certainty = {}\n",
    "    anomalies_indecies = {}\n",
    "\n",
    "\n",
    "    if not multiplicative_error:\n",
    "        for i in range(len(observed)):\n",
    "            lower = predicted[i] - q * sigma[i]\n",
    "            upper = predicted[i] + q * sigma[i]\n",
    "            entry = [lower,upper]\n",
    "            prediction_interval.append(entry)\n",
    "            lower_bound.append(lower)\n",
    "            upper_bound.append(upper)\n",
    "           \n",
    "            if observed[i]>upper :\n",
    "                certainty = (abs(observed[i]-predicted[i])/(sigma[i]+epsilon))*10\n",
    "                each_certainty[i]=certainty\n",
    "                certainty = '( ' + str(round(certainty,0)) + '%' + ' )'\n",
    "                anomalies_indecies[i]=\"too high\" + certainty\n",
    "               \n",
    "            if observed[i]<lower:\n",
    "                certainty = (abs(observed[i]-predicted[i])/(sigma[i]+epsilon))*10\n",
    "                each_certainty[i]=certainty\n",
    "                certainty = '( ' + str(round(certainty,0)) + '%' + ' )'\n",
    "                anomalies_indecies[i]=\"too low\" + certainty\n",
    "   \n",
    "    else:\n",
    "        for i in range(len(observed)):\n",
    "            lower = predicted[i] * ( 1 - q * sigma[i])\n",
    "            upper = predicted[i] * ( 1 + q * sigma[i])\n",
    "            entry = [lower,upper]\n",
    "            prediction_interval.append(entry)\n",
    "            lower_bound.append(lower)\n",
    "            upper_bound.append(upper)\n",
    "           \n",
    "            if observed[i]>upper :\n",
    "                certainty = (abs((observed[i]-predicted[i])/(predicted[i]+epsilon))/(sigma[i]+epsilon))*10\n",
    "                each_certainty[i]=certainty\n",
    "                certainty = '( ' + str(round(certainty,0)) + '%' + ' )'\n",
    "                anomalies_indecies[i]=\"too high\" + certainty\n",
    "            if observed[i]<lower:\n",
    "                certainty = (abs((observed[i]-predicted[i])/(epsilon+predicted[i]))/(epsilon+sigma[i]))*10\n",
    "                each_certainty[i]=certainty\n",
    "                certainty = '( ' + str(round(certainty,0)) + '%' + ' )'\n",
    "                anomalies_indecies[i]=\"too low\" + certainty\n",
    "    return {'prediction_interval':prediction_interval,'lower_bound':lower_bound,'upper_bound':upper_bound,'anomalies_indecies':anomalies_indecies,'certainty':each_certainty}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_prediction_intervals(y,m,integers,min_dif,method='Nelder-Mead',a = 0.05,ck=4.685, k=3,\n",
    "                             multiplicative_error = None,seasonality=None,trend = None,multiplicative_seasonality = None,IC='robAICc'):\n",
    "    \"\"\"Finds the best model and computes prediction intervals.\n",
    "\n",
    "    This function identifies the best model in terms of additive or multiplicative seasonality and error based on the AICc,\n",
    "    takes predictions from that model, and uses the error scale term to obtain a confidence interval around each prediction.\n",
    "\n",
    "    Args:\n",
    "        y: The time series data.\n",
    "        m: The length of one season.\n",
    "        integers: Boolean indicating if the data is integer-valued.\n",
    "        min_dif: Minimum difference to avoid division by zero.\n",
    "        method: The optimization method to use.\n",
    "        a: The confidence level.\n",
    "        ck: A constant influencing the weight calculation.\n",
    "        k: A constant determining the range of influence.\n",
    "        multiplicative_error: Boolean indicating if the error is multiplicative.\n",
    "        seasonality: Boolean indicating if a seasonal component is present.\n",
    "        trend: Boolean indicating if a trend component is present.\n",
    "        multiplicative_seasonality: Boolean indicating if the seasonality is multiplicative.\n",
    "        IC: The information criterion to use for model selection ('robAICc' by default).\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the prediction interval, lower bound, upper bound, best model summary, \n",
    "              best model log, anomalies indices, and certainty.\n",
    "\n",
    "    Usage:\n",
    "        get_prediction_intervals(y=[i for i in range(0,50, 3)], m=3, integers=False, min_dif=0.01, method='Nelder-Mead', \n",
    "                                 a=0.05, ck=4.685, k=3, multiplicative_error=None, seasonality=None, \n",
    "                                 trend=None, multiplicative_seasonality=None, IC='robAICc')\n",
    "    \"\"\"\n",
    "\n",
    "    if multiplicative_error != None or seasonality != None or trend != None or multiplicative_seasonality != None :\n",
    "        best_model = {\"multiplicative_error\":multiplicative_error,\"seasonality\":seasonality,\"trend\":trend,\n",
    "                      \"multiplicative_seasonality\":multiplicative_seasonality}\n",
    "       \n",
    "    else:\n",
    "        # Identifying the best model giving the data we have\n",
    "        best_model = find_best_model(y=y,m=m,method=method,ck=ck,k=k,IC=IC,integers=integers,min_dif=min_dif)\n",
    "        multiplicative_error = best_model['multiplicative_error']\n",
    "        seasonality = best_model['seasonality']\n",
    "        trend = best_model['trend']\n",
    "        multiplicative_seasonality = best_model['multiplicative_seasonality']\n",
    "       \n",
    "    if not seasonality:\n",
    "        m = None\n",
    "    # Fitting the best model\n",
    "    fitted_model = fit_model(multiplicative_error=multiplicative_error,multiplicative_seasonality = multiplicative_seasonality,\n",
    "                             y = y,m = m,ck=ck,k=k,trend=trend,seasonality=seasonality,integers=integers,min_dif=min_dif)\n",
    "    observed = fitted_model['observed']\n",
    "    predicted = fitted_model['predicted']\n",
    "    sigma = fitted_model['sigma']\n",
    "\n",
    "\n",
    "    confidence_interval = get_confidence_interval(observed=observed,predicted=predicted,sigma=sigma,multiplicative_error=multiplicative_error,a=a)\n",
    "    prediction_interval = confidence_interval['prediction_interval']\n",
    "    lower_bound = confidence_interval['lower_bound']\n",
    "    upper_bound = confidence_interval['upper_bound']\n",
    "    anomalies_indecies = confidence_interval['anomalies_indecies']\n",
    "    certainty = confidence_interval['certainty']\n",
    "\n",
    "    return {'prediction_interval':prediction_interval,'lower_bound':lower_bound,\n",
    "            'upper_bound':upper_bound, 'best_model_summary':best_model,\n",
    "            'best_model_log':fitted_model,\"anomalies_indecies\":anomalies_indecies,'certainty':certainty}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot(interval,a = 0.05):\n",
    "    \"\"\"Plots the observed data, prediction interval, and anomalies.\n",
    "\n",
    "    This function takes the prediction interval object and plots the observed data, the prediction interval,\n",
    "    and the anomalies.\n",
    "\n",
    "    Args:\n",
    "        interval: The prediction interval object containing the best model summary and log.\n",
    "        a: The confidence level.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Usage:\n",
    "        plot(interval=interval_object, a=0.05)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    multiplicative_error = interval['best_model_summary']['multiplicative_error']\n",
    "    observed = interval['best_model_log']['observed']\n",
    "    predicted = interval['best_model_log']['predicted']\n",
    "    sigma = interval['best_model_log']['sigma']\n",
    "\n",
    "\n",
    "    confidence_interval = get_confidence_interval(a=a,multiplicative_error=multiplicative_error, sigma=sigma,observed = observed,predicted = predicted)\n",
    "    lower_bound = confidence_interval['lower_bound']\n",
    "    upper_bound = confidence_interval['upper_bound']\n",
    "    anomalies_indecies = confidence_interval['anomalies_indecies']\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'Observed': observed,\n",
    "        'Predicted': predicted,\n",
    "        'Lower Bound': lower_bound,\n",
    "        'Upper Bound': upper_bound})\n",
    "    anomaly_index = anomalies_indecies\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(df['Observed'], label='Observed', color='blue')\n",
    "    plt.plot(df['Predicted'], label='Predicted', color='green', linestyle='--')\n",
    "\n",
    "\n",
    "    # Fill between for confidence interval\n",
    "    plt.fill_between(df.index, df['Lower Bound'], df['Upper Bound'], color='gray', alpha=0.3, label='Confidence Interval')\n",
    "\n",
    "\n",
    "    # Highlight the anomaly\n",
    "    # Note: Ensure your anomaly_index aligns with how your data's index is structured\n",
    "    plt.scatter(anomaly_index, df.iloc[anomaly_index]['Observed'], color='red', zorder=5, label='Anomaly')\n",
    "    #plt.scatter(anomaly_index, df.loc[anomaly_index,'Observed'], color='red', zorder=5, label='Anomaly')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    plt.title('Time Series Forecast')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def get_bidirectional_anomalies(anomalies_dictionary_bidimentional_raw,anomalies_certainty_bidimentional_raw,n_small):\n",
    "    \"\"\"Identifies bidirectional anomalies and classifies them as isolated or patchy.\n",
    "\n",
    "    This function processes anomalies from a bidirectional perspective, classifying each as either isolated or patchy,\n",
    "    based on their position and certainty. It avoids double-counting by skipping certain indices as necessary.\n",
    "\n",
    "    Args:\n",
    "        anomalies_dictionary_bidimentional_raw: A dictionary of anomalies with their indices and types.\n",
    "        anomalies_certainty_bidimentional_raw: A dictionary of anomalies with their indices and certainty levels.\n",
    "        n_small: The length of the original time series (forward pass)\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the classified anomalies, including their indices, types, and certainty levels.\n",
    "\n",
    "    Usage:\n",
    "        get_bidirectional_anomalies(anomalies_dictionary_bidimentional_raw=anomalies_dict, \n",
    "                                    anomalies_certainty_bidimentional_raw=certainties_dict, n_small=10)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    anomalies_indecies_numbers_raw = [i for i in anomalies_dictionary_bidimentional_raw.keys()]\n",
    "    anomalies_certainty_raw = [i for i in anomalies_certainty_bidimentional_raw.values()]\n",
    "    anomalies_indecies_numbers = []\n",
    "    anomalies_indecies_types = []\n",
    "    anomalies_indecies_certainty = []\n",
    "    anomalies_indecies_certainty_total = {}\n",
    "    anomalies_indecies_numbers_total = {}\n",
    "    patchy_skip = int()\n",
    "    for v in anomalies_indecies_numbers_raw:\n",
    "        if patchy_skip == v-1:\n",
    "            patchy_skip = v\n",
    "            continue\n",
    "        if v < n_small:\n",
    "            backward_pass_index = (n_small-1)+(n_small-v-1)\n",
    "            backward_pass_one_before = (n_small-1)+(n_small-(max(v-1,0))-1)\n",
    "            backward_pass_two_before = (n_small-1)+(n_small-(max(v-2,0))-1)\n",
    "            backward_pass_one_after = (n_small-1)+(n_small-(min(v+1,n_small))-1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            if (v+1) in anomalies_indecies_numbers_raw and   backward_pass_one_before in anomalies_indecies_numbers_raw and backward_pass_two_before in anomalies_indecies_numbers_raw and ((str(anomalies_dictionary_bidimentional_raw[v]).find('high') != 0 and str(anomalies_dictionary_bidimentional_raw[v+1]).find('high') != 0) or (str(anomalies_dictionary_bidimentional_raw[v]).find('low') != 0 and str(anomalies_dictionary_bidimentional_raw[v+1]).find('low') != 0)) :\n",
    "                anomalies_indecies_numbers.append(v)\n",
    "                current_certainty = anomalies_certainty_bidimentional_raw[v]\n",
    "                anomalies_indecies_certainty.append(current_certainty)\n",
    "\n",
    "\n",
    "                current_type = str(anomalies_dictionary_bidimentional_raw[v]) +'-patchy'\n",
    "                anomalies_indecies_types.append(current_type)\n",
    "                patchy_skip = v        \n",
    "\n",
    "\n",
    "               \n",
    "            elif  backward_pass_index in anomalies_indecies_numbers_raw:\n",
    "                anomalies_indecies_numbers.append(v)\n",
    "                current_certainty = anomalies_certainty_bidimentional_raw[v]\n",
    "                anomalies_indecies_certainty.append(current_certainty)\n",
    "\n",
    "\n",
    "                current_type = str(anomalies_dictionary_bidimentional_raw[v]) + '-isolated'\n",
    "                anomalies_indecies_types.append(current_type)\n",
    "\n",
    "\n",
    "    for i in range(len(anomalies_indecies_numbers)):\n",
    "        anomalies_indecies_numbers_total[anomalies_indecies_numbers[i]]=anomalies_indecies_types[i]\n",
    "        anomalies_indecies_certainty_total[anomalies_indecies_numbers[i]]=anomalies_indecies_certainty[i]\n",
    "\n",
    "\n",
    "    return {'anomalies_indecies_numbers_total':anomalies_indecies_numbers_total,'anomalies_indecies_numbers':anomalies_indecies_numbers,'anomalies_indecies_types':anomalies_indecies_types,'anomalies_indecies_certainty':anomalies_indecies_certainty,'anomalies_indecies_certainty_total':anomalies_indecies_certainty_total}\n",
    "\n",
    "\n",
    "class ForecastingModel:\n",
    "    def __init__(self, data = {},m=None,dates = {},invoice_ids = {},integers=False,min_dif ={}):\n",
    "        \"\"\"Initializes the ForecastingModel object.\n",
    "\n",
    "    This constructor sets up the initial state of the ForecastingModel object with the provided data and parameters.\n",
    "    It also initializes the logs and anomaly detection attributes.\n",
    "\n",
    "    Args:\n",
    "        data: The time series data. Can be a pandas DataFrame or a dictionary.\n",
    "        m: The length of one season (if applicable).\n",
    "        dates: A dictionary of dates corresponding to the data points.\n",
    "        invoice_ids: A dictionary of invoice IDs corresponding to the data points.\n",
    "        integers: Boolean indicating if the data is integer-valued.\n",
    "        min_dif: Minimum difference to avoid division by zero.\n",
    "\n",
    "    Attributes:\n",
    "        data: The time series data.\n",
    "        dates: The dates corresponding to the data points.\n",
    "        invoice_ids: The invoice IDs corresponding to the data points.\n",
    "        model_type: A dictionary to store the type of the fitted model.\n",
    "        model_params: A dictionary to store the optimal smoothing parameters.\n",
    "        results: A dictionary to store the results of the model fitting.\n",
    "        m: The length of one season.\n",
    "        integers: Boolean indicating if the data is integer-valued.\n",
    "        min_dif: Minimum difference to avoid division by zero.\n",
    "        detected_anomalies: A dictionary to store the detected anomalies.\n",
    "        detected_anomalies_for_calculations: A dictionary to store the detected anomalies for calculations.\n",
    "        analysis_log: A dictionary to store the analysis log.\n",
    "        anomalies_log: A dictionary to store the anomalies log.\n",
    "        resulting: A dictionary to store the resulting data.\n",
    "        \"\"\"\n",
    "\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        import statsmodels.api as sm\n",
    "        import math\n",
    "        import statistics\n",
    "        from scipy.optimize import minimize\n",
    "        import scipy.stats as stats\n",
    "        import matplotlib.pyplot as plt\n",
    "        import warnings\n",
    "        if type(data) == type(pd.DataFrame()):\n",
    "            self.data = data\n",
    "            self.analysis_log = {}\n",
    "            self.anomalies_log = {}\n",
    "            self.resulting = {}\n",
    "\n",
    "        elif type(data) == type(dict()) and data == {}: \n",
    "            self.data = {}\n",
    "            self.analysis_log = {}\n",
    "            self.anomalies_log = {}\n",
    "            self.resulting = {}\n",
    "\n",
    "        else:\n",
    "            self.data = data\n",
    "            self.dates = dates\n",
    "            self.invoice_ids = invoice_ids\n",
    "            self.model_type = {}\n",
    "            self.model_params = {}\n",
    "            self.results = {}\n",
    "            self.m = m\n",
    "            self.integers = integers\n",
    "            if self.integers :\n",
    "                try:\n",
    "                    if len(min_dif)==0 :\n",
    "                        self.min_dif = get_min_pairwise_differences(self.data)\n",
    "                except:\n",
    "                    self.min_dif = min_dif\n",
    "            else:\n",
    "                self.min_dif = 0\n",
    "\n",
    "            self.detected_anomalies = {'anomalies_indecies_numbers':{},\n",
    "                                       'anomalies_indecies_types':{},\n",
    "                                       'number_of_anomalies':{},\n",
    "                                       'anomalies_dates':{},\n",
    "                                       'anomalies_observed_values': {},\n",
    "                                       'anomalies_expected_values': {},\n",
    "                                       'anomalies_invoice_id':{},\n",
    "                                       'anomalies_certainty':{}}\n",
    "            self.detected_anomalies_for_calculations = {'anomalies_indecies_numbers':{},\n",
    "                                                        'anomalies_indecies_types':{},\n",
    "                                                        'number_of_anomalies':{},\n",
    "                                                        'anomalies_dates':{},\n",
    "                                                        'anomalies_observed_values': {},\n",
    "                                                        'anomalies_expected_values': {},\n",
    "                                                        'anomalies_invoice_id':{},\n",
    "                                                        'anomalies_certainty':{}}\n",
    "\n",
    "\n",
    "\n",
    "    def full_automatic(self,IC = 'robAICc',a = 0.05):\n",
    "        \"\"\"Automatically fits the best forecasting model and detects anomalies.\n",
    "\n",
    "    This method finds the best model based on the given Information Criterion (IC) and confidence level, fits the model to the data,\n",
    "    and then detects anomalies based on the fitted model.\n",
    "\n",
    "    Args:\n",
    "        IC: The Information Criterion to use for model selection ('robAICc' by default).\n",
    "        a: The confidence level (0.05 by default).\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    This method updates the following attributes:\n",
    "        - model_type: A dictionary indicating the type of the best model.\n",
    "        - model_params: A dictionary containing the optimal smoothing parameters.\n",
    "        - results: A dictionary containing the observed values, predicted values, cleaned values, prediction intervals,\n",
    "                   anomalies indices, anomalies certainty, robust likelihood, residuals, sigma, level, trend, seasonality,\n",
    "                   and optimal parameters.\n",
    "        - detected_anomalies: A dictionary containing the indices, types, count, dates, observed values, expected values,\n",
    "                              invoice IDs, and certainty of detected anomalies.\n",
    "        \"\"\"\n",
    "    \n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        import statsmodels.api as sm\n",
    "        import math\n",
    "        import statistics\n",
    "        from scipy.optimize import minimize\n",
    "        import scipy.stats as stats\n",
    "        import matplotlib.pyplot as plt\n",
    "        import warnings\n",
    "        #if type(self.data) == type(pd.DataFrame()) or self.data == {}:\n",
    "        #    raise ValueError('This method is not compatible with dataframes')\n",
    "        import math\n",
    "        model = get_prediction_intervals(y = self.data,m=self.m,IC=IC,a=a,integers=self.integers,min_dif = self.min_dif)\n",
    "        self.model_type = {'trend': model['best_model_summary']['trend'],\n",
    "                        'seasonality': model['best_model_summary']['seasonality'],\n",
    "                        'multiplicative_seasonality': model['best_model_summary']['multiplicative_seasonality'],\n",
    "                        'multiplicative_error': model['best_model_summary']['multiplicative_error']}\n",
    "        \n",
    "        self.model_params = {'alpha':model['best_model_log']['opt_parameters']   ['alpha'],\n",
    "                            'beta':model['best_model_log']['opt_parameters']   ['beta'],\n",
    " \n",
    "                            'gamma':model['best_model_log']['opt_parameters']   ['gamma'],\n",
    "                            'lmbda':model['best_model_log']['opt_parameters']   ['lmbda']}\n",
    "        \n",
    "        p = len([ i for i in model['best_model_log']['opt_parameters'].keys() if model['best_model_log']['opt_parameters'][i] != None])\n",
    "        n = len(self.data)\n",
    "        robAIC = -2 * model['best_model_log']['robust_likelihood'] + 2*p\n",
    "        robBIC = -2 * model['best_model_log']['robust_likelihood'] + math.log(n) * p\n",
    "        robAICc = -2 * model['best_model_log']['robust_likelihood'] + 2 * ((p * n)/(n - p -1))\n",
    "        \n",
    "        self.results = {'observed_bidrectional': {},\n",
    "                        'predicted_bidrectional':{},\n",
    "                        'cleaned_bidrectional':{},\n",
    "                        'prediction_interval_bidrectional':{},\n",
    "                        'sigma_bidrectional':{},\n",
    "\n",
    "                        'observed': model['best_model_log']['observed'],\n",
    "                        'predicted':model['best_model_log']['predicted'],\n",
    "                        'cleaned':model['best_model_log']['cleaned'],\n",
    "                        'prediction_interval':model['prediction_interval'],\n",
    "                        'anomalies_indecies':model['anomalies_indecies'],\n",
    "                        'anomalies_certainty':model['certainty'],\n",
    "                        'robust_likelihood':model['best_model_log']['robust_likelihood'],\n",
    "                        'residuals':model['best_model_log']['residuals'],\n",
    "                        'sigma':model['best_model_log']['sigma'],\n",
    "                        'l':model['best_model_log']['l'],\n",
    "                        'b':model['best_model_log']['b'],\n",
    "                        's':model['best_model_log']['s'],\n",
    "                        'opt_parameters':model['best_model_log']['opt_parameters'],\n",
    "                        'p':p,\n",
    "                        'n':n,\n",
    "                        'robAIC':robAIC,\n",
    "                        'robBIC':robBIC,\n",
    "                        'robAICc':robAICc}\n",
    "        self.detected_anomalies['anomalies_indecies_numbers'] = [i for i in model['anomalies_indecies'].keys()]\n",
    "        self.detected_anomalies['anomalies_indecies_types'] =  [i for i in model['anomalies_indecies'].values()]\n",
    "        self.detected_anomalies['number_of_anomalies'] = len(self.detected_anomalies['anomalies_indecies_numbers'])\n",
    "        self.detected_anomalies['anomalies_dates'] = self.dates[self.detected_anomalies['anomalies_indecies_numbers']]\n",
    "        self.detected_anomalies['anomalies_observed_values'] = self.data[self.detected_anomalies['anomalies_indecies_numbers']]\n",
    "        self.detected_anomalies['anomalies_expected_values'] = self.results['predicted'][self.detected_anomalies['anomalies_indecies_numbers']]\n",
    "        self.detected_anomalies['anomalies_invoice_id'] = self.invoice_ids[self.detected_anomalies['anomalies_indecies_numbers']]\n",
    "        self.detected_anomalies['anomalies_certainty'] =  [i for i in model['certainty'].values()]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def full_automatic_bidirectional(self,IC = 'robAICc',a = 0.05):\n",
    "        \"\"\"Automatically fits the best bidirectional forecasting model and detects anomalies.\n",
    "\n",
    "    This method finds the best model based on the given Information Criterion (IC) and confidence level, fits the model \n",
    "    to the bidirectional data (both forward and reversed), and then detects anomalies based on the fitted model.\n",
    "\n",
    "    Args:\n",
    "        IC: The Information Criterion to use for model selection ('robAICc' by default).\n",
    "        a: The confidence level (0.05 by default).\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    This method updates the following attributes:\n",
    "        - model_type: A dictionary indicating the type of the best model.\n",
    "        - model_params: A dictionary containing the optimal smoothing parameters.\n",
    "        - results: A dictionary containing the observed values, predicted values, cleaned values, prediction intervals,\n",
    "                   anomalies indices, anomalies certainty, robust likelihood, residuals, sigma, level, trend, seasonality,\n",
    "                   and optimal parameters.\n",
    "        - detected_anomalies: A dictionary containing the indices, types, count, dates, observed values, expected values,\n",
    "                              invoice IDs, and certainty of detected anomalies.\n",
    "        \"\"\"\n",
    "\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        import statsmodels.api as sm\n",
    "        import math\n",
    "        import statistics\n",
    "        from scipy.optimize import minimize\n",
    "        import scipy.stats as stats\n",
    "        import matplotlib.pyplot as plt\n",
    "        import warnings\n",
    "        #if type(self.data) == type(pd.DataFrame()) or self.data == {}:\n",
    "        #    raise ValueError('This method is not compatible with dataframes')\n",
    "        import math\n",
    "\n",
    "        time_series = self.data\n",
    "        time_series_reversed = time_series[::-1] \n",
    "        n_small = time_series.shape[0]\n",
    "        bidirectional_array = np.concatenate((time_series, time_series_reversed[1:]), axis=None)\n",
    "        bidirectional_array_midpoint = bidirectional_array[n_small-1] # it is also the last point of the forward pass\n",
    "\n",
    "\n",
    "\n",
    "        model = get_prediction_intervals(y = bidirectional_array,m=self.m,IC=IC,a=a,integers=self.integers,min_dif =self.min_dif)\n",
    "        self.model_type = {'trend': model['best_model_summary']['trend'],\n",
    "                        'seasonality': model['best_model_summary']['seasonality'],\n",
    "                        'multiplicative_seasonality': model['best_model_summary']['multiplicative_seasonality'],\n",
    "                        'multiplicative_error': model['best_model_summary']['multiplicative_error']}\n",
    "        \n",
    "        self.model_params = {'alpha':model['best_model_log']['opt_parameters']   ['alpha'],\n",
    "                            'beta':model['best_model_log']['opt_parameters']   ['beta'],\n",
    " \n",
    "                            'gamma':model['best_model_log']['opt_parameters']   ['gamma'],\n",
    "                            'lmbda':model['best_model_log']['opt_parameters']   ['lmbda']}\n",
    "        \n",
    "        p = len([ i for i in model['best_model_log']['opt_parameters'].keys() if model['best_model_log']['opt_parameters'][i] != None])\n",
    "        n = len(self.data)\n",
    "        robAIC = -2 * model['best_model_log']['robust_likelihood'] + 2*p\n",
    "        robBIC = -2 * model['best_model_log']['robust_likelihood'] + math.log(n) * p\n",
    "        robAICc = -2 * model['best_model_log']['robust_likelihood'] + 2 * ((p * n)/(n - p -1))\n",
    "        \n",
    "        \n",
    "        bidirectional_anomalies = get_bidirectional_anomalies(anomalies_dictionary_bidimentional_raw = model['anomalies_indecies'],anomalies_certainty_bidimentional_raw = model['certainty'],n_small=n_small)\n",
    "        anomalies_indecies_numbers = bidirectional_anomalies['anomalies_indecies_numbers']\n",
    "        anomalies_indecies_types = bidirectional_anomalies['anomalies_indecies_types']\n",
    "        anomalies_indecies_numbers_total = bidirectional_anomalies['anomalies_indecies_numbers_total']\n",
    "        anomalies_indecies_certainty = bidirectional_anomalies['anomalies_indecies_certainty']\n",
    "        anomalies_indecies_certainty_total = bidirectional_anomalies['anomalies_indecies_certainty_total']\n",
    "        \n",
    "        self.results = {'observed': model['best_model_log']['observed'][:n_small],\n",
    "                        'predicted':model['best_model_log']['predicted'][:n_small],\n",
    "                        'cleaned':model['best_model_log']['cleaned'][:n_small],\n",
    "                        'prediction_interval':model['prediction_interval'][:n_small],\n",
    "\n",
    "                        'observed_bidrectional': model['best_model_log']['observed'],\n",
    "                        'predicted_bidrectional':model['best_model_log']['predicted'],\n",
    "                        'cleaned_bidrectional':model['best_model_log']['cleaned'],\n",
    "                        'prediction_interval_bidrectional':model['prediction_interval'],\n",
    "                        'sigma_bidrectional':model['best_model_log']['sigma'],\n",
    "\n",
    "                        'anomalies_indecies':anomalies_indecies_numbers_total,\n",
    "                        'anomalies_certainty':anomalies_indecies_certainty_total,\n",
    "                        'robust_likelihood':model['best_model_log']['robust_likelihood'],\n",
    "                        'residuals':model['best_model_log']['residuals'][:n_small],\n",
    "                        'sigma':model['best_model_log']['sigma'][:n_small],\n",
    "                        'l':model['best_model_log']['l'],\n",
    "                        'b':model['best_model_log']['b'],\n",
    "                        's':model['best_model_log']['s'],\n",
    "                        'opt_parameters':model['best_model_log']['opt_parameters'],\n",
    "                        'p':p,\n",
    "                        'n':n,\n",
    "                        'robAIC':robAIC,\n",
    "                        'robBIC':robBIC,\n",
    "                        'robAICc':robAICc}\n",
    "        self.detected_anomalies['anomalies_indecies_numbers'] = [i for i in anomalies_indecies_numbers_total.keys()]\n",
    "        self.detected_anomalies['anomalies_indecies_types'] =  [i for i in anomalies_indecies_numbers_total.values()]\n",
    "        self.detected_anomalies['number_of_anomalies'] = len(self.detected_anomalies['anomalies_indecies_numbers'])\n",
    "        self.detected_anomalies['anomalies_dates'] = self.dates[anomalies_indecies_numbers]\n",
    "        self.detected_anomalies['anomalies_observed_values'] = self.data[anomalies_indecies_numbers]\n",
    "        self.detected_anomalies['anomalies_expected_values'] = self.results['predicted'][anomalies_indecies_numbers]\n",
    "        self.detected_anomalies['anomalies_invoice_id'] = self.invoice_ids[anomalies_indecies_numbers]\n",
    "        self.detected_anomalies['anomalies_certainty'] = [i for i in anomalies_indecies_certainty_total.values()]\n",
    "\n",
    "\n",
    "    def find_best_model(self,IC = 'robAICc'):\n",
    "        \"\"\"Finds the best forecasting model based on the specified Information Criterion (IC).\n",
    "\n",
    "    This method evaluates various combinations of additive and multiplicative error and seasonality models,\n",
    "    and selects the best model based on the specified Information Criterion (IC).\n",
    "\n",
    "    Args:\n",
    "        IC: The Information Criterion to use for model selection ('robAICc' by default).\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary indicating the type of the best model, including trend, seasonality, multiplicative seasonality,\n",
    "              and multiplicative error.\n",
    "\n",
    "    This method updates the following attribute:\n",
    "        - model_type: A dictionary indicating the type of the best model.\n",
    "        \"\"\"\n",
    "\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        import statsmodels.api as sm\n",
    "        import math\n",
    "        import statistics\n",
    "        from scipy.optimize import minimize\n",
    "        import scipy.stats as stats\n",
    "        import matplotlib.pyplot as plt\n",
    "        import warnings\n",
    "        #if type(self.data) == type(pd.DataFrame()) or self.data == {}:\n",
    "        #    raise ValueError('This method is not compatible with dataframes')\n",
    "        model = find_best_model(y=self.data,m=self.m,IC=IC,integers=self.integers,min_dif = self.min_dif)\n",
    "        self.model_type = {'trend': model['trend'],\n",
    "                        'seasonality': model['seasonality'],\n",
    "                        'multiplicative_seasonality': model['multiplicative_seasonality'],\n",
    "                        'multiplicative_error': model['multiplicative_error']}\n",
    "        return self.model_type\n",
    "                \n",
    "\n",
    "    def select_model_manually(self,trend,seasonality,multiplicative_seasonality,multiplicative_error,IC = 'robAICc',a =0.05):\n",
    "        \"\"\"Manually selects and fits a desired forecasting model and detects anomalies.\n",
    "\n",
    "    This method allows the user to manually specify the model components (trend, seasonality, multiplicative seasonality, and multiplicative error).\n",
    "    It fits the specified model to the data and detects anomalies based on the fitted model.\n",
    "\n",
    "    Args:\n",
    "        trend: Boolean indicating if a trend component is present.\n",
    "        seasonality: Boolean indicating if a seasonal component is present.\n",
    "        multiplicative_seasonality: Boolean indicating if the seasonality is multiplicative.\n",
    "        multiplicative_error: Boolean indicating if the error is multiplicative.\n",
    "        IC: The Information Criterion to use for model selection ('robAICc' by default).\n",
    "        a: The confidence level (0.05 by default).\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    This method updates the following attributes:\n",
    "        - model_type: A dictionary indicating the type of the specified model.\n",
    "        - model_params: A dictionary containing the optimal smoothing parameters.\n",
    "        - results: A dictionary containing the observed values, predicted values, cleaned values, prediction intervals,\n",
    "                   anomalies indices, anomalies certainty, robust likelihood, residuals, sigma, level, trend, seasonality,\n",
    "                   and optimal parameters.\n",
    "        - detected_anomalies: A dictionary containing the indices, types, count, dates, observed values, expected values,\n",
    "                              invoice IDs, and certainty of detected anomalies.\n",
    "        \"\"\"\n",
    "\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        import statsmodels.api as sm\n",
    "        import math\n",
    "        import statistics\n",
    "        from scipy.optimize import minimize\n",
    "        import scipy.stats as stats\n",
    "        import matplotlib.pyplot as plt\n",
    "        import warnings\n",
    "        #if type(self.data) == type(pd.DataFrame()) or self.data == {}:\n",
    "        #    raise ValueError('This method is not compatible with dataframes')\n",
    "        import math\n",
    "\n",
    "        model = get_prediction_intervals(y = self.data,m=self.m,trend=trend,seasonality=seasonality,\n",
    "                                                multiplicative_seasonality=multiplicative_seasonality,\n",
    "                                                multiplicative_error=multiplicative_error,IC=IC,a=a,integers=self.integers,min_dif=self.min_dif)\n",
    "        self.model_type = {'trend': model['best_model_summary']['trend'],\n",
    "                        'seasonality': model['best_model_summary']['seasonality'],\n",
    "                        'multiplicative_seasonality': model['best_model_summary']['multiplicative_seasonality'],\n",
    "                        'multiplicative_error': model['best_model_summary']['multiplicative_error']}\n",
    "        \n",
    "        self.model_params = {'alpha':model['best_model_log']['opt_parameters']   ['alpha'],\n",
    "                            'beta':model['best_model_log']['opt_parameters']   ['beta'],\n",
    " \n",
    "                            'gamma':model['best_model_log']['opt_parameters']   ['gamma'],\n",
    "                            'lmbda':model['best_model_log']['opt_parameters']   ['lmbda']}\n",
    "        \n",
    "        p = len([ i for i in model['best_model_log']['opt_parameters'].keys() if model['best_model_log']['opt_parameters'][i] != None])\n",
    "        n = len(self.data)\n",
    "        robAIC = -2 * model['best_model_log']['robust_likelihood'] + 2*p\n",
    "        robBIC = -2 * model['best_model_log']['robust_likelihood'] + math.log(n) * p\n",
    "        robAICc = -2 * model['best_model_log']['robust_likelihood'] + 2 * ((p * n)/(n - p -1))\n",
    "        \n",
    "        self.results = {'observed_bidrectional': {},\n",
    "                        'predicted_bidrectional':{},\n",
    "                        'cleaned_bidrectional':{},\n",
    "                        'prediction_interval_bidrectional':{},\n",
    "                        'sigma_bidrectional':{},\n",
    "                        'observed': model['best_model_log']['observed'],\n",
    "                        'predicted':model['best_model_log']['predicted'],\n",
    "                        'cleaned':model['best_model_log']['cleaned'],\n",
    "                        'prediction_interval':model['prediction_interval'],\n",
    "                        'anomalies_indecies':model['anomalies_indecies'],\n",
    "                        'anomalies_certainty':model['certainty'],\n",
    "                        'robust_likelihood':model['best_model_log']['robust_likelihood'],\n",
    "                        'residuals':model['best_model_log']['residuals'],\n",
    "                        'sigma':model['best_model_log']['sigma'],\n",
    "                        'l':model['best_model_log']['l'],\n",
    "                        'b':model['best_model_log']['b'],\n",
    "                        's':model['best_model_log']['s'],\n",
    "                        'opt_parameters':model['best_model_log']['opt_parameters'],\n",
    "                        'p':p,\n",
    "                        'n':n,\n",
    "                        'robAIC':robAIC,\n",
    "                        'robBIC':robBIC,\n",
    "                        'robAICc':robAICc}\n",
    "        self.detected_anomalies['anomalies_indecies_numbers'] = [i for i in model['anomalies_indecies'].keys()]\n",
    "        self.detected_anomalies['anomalies_indecies_types'] =  [i for i in model['anomalies_indecies'].values()]\n",
    "        self.detected_anomalies['number_of_anomalies'] = len(self.detected_anomalies['anomalies_indecies_numbers'])\n",
    "        self.detected_anomalies['anomalies_dates'] = self.dates[self.detected_anomalies['anomalies_indecies_numbers']]\n",
    "        self.detected_anomalies['anomalies_observed_values'] = self.data[self.detected_anomalies['anomalies_indecies_numbers']]\n",
    "        self.detected_anomalies['anomalies_expected_values'] = self.results['predicted'][self.detected_anomalies['anomalies_indecies_numbers']]\n",
    "        self.detected_anomalies['anomalies_invoice_id'] = self.invoice_ids[self.detected_anomalies['anomalies_indecies_numbers']]\n",
    "        self.detected_anomalies['anomalies_certainty'] =  [i for i in model['certainty'].values()]\n",
    "\n",
    "\n",
    "    def select_model_manually_bidirectional(self,trend,seasonality,multiplicative_seasonality,multiplicative_error,IC = 'robAICc',a =0.05):\n",
    "        \"\"\"Manually selects and fits a bidirectional forecasting model and detects anomalies.\n",
    "\n",
    "    This method allows the user to manually specify the model components (trend, seasonality, multiplicative seasonality, and multiplicative error).\n",
    "    It fits the specified model to the bidirectional data (both forward and reversed), and detects anomalies based on the fitted model.\n",
    "\n",
    "    Args:\n",
    "        trend: Boolean indicating if a trend component is present.\n",
    "        seasonality: Boolean indicating if a seasonal component is present.\n",
    "        multiplicative_seasonality: Boolean indicating if the seasonality is multiplicative.\n",
    "        multiplicative_error: Boolean indicating if the error is multiplicative.\n",
    "        IC: The Information Criterion to use for model selection ('robAICc' by default).\n",
    "        a: The confidence level (0.05 by default).\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    This method updates the following attributes:\n",
    "        - model_type: A dictionary indicating the type of the specified model.\n",
    "        - model_params: A dictionary containing the optimal smoothing parameters.\n",
    "        - results: A dictionary containing the observed values, predicted values, cleaned values, prediction intervals,\n",
    "                   anomalies indices, anomalies certainty, robust likelihood, residuals, sigma, level, trend, seasonality,\n",
    "                   and optimal parameters.\n",
    "        - detected_anomalies: A dictionary containing the indices, types, count, dates, observed values, expected values,\n",
    "                              invoice IDs, and certainty of detected anomalies.\n",
    "        \"\"\"\n",
    "\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        import statsmodels.api as sm\n",
    "        import math\n",
    "        import statistics\n",
    "        from scipy.optimize import minimize\n",
    "        import scipy.stats as stats\n",
    "        import matplotlib.pyplot as plt\n",
    "        import warnings\n",
    "        #if type(self.data) == type(pd.DataFrame()) or self.data == {}:\n",
    "        #    raise ValueError('This method is not compatible with dataframes')\n",
    "        import math\n",
    "\n",
    "        time_series = self.data\n",
    "        time_series_reversed = time_series[::-1] \n",
    "        n_small = time_series.shape[0]\n",
    "        bidirectional_array = np.concatenate((time_series, time_series_reversed[1:]), axis=None)\n",
    "        bidirectional_array_midpoint = bidirectional_array[n_small-1] # it is also the last point of the forward pass\n",
    "\n",
    "        model = get_prediction_intervals(y = bidirectional_array,m=self.m,trend=trend,seasonality=seasonality,\n",
    "                                                multiplicative_seasonality=multiplicative_seasonality,\n",
    "                                                multiplicative_error=multiplicative_error,IC=IC,a=a,integers=self.integers,min_dif=self.min_dif)\n",
    "        self.model_type = {'trend': model['best_model_summary']['trend'],\n",
    "                        'seasonality': model['best_model_summary']['seasonality'],\n",
    "                        'multiplicative_seasonality': model['best_model_summary']['multiplicative_seasonality'],\n",
    "                        'multiplicative_error': model['best_model_summary']['multiplicative_error']}\n",
    "        \n",
    "        self.model_params = {'alpha':model['best_model_log']['opt_parameters']   ['alpha'],\n",
    "                            'beta':model['best_model_log']['opt_parameters']   ['beta'],\n",
    " \n",
    "                            'gamma':model['best_model_log']['opt_parameters']   ['gamma'],\n",
    "                            'lmbda':model['best_model_log']['opt_parameters']   ['lmbda']}\n",
    "        \n",
    "        p = len([ i for i in model['best_model_log']['opt_parameters'].keys() if model['best_model_log']['opt_parameters'][i] != None])\n",
    "        n = len(self.data)\n",
    "        robAIC = -2 * model['best_model_log']['robust_likelihood'] + 2*p\n",
    "        robBIC = -2 * model['best_model_log']['robust_likelihood'] + math.log(n) * p\n",
    "        robAICc = -2 * model['best_model_log']['robust_likelihood'] + 2 * ((p * n)/(n - p -1))\n",
    "        \n",
    "        \n",
    "        bidirectional_anomalies = get_bidirectional_anomalies(anomalies_dictionary_bidimentional_raw = model['anomalies_indecies'],anomalies_certainty_bidimentional_raw = model['certainty'],n_small=n_small)\n",
    "        anomalies_indecies_numbers = bidirectional_anomalies['anomalies_indecies_numbers']\n",
    "        anomalies_indecies_types = bidirectional_anomalies['anomalies_indecies_types']\n",
    "        anomalies_indecies_numbers_total = bidirectional_anomalies['anomalies_indecies_numbers_total']\n",
    "        anomalies_indecies_certainty = bidirectional_anomalies['anomalies_indecies_certainty']\n",
    "        anomalies_indecies_certainty_total = bidirectional_anomalies['anomalies_indecies_certainty_total']\n",
    "        \n",
    "        self.results = {'observed': model['best_model_log']['observed'][:n_small],\n",
    "                        'predicted':model['best_model_log']['predicted'][:n_small],\n",
    "                        'cleaned':model['best_model_log']['cleaned'][:n_small],\n",
    "                        'prediction_interval':model['prediction_interval'][:n_small],\n",
    "\n",
    "                        'observed_bidrectional': model['best_model_log']['observed'],\n",
    "                        'predicted_bidrectional':model['best_model_log']['predicted'],\n",
    "                        'cleaned_bidrectional':model['best_model_log']['cleaned'],\n",
    "                        'prediction_interval_bidrectional':model['prediction_interval'],\n",
    "                        'sigma_bidrectional':model['best_model_log']['sigma'],\n",
    "\n",
    "                        'anomalies_indecies':anomalies_indecies_numbers_total,\n",
    "                        'anomalies_certainty':anomalies_indecies_certainty_total,\n",
    "                        'robust_likelihood':model['best_model_log']['robust_likelihood'],\n",
    "                        'residuals':model['best_model_log']['residuals'][:n_small],\n",
    "                        'sigma':model['best_model_log']['sigma'][:n_small],\n",
    "                        'l':model['best_model_log']['l'],\n",
    "                        'b':model['best_model_log']['b'],\n",
    "                        's':model['best_model_log']['s'],\n",
    "                        'opt_parameters':model['best_model_log']['opt_parameters'],\n",
    "                        'p':p,\n",
    "                        'n':n,\n",
    "                        'robAIC':robAIC,\n",
    "                        'robBIC':robBIC,\n",
    "                        'robAICc':robAICc}\n",
    "        self.detected_anomalies['anomalies_indecies_numbers'] = [i for i in anomalies_indecies_numbers_total.keys()]\n",
    "        self.detected_anomalies['anomalies_indecies_types'] =  [i for i in anomalies_indecies_numbers_total.values()]\n",
    "        self.detected_anomalies['number_of_anomalies'] = len(self.detected_anomalies['anomalies_indecies_numbers'])\n",
    "        self.detected_anomalies['anomalies_dates'] = self.dates[anomalies_indecies_numbers]\n",
    "        self.detected_anomalies['anomalies_observed_values'] = self.data[anomalies_indecies_numbers]\n",
    "        self.detected_anomalies['anomalies_expected_values'] = self.results['predicted'][anomalies_indecies_numbers]\n",
    "        self.detected_anomalies['anomalies_invoice_id'] = self.invoice_ids[anomalies_indecies_numbers]\n",
    "        self.detected_anomalies['anomalies_certainty'] = [i for i in anomalies_indecies_certainty_total.values()]\n",
    "\n",
    "\n",
    "    def optimize_parameters(self):\n",
    "        \"\"\"Optimizes the parameters of the forecasting model based on the specified model type.\n",
    "\n",
    "    This method optimizes the smoothing parameters (alpha, beta, gamma, lambda) for the specified model type.\n",
    "    If the model type is not set, it raises a ValueError.\n",
    "\n",
    "    Args:\n",
    "        None\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    This method updates the following attribute:\n",
    "        - model_params: A dictionary containing the optimized smoothing parameters.\n",
    "        \"\"\"\n",
    "\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        import statsmodels.api as sm\n",
    "        import math\n",
    "        import statistics\n",
    "        from scipy.optimize import minimize\n",
    "        import scipy.stats as stats\n",
    "        import matplotlib.pyplot as plt\n",
    "        import warnings\n",
    "        #if type(self.data) == type(pd.DataFrame()) or self.data == {}:\n",
    "        #    raise ValueError('This method is not compatible with dataframes')\n",
    "\n",
    "        if self.model_type == {}:\n",
    "            raise ValueError(\"Set first a model type, or try full automatic mode\")\n",
    "        trend = self.model_type['trend']\n",
    "        seasonality = self.model_type['seasonality']\n",
    "        multiplicative_seasonality = self.model_type['multiplicative_seasonality']\n",
    "        multiplicative_error = self.model_type['multiplicative_error']\n",
    "\n",
    "        model = fit_model(y=self.data,m=self.m,trend=trend,seasonality=seasonality,\n",
    "                                    multiplicative_seasonality=multiplicative_seasonality,\n",
    "                                    multiplicative_error =multiplicative_error,integers=self.integers,min_dif = self.min_dif)\n",
    "        \n",
    "        self.model_params = {'alpha':model['opt_parameters']['alpha'],'beta':model['opt_parameters']['beta'],'gamma':model['opt_parameters']['gamma'],'lmbda':model['opt_parameters']['lmbda']}\n",
    "        #return self.model_params\n",
    "    \n",
    "\n",
    "    def forecast(self):\n",
    "        \"\"\"Generates forecasts based on the optimized model parameters.\n",
    "\n",
    "    This method fits the model using the specified model type and the optimized parameters, then generates forecasts and updates the results.\n",
    "    If the model type is not set, it raises a ValueError.\n",
    "\n",
    "    Args:\n",
    "        None\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the observed values, predicted values, cleaned values, robust likelihood, residuals,\n",
    "              sigma, level, trend, seasonality, optimal parameters, number of parameters, number of observations, robust AIC,\n",
    "              robust BIC, and robust AICc.\n",
    "\n",
    "    This method updates the following attribute:\n",
    "        - results: A dictionary containing the results of the forecasting model.\n",
    "        \"\"\"\n",
    "\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        import statsmodels.api as sm\n",
    "        import math\n",
    "        import statistics\n",
    "        from scipy.optimize import minimize\n",
    "        import scipy.stats as stats\n",
    "        import matplotlib.pyplot as plt\n",
    "        import warnings\n",
    "        #if type(self.data) == type(pd.DataFrame()) or self.data == {}:\n",
    "        #    raise ValueError('This method is not compatible with dataframes')        \n",
    "\n",
    "        import math\n",
    "        model = fit_model(seasonality=self.model_type['seasonality'],\n",
    "                                        trend=self.model_type['trend'],\n",
    "                                        multiplicative_error=self.model_type['multiplicative_error'],\n",
    "                                        multiplicative_seasonality=self.model_type['multiplicative_seasonality'],\n",
    "                                        m = self.m,y=self.data,integers=self.integers,min_dif = self.min_dif)\n",
    "        \n",
    "        p = len([ i for i in model['opt_parameters'].keys() if model['opt_parameters'][i] != None])\n",
    "        n = len(self.data)\n",
    "        robAIC = -2 * model['robust_likelihood'] + 2*p\n",
    "        robBIC = -2 * model['robust_likelihood'] + math.log(n) * p\n",
    "        robAICc = -2 * model['robust_likelihood'] + 2 * ((p * n)/(n - p -1))\n",
    "        \n",
    "        self.results = {'observed': model['observed'],'predicted':model['predicted'],'cleaned':model['cleaned'],'prediction_interval':None,'anomalies_indecies':None,'robust_likelihood':model['robust_likelihood'],'residuals':model['residuals'],'sigma':model['sigma'],'l':model['l'],'b':model['b'],'s':model['s'],'opt_parameters':model['opt_parameters'],'p':p,'n':n,'robAIC':robAIC,'robBIC':robBIC,'robAICc':robAICc}\n",
    "        print('model results has been updated')\n",
    "        return self.results\n",
    "\n",
    "\n",
    "    def plot_results(self,title,a=0.05,anomalies_recalculated = False,threshold = 30):\n",
    "        \"\"\"Plots the forecasting results, including observed data, predicted values, and anomalies.\n",
    "\n",
    "    This method visualizes the observed data, predicted values, confidence intervals, and anomalies. It can also recalculate anomalies based on a specified threshold.\n",
    "\n",
    "    Args:\n",
    "        title: The title of the plot.\n",
    "        a: The confidence level (0.05 by default).\n",
    "        anomalies_recalculated: Boolean indicating if anomalies should be recalculated based on a threshold (False by default).\n",
    "        threshold: The certainty threshold for identifying anomalies (30 by default).\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    This method uses the following attributes:\n",
    "        - model_type: A dictionary indicating the type of the model.\n",
    "        - results: A dictionary containing the results of the forecasting model.\n",
    "        - detected_anomalies: A dictionary containing the indices of detected anomalies.\n",
    "        \"\"\"\n",
    "\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        import statsmodels.api as sm\n",
    "        import math\n",
    "        import statistics\n",
    "        from scipy.optimize import minimize\n",
    "        import scipy.stats as stats\n",
    "        import matplotlib.pyplot as plt\n",
    "        import warnings\n",
    "\n",
    "        #if type(self.data) == type(pd.DataFrame()) or self.data == {}:\n",
    "        #    raise ValueError('This method is not compatible with dataframes')\n",
    "\n",
    "\n",
    "        multiplicative_error = self.model_type['multiplicative_error']\n",
    "        observed = self.results['observed']\n",
    "        predicted = self.results['predicted']\n",
    "        sigma = self.results['sigma']\n",
    "\n",
    "\n",
    "        confidence_interval = get_confidence_interval(observed=observed,predicted=predicted,sigma=sigma,multiplicative_error=multiplicative_error,a=a)\n",
    "        prediction_interval = confidence_interval['prediction_interval']\n",
    "        lower_bound = confidence_interval['lower_bound']\n",
    "        upper_bound = confidence_interval['upper_bound']\n",
    "        anomalies_indecies = confidence_interval['anomalies_indecies']\n",
    "        anomalies_certainty = confidence_interval['certainty']\n",
    "\n",
    "       \n",
    "        df = pd.DataFrame({\n",
    "            'Observed': observed,\n",
    "            'Predicted': predicted,\n",
    "            'Lower Bound': lower_bound,\n",
    "            'Upper Bound': upper_bound})\n",
    "\n",
    "        if anomalies_recalculated:\n",
    "            anomaly_index = [i for i in anomalies_indecies if anomalies_certainty[i] > threshold]\n",
    "        elif not anomalies_recalculated:\n",
    "            anomaly_index = self.detected_anomalies['anomalies_indecies_numbers']\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(df['Observed'], label='Observed', color='blue')\n",
    "        plt.plot(df['Predicted'], label='Predicted', color='green', linestyle='--')\n",
    "\n",
    "        # Fill between for confidence interval\n",
    "        plt.fill_between(df.index, df['Lower Bound'], df['Upper Bound'], color='gray', alpha=0.3, label='Confidence Interval')\n",
    "\n",
    "        # Highlight the anomaly\n",
    "        # Note: Ensure your anomaly_index aligns with how your data's index is structured\n",
    "\n",
    "        plt.scatter(anomaly_index, df.iloc[anomaly_index]['Observed'], color='red', zorder=5, label='Anomaly')\n",
    "\n",
    "        #plt.scatter(anomaly_index, df.loc[anomaly_index,'Observed'], color='red', zorder=5, label='Anomaly')\n",
    "        title =str(title)\n",
    "        plt.title(title)\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Value')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def reset_interval(self,a=0.05):\n",
    "        \"\"\"Resets the prediction interval and recalculates anomalies.\n",
    "\n",
    "    This method recalculates the prediction intervals and anomalies based on the updated data. It handles both \n",
    "    unidirectional and bidirectional data, updating the detected anomalies for further calculations.\n",
    "\n",
    "    Args:\n",
    "        a: The confidence level (0.05 by default).\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    This method updates the following attribute:\n",
    "        - detected_anomalies_for_calculations: A dictionary containing the recalculated anomalies and their attributes.\n",
    "        \"\"\"\n",
    "\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        import statsmodels.api as sm\n",
    "        import math\n",
    "        import statistics\n",
    "        from scipy.optimize import minimize\n",
    "        import scipy.stats as stats\n",
    "        import matplotlib.pyplot as plt\n",
    "        import warnings\n",
    "\n",
    "        #if type(self.data) == type(pd.DataFrame()) or self.data == {}:\n",
    "        #    raise ValueError('This method is not compatible with dataframes')\n",
    "        import scipy.stats as stats \n",
    "        import pandas as pd\n",
    "        \n",
    "\n",
    "        multiplicative_error = self.model_type['multiplicative_error']\n",
    "        if len(self.results['observed_bidrectional']) != 0:\n",
    "            observed = self.results['observed_bidrectional']\n",
    "            predicted = self.results['predicted_bidrectional']\n",
    "            sigma = self.results['sigma_bidrectional']\n",
    "            n_small = self.results['n']\n",
    "\n",
    "\n",
    "\n",
    "            confidence_interval = get_confidence_interval(observed=observed,predicted=predicted,sigma=sigma,multiplicative_error=multiplicative_error,a=a)\n",
    "            prediction_interval = confidence_interval['prediction_interval']\n",
    "            lower_bound = confidence_interval['lower_bound']\n",
    "            upper_bound = confidence_interval['upper_bound']\n",
    "            anomalies_indecies = confidence_interval['anomalies_indecies']\n",
    "            anomalies_certainty = confidence_interval['certainty']\n",
    "\n",
    "\n",
    "\n",
    "            bidirectional_anomalies = get_bidirectional_anomalies(anomalies_dictionary_bidimentional_raw = anomalies_indecies,anomalies_certainty_bidimentional_raw =anomalies_certainty ,n_small=n_small)\n",
    "            anomalies_indecies_numbers = bidirectional_anomalies['anomalies_indecies_numbers']\n",
    "            anomalies_indecies_types = bidirectional_anomalies['anomalies_indecies_types']\n",
    "            anomalies_indecies_numbers_total = bidirectional_anomalies['anomalies_indecies_numbers_total']\n",
    "            anomalies_indecies_certainty = bidirectional_anomalies['anomalies_indecies_certainty']\n",
    "            anomalies_indecies_certainty_total = bidirectional_anomalies['anomalies_indecies_certainty_total']\n",
    "\n",
    "\n",
    "            self.detected_anomalies_for_calculations['anomalies_indecies_numbers'] = [i for i in anomalies_indecies_numbers_total.keys()]\n",
    "            self.detected_anomalies_for_calculations['anomalies_indecies_types'] = [i for i in anomalies_indecies_numbers_total.values()]\n",
    "            self.detected_anomalies_for_calculations['number_of_anomalies'] = len(self.detected_anomalies['anomalies_indecies_numbers'])\n",
    "            self.detected_anomalies_for_calculations['anomalies_dates'] = self.dates[anomalies_indecies_numbers]\n",
    "            self.detected_anomalies_for_calculations['anomalies_observed_values'] = self.data[anomalies_indecies_numbers]\n",
    "            self.detected_anomalies_for_calculations['anomalies_expected_values'] = self.results['predicted'][anomalies_indecies_numbers]\n",
    "            self.detected_anomalies_for_calculations['anomalies_invoice_id'] = self.invoice_ids[anomalies_indecies_numbers]\n",
    "            self.detected_anomalies_for_calculations['anomalies_certainty'] = [i for i in anomalies_indecies_certainty_total.values()]\n",
    " \n",
    "\n",
    "        \n",
    "        if len(self.results['observed_bidrectional']) == 0:\n",
    "            observed = self.results['observed']\n",
    "            predicted = self.results['predicted']\n",
    "            sigma = self.results['sigma']\n",
    "            n_small = self.results['n']\n",
    "\n",
    "\n",
    "            confidence_interval = get_confidence_interval(observed=observed,predicted=predicted,sigma=sigma,multiplicative_error=multiplicative_error,a=a)\n",
    "            prediction_interval = confidence_interval['prediction_interval']\n",
    "            lower_bound = confidence_interval['lower_bound']\n",
    "            upper_bound = confidence_interval['upper_bound']\n",
    "            anomalies_indecies = confidence_interval['anomalies_indecies']\n",
    "            anomalies_certainty = confidence_interval['anomalies_certainty']\n",
    "\n",
    "\n",
    "            self.detected_anomalies_for_calculations['anomalies_indecies_numbers'] = [i for i in anomalies_indecies_numbers_total.keys()]\n",
    "            self.detected_anomalies_for_calculations['anomalies_indecies_types'] = [i for i in anomalies_indecies_numbers_total.values()]\n",
    "            self.detected_anomalies_for_calculations['number_of_anomalies'] = len(self.detected_anomalies['anomalies_indecies_numbers'])\n",
    "            self.detected_anomalies_for_calculations['anomalies_dates'] = self.dates[anomalies_indecies_numbers]\n",
    "            self.detected_anomalies_for_calculations['anomalies_observed_values'] = self.data[anomalies_indecies_numbers]\n",
    "            self.detected_anomalies_for_calculations['anomalies_expected_values'] = self.results['predicted'][anomalies_indecies_numbers]\n",
    "            self.detected_anomalies_for_calculations['anomalies_invoice_id'] = self.invoice_ids[anomalies_indecies_numbers]\n",
    "            self.detected_anomalies_for_calculations['anomalies_certainty'] = [i for i in anomalies_indecies_certainty_total.values()]\n",
    "\n",
    "\n",
    "    def detect_anomalies_from_dataframe(self,target_column = 'Invoice Spend',date_column = 'Date',default_m =1,default_alpha = 0.05,column_to_exclude = ['Date'], path_dump_pkl = '/Workspace/Users/ashraf.zedane@nl.abnamro.com/data_simulation/Reduced_data/v_1/analysis_outcome.pkl', path_anomaly = \"/Workspace/Users/ashraf.zedane@nl.abnamro.com/data_simulation/Reduced_data/v_1/anomalies_log.xlsx\"):\n",
    "        \"\"\"Detects anomalies in a DataFrame and saves the analysis results.\n",
    "\n",
    "    This method processes a DataFrame to detect anomalies in the specified target column for different categories. It fits various models,\n",
    "    detects anomalies, and saves the analysis results and logs in specified file paths.\n",
    "\n",
    "    Args:\n",
    "        target_column: The column name containing the values to be analyzed ('Invoice Spend' by default).\n",
    "        date_column: The column name containing the dates ('Date' by default).\n",
    "        default_m: The default number of seasons (1 by default).\n",
    "        default_alpha: The default confidence level (0.05 by default).\n",
    "        column_to_exclude: A list of columns to exclude from the analysis (['Date'] by default).\n",
    "        path_dump_pkl: The file path to save the analysis log as a pickle file.\n",
    "        path_anomaly: The file path to read the anomalies log from an Excel file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    This method updates the following attributes:\n",
    "        - analysis_log: A dictionary containing the detailed analysis log.\n",
    "        - anomalies_log: A DataFrame containing the anomalies log read from the specified file.\n",
    "        - resulting: A dictionary containing the summary of the analysis results.\n",
    "        \"\"\"\n",
    "\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        import statsmodels.api as sm\n",
    "        import math\n",
    "        import statistics\n",
    "        from scipy.optimize import minimize\n",
    "        import scipy.stats as stats\n",
    "        import matplotlib.pyplot as plt\n",
    "        import warnings\n",
    "\n",
    "        #if type(self.data) != type(pd.DataFrame()):\n",
    "        #    raise ValueError('This method supports only dataframes')\n",
    "        import pickle\n",
    "        # To keep track of the fitting progress\n",
    "        total_number_of_models = 0\n",
    "        number_of_models = 0\n",
    "        number_of_failed_models = 0\n",
    "        # The dictionary to save everything\n",
    "        self.analysis_log = {}\n",
    "\n",
    "        # Generating a performance metric summary\n",
    "        anomalies_log = pd.read_excel(path_anomaly)\n",
    "        self.anomalies_log = anomalies_log\n",
    "\n",
    "        # Extracting columns info\n",
    "        numeric_columns = [i for i in self.data.select_dtypes(include='number').columns]\n",
    "        all_columns = [i for i in self.data.columns]\n",
    "        other_columns = []\n",
    "        for i in all_columns:\n",
    "            if i not in numeric_columns and i not in column_to_exclude:\n",
    "                other_columns.append(i)\n",
    "\n",
    "        # Getting the total number of columns to be fitted\n",
    "        for categorical_column in other_columns:\n",
    "            if categorical_column == 'Department (D3)':\n",
    "                self.analysis_log[categorical_column] = {}\n",
    "                unique_categories = set(self.data[categorical_column])\n",
    "                unique_categories = list(unique_categories)\n",
    "                for category in unique_categories:\n",
    "                    if category not in [j for j in set([i for i in self.anomalies_log['department_D3']])]: \n",
    "                        continue\n",
    "                    if ('patchy higher' in [i for i in self.anomalies_log['anomaly type']]) or  ('patchy lower' in [i for i in self.anomalies_log['anomaly type']]) or  ('higher value' in [i for i in self.anomalies_log['anomaly type']]) or ('lower value' in [i for i in self.anomalies_log['anomaly type']]) or ('extremely higher value' in [i for i in self.anomalies_log['anomaly type']]):\n",
    "                        total_number_of_models = total_number_of_models +1\n",
    "\n",
    "                    if ('lower frequency' in [i for i in self.anomalies_log['anomaly type']]) or ('higher frequency' in [i for i in self.anomalies_log['anomaly type']]) :   \n",
    "                        time_frames = ['W','M','Q','Y']\n",
    "                        value_frequency = ['sum','count']\n",
    "                        for time_frame in time_frames:\n",
    "                            for count_sum in value_frequency:\n",
    "                                total_number_of_models = total_number_of_models + 1\n",
    "\n",
    "        # Fitting the actual models\n",
    "        for categorical_column in other_columns:\n",
    "            if categorical_column == 'Department (D3)':\n",
    "                self.analysis_log[categorical_column] = {}\n",
    "                unique_categories = set(self.data[categorical_column])\n",
    "                unique_categories = list(unique_categories)\n",
    "\n",
    "                for category in unique_categories:\n",
    "                    if category not in [j for j in set([i for i in self.anomalies_log['department_D3']])]:\n",
    "                        continue\n",
    "                    else:\n",
    "                        print('Analyzing: ',category)\n",
    "                        self.analysis_log[categorical_column][category]={}\n",
    "                        filtered_department = self.data[self.data[categorical_column]==category][[target_column,date_column]]\n",
    "                        invoice_ids = self.data[self.data[categorical_column]==category]['InvoiceID']\n",
    "                        filtered_department[date_column] = pd.to_datetime(filtered_department[date_column])\n",
    "                        # Fitting the invoices sequence model\n",
    "                        self.analysis_log[categorical_column][category]['sequenced']={target_column:np.array(filtered_department[target_column]),\n",
    "                                                                                date_column:np.array(filtered_department[date_column]),\n",
    "                                                                                'ids':np.array(invoice_ids),\n",
    "                                                                                'model_sequenced':None}\n",
    "                        object_sequenced = 0\n",
    "                        number_of_models = number_of_models +1\n",
    "                        print('So far fitted ',round((number_of_models/total_number_of_models)*100,2),\"% of total models\")\n",
    "\n",
    "                        y = np.array(filtered_department[target_column])\n",
    "                        integers = True\n",
    "                        object_sequenced = ForecastingModel(data=y,m=default_m, dates=np.array(filtered_department[date_column]),invoice_ids=np.array(invoice_ids),integers = integers)\n",
    "                        try:\n",
    "                            object_sequenced.full_automatic_bidirectional (a=default_alpha)\n",
    "                            \n",
    "\n",
    "\n",
    "                            \n",
    "                        except:\n",
    "                            print('A model could not be fitted',f\"{categorical_column} | {category} | sequenced \")\n",
    "                            number_of_failed_models = number_of_failed_models + 1\n",
    "\n",
    "                        self.analysis_log[categorical_column][category]['sequenced']['model_sequenced'] = object_sequenced\n",
    "\n",
    "\n",
    "                        # Getting the summary around the invoices sequence model\n",
    "                        if self.analysis_log[categorical_column][category]['sequenced']['model_sequenced'].model_type != {}:\n",
    "                            try:\n",
    "                                # Extracting info\n",
    "                                anomalies_indecies_numbers = [i for i in self.analysis_log[categorical_column][category]['sequenced']['model_sequenced'].results['anomalies_indecies'].keys()]\n",
    "\n",
    "                                anomalies_indecies_types = [i for i in self.analysis_log[categorical_column][category]['sequenced']['model_sequenced'].results['anomalies_indecies'].values()]\n",
    "\n",
    "                                number_of_anomalies = len(anomalies_indecies_numbers)\n",
    "                                anomalies_dates = self.analysis_log[categorical_column][category]['sequenced']['Date'][anomalies_indecies_numbers]\n",
    "                                anomalies_observed_values = self.analysis_log[categorical_column][category]['sequenced']['Invoice Spend'][anomalies_indecies_numbers]\n",
    "                                anomalies_expected_values = self.analysis_log[categorical_column][category]['sequenced']['model_sequenced'].results['predicted'][anomalies_indecies_numbers]\n",
    "                                anomalies_invoice_id = self.analysis_log[categorical_column][category]['sequenced']['ids'][anomalies_indecies_numbers]\n",
    "                                \n",
    "                                # Plugging them into the dictionary\n",
    "                                self.analysis_log[categorical_column][category]['sequenced']['anomalies_indecies_numbers'] = anomalies_indecies_numbers\n",
    "                                self.analysis_log[categorical_column][category]['sequenced']['anomalies_indecies_types'] = anomalies_indecies_types\n",
    "                                self.analysis_log[categorical_column][category]['sequenced']['number_of_anomalies'] = number_of_anomalies\n",
    "                                self.analysis_log[categorical_column][category]['sequenced']['anomalies_dates'] = anomalies_dates\n",
    "                                self.analysis_log[categorical_column][category]['sequenced']['anomalies_observed_values'] = anomalies_observed_values\n",
    "                                self.analysis_log[categorical_column][category]['sequenced']['anomalies_expected_values'] = anomalies_expected_values\n",
    "                                self.analysis_log[categorical_column][category]['sequenced']['anomalies_invoice_id'] = anomalies_invoice_id\n",
    "                            except:\n",
    "                                print('An issue occured while extracting values around the fitted model')\n",
    "                                self.analysis_log[categorical_column][category]['sequenced']['anomalies_indecies_numbers'] = None\n",
    "                                self.analysis_log[categorical_column][category]['sequenced']['anomalies_indecies_types'] = None\n",
    "                                self.analysis_log[categorical_column][category]['sequenced']['number_of_anomalies'] = None\n",
    "                                self.analysis_log[categorical_column][category]['sequenced']['anomalies_dates'] = None\n",
    "                                self.analysis_log[categorical_column][category]['sequenced']['anomalies_observed_values'] = None\n",
    "                                self.analysis_log[categorical_column][category]['sequenced']['anomalies_expected_values'] = None\n",
    "                                self.analysis_log[categorical_column][category]['sequenced']['anomalies_invoice_id'] = None\n",
    "                                \n",
    "\n",
    "                    if ('lower frequency' in [i for i in self.anomalies_log['anomaly type']]) or ('higher frequency' in [i for i in self.anomalies_log['anomaly type']]) :                             \n",
    "\n",
    "                        filtered_department.set_index(date_column, inplace=True)\n",
    "                        time_frames = ['W','M','Q','Y']\n",
    "                        corresponding_number_of_seasons = [1,12,4,1]\n",
    "                        value_frequency = ['sum','count']\n",
    "                        for time_frame in time_frames:\n",
    "                            self.analysis_log[categorical_column][category][time_frame]={}\n",
    "                            for count_sum in value_frequency:\n",
    "                                m_aggregated = corresponding_number_of_seasons[time_frames.index(time_frame)]\n",
    "                                try:\n",
    "                                    if count_sum == 'sum':\n",
    "                                        time_series = filtered_department[target_column].resample(time_frame).sum()\n",
    "                                        integers=True\n",
    "                                        time_series_date = np.array(time_series.index)\n",
    "                                        time_series = np.array(time_series)\n",
    "                                        time_series_ids = np.array([i for i in range(len(time_series))])\n",
    "                                        object = 0\n",
    "                                        object = ForecastingModel(time_series,m=m_aggregated,dates=time_series_date,invoice_ids=time_series_ids,integers=integers)\n",
    "                                        number_of_models = number_of_models + 1\n",
    "                                        print('So far fitted ',round((number_of_models/total_number_of_models)*100,2),\"% of total models\")\n",
    "                                        object.full_automatic(a=default_alpha)\n",
    "                                        \n",
    "                                        title = f\"{categorical_column} : {category}, {time_frame}, {count_sum}\"\n",
    "                                        indecies_anomalies = object.detected_anomalies['anomalies_indecies_numbers']\n",
    "                                        anomalies_types = object.detected_anomalies['anomalies_indecies_types']\n",
    "                                        dates_anomalies = object.detected_anomalies['anomalies_dates']\n",
    "\n",
    "                                        # Building the summary around the aggregate fitted models\n",
    "                                        self.analysis_log[categorical_column][category][time_frame][count_sum] = {'time_series':time_series,'time_series_date':time_series_date,\n",
    "                                                                                                            'number_of_anomalies':len(indecies_anomalies),\n",
    "                                                                                                                'anomalies_indecies':indecies_anomalies,\n",
    "                                                                                                                'anomalies_dates':dates_anomalies,\n",
    "                                                                                                                'anomalies_types':anomalies_types,\n",
    "                                                                                                                'model object':object}\n",
    "                                        \n",
    "                                    elif count_sum == 'count':\n",
    "                                        time_series = filtered_department[target_column].resample(time_frame).count()\n",
    "                                        integers=True\n",
    "                                        time_series_date = np.array(time_series.index)\n",
    "                                        time_series = np.array(time_series)\n",
    "                                        time_series_ids = np.array([i for i in range(len(time_series))])\n",
    "                                        object = 0\n",
    "                                        object = ForecastingModel(time_series,m=m_aggregated,dates=time_series_date,invoice_ids=time_series_ids,integers=integers)\n",
    "                                        number_of_models = number_of_models + 1\n",
    "                                        print('So far fitted ',round((number_of_models/total_number_of_models)*100,2),\"% of total models\")    \n",
    "                                        object.select_model_manually(True,True,True,True,a=default_alpha)                            \n",
    "                                        title = f\"{categorical_column} : {category}, {time_frame}, {count_sum}\"\n",
    "                                        indecies_anomalies = object.detected_anomalies['anomalies_indecies_numbers']\n",
    "                                        anomalies_types = object.detected_anomalies['anomalies_indecies_types']\n",
    "                                        dates_anomalies = object.detected_anomalies['anomalies_dates']\n",
    "\n",
    "                                        # Building the summary around the aggregate fitted models\n",
    "                                        self.analysis_log[categorical_column][category][time_frame][count_sum] = {'time_series':time_series,'time_series_date':time_series_date,\n",
    "                                                                                                            'number_of_anomalies':len(indecies_anomalies),\n",
    "                                                                                                                'anomalies_indecies':indecies_anomalies,\n",
    "                                                                                                                'anomalies_dates':dates_anomalies,\n",
    "                                                                                                                'anomalies_types':anomalies_types,\n",
    "                                                                                                                'model object':object}\n",
    "\n",
    "                                except:\n",
    "                                    print('A model could not be fitted',f\"{categorical_column} | {category} | {time_frame} | {count_sum}\")\n",
    "                                    number_of_failed_models = number_of_failed_models + 1\n",
    "                                    self.analysis_log[categorical_column][category][time_frame][count_sum] = {'time_series':time_series,'time_series_date':time_series_date,\n",
    "                                                                                                        'number_of_anomalies':None,\n",
    "                                                                                                            'anomalies_indecies':None,\n",
    "                                                                                                            'anomalies_dates':None,\n",
    "                                                                                                            'anomalies_types':None,\n",
    "                                                                                                            'model object':object}\n",
    "\n",
    "        \n",
    "\n",
    "        self.analysis_log['model numbers']={'total_number_of_models':total_number_of_models, 'number_of_failed_models':number_of_failed_models }\n",
    "        \n",
    "        # Save everything in a dictionary file pkl\n",
    "        # to transfer the results of the analysis from RAM to a file.\n",
    "        with open(path_dump_pkl, 'wb') as file:\n",
    "            # Use pickle.dump to save the dictionary to the file\n",
    "            pickle.dump(self.analysis_log, file)\n",
    "        print('A pkl version of the analysis_log was dumped in: ','\\n',\n",
    "            path_dump_pkl)        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Possible values of a (default is 0.05)\n",
    "        a_values = [1/10**i for i in range(50)]\n",
    "        a_values[-1]=0\n",
    "        self.resulting = {}\n",
    "        epsilon = 1e-6\n",
    "        for a in a_values:\n",
    "            if a == 1:\n",
    "                self.resulting[a]= {'true_anomalies_detected':20,\n",
    "                                    'true_anomalies_undetected':0,\n",
    "                                    'total_inserted_anomalies':20,\n",
    "                                    'total_detected_anomalies':100,\n",
    "                                    'detection_ratio':1,\n",
    "                                    'false_positive_rate':0.99}\n",
    "                continue\n",
    "\n",
    "            true_anomalies_detected = 0\n",
    "            true_anomalies_undetected = 0\n",
    "            total_inserted_anomalies = 0\n",
    "            total_detected_anomalies = 0\n",
    "            if len([j for j in set([i for i in self.anomalies_log['department_D3']])]) != 0:\n",
    "                for department_d3 in [j for j in set([i for i in self.anomalies_log['department_D3']])]:\n",
    "                    if [i for i in self.anomalies_log[self.anomalies_log['department_D3']==department_d3]['anomaly type']][0] == 'aggregate increase':\n",
    "                        continue\n",
    "                    else:\n",
    "                        if (len(self.analysis_log['Department (D3)'][department_d3]['sequenced']['model_sequenced'].model_type) != 0) and (self.analysis_log['Department (D3)'][department_d3]['sequenced']['model_sequenced'].detected_anomalies['number_of_anomalies'] != 0):\n",
    "\n",
    "                            self.analysis_log['Department (D3)'][department_d3]['sequenced']['model_sequenced'].reset_interval(a = a)\n",
    "\n",
    "                            anomalies_indecies_numbers = self.analysis_log['Department (D3)'][department_d3]['sequenced']['model_sequenced'].detected_anomalies_for_calculations['anomalies_indecies_numbers']\n",
    "\n",
    "                            anomalies_invoice_id = self.analysis_log['Department (D3)'][department_d3]['sequenced']['model_sequenced'].detected_anomalies_for_calculations['anomalies_invoice_id']\n",
    "\n",
    "                            total_inserted_anomalies = total_inserted_anomalies + self.anomalies_log[self.anomalies_log['department_D3']==department_d3].shape[0] \n",
    "                            total_detected_anomalies = total_detected_anomalies + len(anomalies_invoice_id)\n",
    "                            for inserted_anomaly in [i for i in self.anomalies_log[self.anomalies_log['department_D3']==department_d3]['invoice_ID']]:\n",
    "                                if inserted_anomaly in anomalies_invoice_id:\n",
    "                                    true_anomalies_detected = true_anomalies_detected + 1 \n",
    "                                else:\n",
    "                                    true_anomalies_undetected = true_anomalies_undetected + 1 \n",
    "                            falsely_detected_anomalies = total_detected_anomalies - true_anomalies_detected\n",
    "                            false_positive_rate = falsely_detected_anomalies/(total_detected_anomalies +epsilon )\n",
    "            self.resulting[a]= {'true_anomalies_detected':true_anomalies_detected,\n",
    "                                'true_anomalies_undetected':true_anomalies_undetected,\n",
    "                                'total_inserted_anomalies':total_inserted_anomalies,\n",
    "                                'total_detected_anomalies':total_detected_anomalies,\n",
    "                                'detection_ratio':true_anomalies_detected/(epsilon + total_inserted_anomalies),\n",
    "                                'false_positive_rate':false_positive_rate}\n",
    "\n",
    "        out = {'analysis_log':self.analysis_log,'anomalies_log':self.anomalies_log,'resulting':self.resulting,'data':self.data}\n",
    "\n",
    "        # Save everything in a dictionary file pkl\n",
    "        # to transfer the results of the analysis from RAM to a file.\n",
    "        with open(path_dump_pkl, 'wb') as file:\n",
    "            # Use pickle.dump to save the dictionary to the file\n",
    "            pickle.dump(out, file)\n",
    "        print('A pkl version of the results was dumped in: ','\\n',\n",
    "            path_dump_pkl)\n",
    "    \n",
    "    def performance_metrics(self,a_values):\n",
    "        \"\"\"Calculates performance metrics for anomaly detection at various confidence levels.\n",
    "\n",
    "    This method calculates and returns performance metrics for anomaly detection, such as true anomalies detected,\n",
    "    true anomalies undetected, total inserted anomalies, total detected anomalies, detection ratio, and false positive rate\n",
    "    for each specified confidence level.\n",
    "\n",
    "    Args:\n",
    "        a_values: A list of confidence levels to evaluate (e.g., [0.05, 0.01]).\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the performance metrics for each confidence level.\n",
    "        \"\"\"\n",
    "\n",
    "        # Possible values of a (default is 0.05)\n",
    "        a_values = a_values\n",
    "        resulting = {}\n",
    "        epsilon = 1e-6\n",
    "        for a in a_values:\n",
    "            true_anomalies_detected = 0\n",
    "            true_anomalies_undetected = 0\n",
    "            total_inserted_anomalies = 0\n",
    "            total_detected_anomalies = 0\n",
    "            if len([j for j in set([i for i in self.anomalies_log['department_D3']])]) != 0:\n",
    "                for department_d3 in [j for j in set([i for i in self.anomalies_log['department_D3']])]:\n",
    "                    if [i for i in self.anomalies_log[self.anomalies_log['department_D3']==department_d3]['anomaly type']][0] == 'aggregate increase':\n",
    "                        continue\n",
    "                    else:\n",
    "                        if (len(self.analysis_log['Department (D3)'][department_d3]['sequenced']['model_sequenced'].model_type) != 0) and (self.analysis_log['Department (D3)'][department_d3]['sequenced']['model_sequenced'].detected_anomalies['number_of_anomalies'] != 0):\n",
    "\n",
    "                            self.analysis_log['Department (D3)'][department_d3]['sequenced']['model_sequenced'].reset_interval(a = a)\n",
    "\n",
    "                            anomalies_indecies_numbers = self.analysis_log['Department (D3)'][department_d3]['sequenced']['model_sequenced'].detected_anomalies_for_calculations['anomalies_indecies_numbers']\n",
    "\n",
    "                            anomalies_invoice_id = self.analysis_log['Department (D3)'][department_d3]['sequenced']['model_sequenced'].detected_anomalies_for_calculations['anomalies_invoice_id']\n",
    "\n",
    "                            total_inserted_anomalies = total_inserted_anomalies + self.anomalies_log[self.anomalies_log['department_D3']==department_d3].shape[0] \n",
    "                            print('Department: ',department_d3)\n",
    "                            print('Added: ',self.anomalies_log[self.anomalies_log['department_D3']==department_d3].shape[0],' inserted anomalies')\n",
    "                            print('Added: ',len(anomalies_invoice_id),' detected anomalies')\n",
    "                            total_detected_anomalies = total_detected_anomalies + len(anomalies_invoice_id)\n",
    "                            for inserted_anomaly in [i for i in self.anomalies_log[self.anomalies_log['department_D3']==department_d3]['invoice_ID']]:\n",
    "                                if inserted_anomaly in anomalies_invoice_id:\n",
    "                                    true_anomalies_detected = true_anomalies_detected + 1 \n",
    "                                else:\n",
    "                                    true_anomalies_undetected = true_anomalies_undetected + 1 \n",
    "                            falsely_detected_anomalies = total_detected_anomalies - true_anomalies_detected\n",
    "                            false_positive_rate = falsely_detected_anomalies/(total_detected_anomalies +epsilon )\n",
    "            resulting[a]= {'true_anomalies_detected':true_anomalies_detected,\n",
    "                                'true_anomalies_undetected':true_anomalies_undetected,\n",
    "                                'total_inserted_anomalies':total_inserted_anomalies,\n",
    "                                'total_detected_anomalies':total_detected_anomalies,\n",
    "                                'detection_ratio':true_anomalies_detected/(epsilon + total_inserted_anomalies),\n",
    "                                'false_positive_rate':false_positive_rate}\n",
    "\n",
    "        return resulting\n",
    "\n",
    "\n",
    "\n",
    "    def inspect_performance_visually(self,a = 0.05):\n",
    "        \"\"\"Visual inspection of performance metrics for detected anomalies.\n",
    "\n",
    "    This method visually inspects and plots the performance of the anomaly detection models \n",
    "    by comparing detected anomalies with inserted anomalies for each department in the dataset.\n",
    "    It generates plots for sequenced and aggregated time frames to illustrate the detection results.\n",
    "\n",
    "    Args:\n",
    "        a (float, optional): Significance level for confidence interval calculation. Default is 0.05.\n",
    "        \"\"\"\n",
    "\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        import statsmodels.api as sm\n",
    "        import math\n",
    "        import statistics\n",
    "        from scipy.optimize import minimize\n",
    "        import scipy.stats as stats\n",
    "        import matplotlib.pyplot as plt\n",
    "        import warnings\n",
    "\n",
    "\n",
    "        #if type(self.data) != type(pd.DataFrame()) and self.data != {}:\n",
    "        #    raise ValueError('This method supports only dataframes and/or importing data from .pkl')\n",
    "        for department_d3 in [j for j in set([i for i in self.anomalies_log['department_D3']])]:\n",
    "            #  ([i for i in self.anomalies_log[self.anomalies_log['department_D3']==department_d3]['anomaly type']][0] == 'aggregate increase') or \n",
    "            if (len(self.analysis_log['Department (D3)'][department_d3]['sequenced']['model_sequenced'].model_type)==0):\n",
    "                continue\n",
    "\n",
    "            if [i for i in self.anomalies_log[self.anomalies_log['department_D3']==department_d3]['anomaly type']][0] in ['patchy higher','patchy lower','higher value','lower value','extremely higher value']:\n",
    "\n",
    "                print('Inserted anomalies: ',[ i for i in self.anomalies_log[self.anomalies_log['department_D3']==department_d3].loc[:,'invoice_ID']],[ i for i in self.anomalies_log[self.anomalies_log['department_D3']==department_d3].loc[:,'anomaly type']])\n",
    "                print('Detected anomalies: ',self.analysis_log['Department (D3)'][department_d3]['sequenced']['model_sequenced'].detected_anomalies['anomalies_invoice_id'],\n",
    "                    self.analysis_log['Department (D3)'][department_d3]['sequenced']['model_sequenced'].detected_anomalies['anomalies_indecies_types'])\n",
    "                \n",
    "                self.analysis_log['Department (D3)'][department_d3]['sequenced']['model_sequenced'].plot_results(title=f'{department_d3} | Sequenced',a=a,anomalies_recalculated = False)\n",
    "            '''\n",
    "\n",
    "\n",
    "            if [i for i in self.anomalies_log[self.anomalies_log['department_D3']==department_d3]['anomaly type']][0] in ['patchy higher','patchy lower','higher value','lower value','extremely higher value']:\n",
    "                for time_frame in ['W','M', 'Q', 'Y']:\n",
    "                    for sum_count in ['sum']:\n",
    "                        if len(self.analysis_log['Department (D3)'][department_d3][time_frame][sum_count]['model object'].model_type)==0:\n",
    "                            continue\n",
    "                        \n",
    "                        print('Detected anomalies: ',self.analysis_log['Department (D3)'][department_d3][time_frame][sum_count]['model object'].detected_anomalies['anomalies_invoice_id'],\n",
    "                            self.analysis_log['Department (D3)'][department_d3][time_frame][sum_count]['model object'].detected_anomalies['anomalies_indecies_types'])\n",
    "                        \n",
    "                        self.analysis_log['Department (D3)'][department_d3][time_frame][sum_count]['model object'].plot_results(title=f'{department_d3} | {time_frame} | {sum_count}',a=a,anomalies_recalculated = False)\n",
    "\n",
    "\n",
    "\n",
    "            if [i for i in self.anomalies_log[self.anomalies_log['department_D3']==department_d3]['anomaly type']][0] in ['higher frequency','lower frequency']:\n",
    "\n",
    "                print('Inserted anomalies: ',[ i for i in self.anomalies_log[self.anomalies_log['department_D3']==department_d3].loc[:,'anomaly type']])\n",
    "                for time_frame in ['W','M', 'Q', 'Y']:\n",
    "                    for sum_count in ['count']:\n",
    "                        if len(self.analysis_log['Department (D3)'][department_d3][time_frame][sum_count]['model object'].model_type)==0:\n",
    "                            continue\n",
    "                        \n",
    "                        print('Detected anomalies: ',self.analysis_log['Department (D3)'][department_d3][time_frame][sum_count]['model object'].detected_anomalies['anomalies_invoice_id'],\n",
    "                            self.analysis_log['Department (D3)'][department_d3][time_frame][sum_count]['model object'].detected_anomalies['anomalies_indecies_types'])\n",
    "                        \n",
    "                        self.analysis_log['Department (D3)'][department_d3][time_frame][sum_count]['model object'].plot_results(title=f'{department_d3} | {time_frame} | {sum_count}',a=a,anomalies_recalculated = False)\n",
    "            '''\n",
    "\n",
    "\n",
    "    def get_results_from_pkl(self,pkl_path = '/Workspace/Users/ashraf.zedane@nl.abnamro.com/data_simulation/Reduced_data/v_1/analysis_outcome.pkl'):\n",
    "        \"\"\"Load analysis results from a pickle file.\n",
    "\n",
    "    This method loads the results of a previous analysis from a specified pickle file.\n",
    "    The loaded data includes the analysis log, anomalies log, and resulting performance metrics.\n",
    "\n",
    "    Args:\n",
    "        pkl_path (str, optional): Path to the pickle file containing the analysis results. \n",
    "            Default is '/Workspace/Users/ashraf.zedane@nl.abnamro.com/data_simulation/Reduced_data/v_1/analysis_outcome.pkl'.\n",
    "        \"\"\"\n",
    "\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        import statsmodels.api as sm\n",
    "        import math\n",
    "        import statistics\n",
    "        from scipy.optimize import minimize\n",
    "        import scipy.stats as stats\n",
    "        import matplotlib.pyplot as plt\n",
    "        import warnings\n",
    "\n",
    "        #if self.data != {} :\n",
    "        #    raise ValueError('This method needs no data')        \n",
    "        import pickle\n",
    "        # Open the file for reading in binary mode\n",
    "        with open(pkl_path, 'rb') as file:\n",
    "            # Use pickle.load to load the dictionary from the file\n",
    "            loaded_analysis_outcome = pickle.load(file)\n",
    "\n",
    "        #self.data = loaded_analysis_outcome['data']\n",
    "        self.analysis_log = loaded_analysis_outcome['analysis_log']\n",
    "        self.anomalies_log = loaded_analysis_outcome['anomalies_log']\n",
    "        self.resulting = loaded_analysis_outcome['resulting']\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42c31bdd-2c15-402d-a8f2-18fac9e58ee1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Implementing the robust algorithm on the simulated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ddf8f30f-4a7d-44e6-b915-4aef74676c62",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "seeds = [i for i in range(1,10)]\n",
    "\n",
    "path_anomaly = \"/Workspace/Users/ashraf.zedane@nl.abnamro.com/data_simulation/Reduced_data/v_1/anomalies_log.xlsx\"\n",
    "path_dummy = \"/Workspace/Users/ashraf.zedane@nl.abnamro.com/data_simulation/Reduced_data/v_1/dummy.xlsx\"\n",
    "replication_outcome_path = '/Workspace/Users/ashraf.zedane@nl.abnamro.com/data_simulation/Reduced_data/v_1/replication_outcome.pkl'\n",
    "replication_outcome_path_temp = '/Workspace/Users/ashraf.zedane@nl.abnamro.com/data_simulation/Reduced_data/v_1/replication_outcome_temp.pkl'\n",
    "replication_outcome = {}\n",
    "for i in seeds:\n",
    "    simulate_data(seed = seeds,\n",
    "                Commodities_L1_number = 1,\n",
    "                    Departments_L1_number = 2,\n",
    "                    Account_level_number = 1,\n",
    "                    Suppliers_number = 2,\n",
    "                    start_year = 2022,\n",
    "                    end_year = 2023,\n",
    "                    number_of_anomalies = 40,\n",
    "                    path = '/Workspace/Users/ashraf.zedane@nl.abnamro.com/data_simulation/Reduced_data/v_1/',\n",
    "                    data_name_str = 'dummy.xlsx',\n",
    "                    anomalies_log_str = 'anomalies_log.xlsx',\n",
    "                    mu = [100000,300000],sdv = [50000,100000],\n",
    "                    rtrend = [5000,25000])\n",
    "    \n",
    "    anomalies_dummy = pd.read_excel(path_anomaly)\n",
    "    # Import the complete simulation data\n",
    "    \n",
    "    data_dummy = pd.read_excel(path_dummy)\n",
    "    df = data_dummy\n",
    "\n",
    "    The_only_model =ForecastingModel(data=df)\n",
    "    The_only_model.detect_anomalies_from_dataframe(default_alpha = 0.0005)\n",
    "\n",
    "    replication_outcome[i] = The_only_model.resulting\n",
    "\n",
    "    import pickle\n",
    "    # Save everything in a dictionary file pkl\n",
    "    # to transfer the results of the analysis from RAM to a file.\n",
    "    with open(replication_outcome_path_temp, 'wb') as file:\n",
    "        # Use pickle.dump to save the dictionary to the file\n",
    "        pickle.dump(replication_outcome, file)\n",
    "    print('A pkl version of the replication_outcome_temp was dumped in: ','\\n',\n",
    "        replication_outcome_path_temp)  \n",
    "\n",
    "import pickle\n",
    "# Save everything in a dictionary file pkl\n",
    "# to transfer the results of the analysis from RAM to a file.\n",
    "with open(replication_outcome_path, 'wb') as file:\n",
    "    # Use pickle.dump to save the dictionary to the file\n",
    "    pickle.dump(replication_outcome, file)\n",
    "print('A pkl version of the replication_outcome was dumped in: ','\\n',\n",
    "    replication_outcome_path)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "960c96d9-ab35-4fab-93fd-e1dc0175cefb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "replication_outcome_path = '/Workspace/Users/ashraf.zedane@nl.abnamro.com/data_simulation/Reduced_data/v_1/replication_outcome.pkl'\n",
    "import pickle\n",
    "# Open the file for reading in binary mode\n",
    "with open(replication_outcome_path, 'rb') as file:\n",
    "    # Use pickle.load to load the dictionary from the file\n",
    "    loaded_replication_outcome = pickle.load(file)\n",
    "loaded_replication_outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e108b94e-3cd2-46eb-82d9-573c36b0d750",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "alphas = [i for i in loaded_replication_outcome[1].keys()]\n",
    "detection = []\n",
    "false_positive = []\n",
    "for alpha in alphas:\n",
    "    detection_ratio_temp= []\n",
    "    false_positive_rate_temp = []\n",
    "    for i in loaded_replication_outcome.keys():\n",
    "        detection_ratio_temp.append(loaded_replication_outcome[i][alpha]['detection_ratio'])\n",
    "        false_positive_rate_temp.append(loaded_replication_outcome[i][alpha]['false_positive_rate'])\n",
    "    detection_ratio = np.mean(detection_ratio_temp)\n",
    "    false_positive_rate = np.mean(false_positive_rate_temp)\n",
    "    detection.append(detection_ratio)\n",
    "    false_positive.append(false_positive_rate)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce6a8a1a-1da2-4bf5-a2be-16988a6278b9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Inspecting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d0054be-3ed8-469b-a547-cf6a95c0d18f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Example data\n",
    "fpr1 = false_positive\n",
    "tpr1 = detection\n",
    "alpha1 = alphas\n",
    "\n",
    "fpr2 = [0.0, 0.1, 0.15, 0.25, 0.35, 0.45, 0.55, 0.65, 0.75, 0.85, 1.0]\n",
    "tpr2 = [0.0, 0.1, 0.25, 0.5, 0.65, 0.75, 0.8, 0.85, 0.9, 0.95, 1.0]\n",
    "alpha2 = [0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "# Plotting the ROC curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr1, tpr1, marker='o', linestyle='-', label='Predictor 1')\n",
    "#plt.plot(fpr2, tpr2, marker='o', linestyle='-', label='Predictor 2')\n",
    "for i ,alpha in enumerate(alpha1):\n",
    "    plt.annotate(f\"{alpha}\",(fpr1[i],tpr1[i]),textcoords = 'offset points',xytext=(0,10),ha ='center',fontsize=6)\n",
    "\n",
    "\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve Comparison')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "# Display the plot\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85ee0fe9-7a1d-4c43-ac34-8bf5f5b7accd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# The Non-robust exponential smoothing\n",
    "## The reference method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44abff8d-44d5-49bb-9fc0-a3e217c443e5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## The Non-robust algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9fe26e01-cfc3-4e2f-b32e-14f799f3a790",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import math\n",
    "import statistics\n",
    "from scipy.optimize import minimize\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "np.random.seed(1)\n",
    "# Return sign        \n",
    "def sign(x):\n",
    "   '''\n",
    "   A function that returns the signal of a number.\n",
    "   Takes the number as an argument.\n",
    "   Returns either -1 or 1\n",
    "   '''\n",
    "   return int(math.copysign(1, x))\n",
    "\n",
    "\n",
    "# Huber function to limit the effect of extreme values:\n",
    "def huber(x,k_huber=3):\n",
    "   '''\n",
    "   A function that prevent observations from having unbounded values\n",
    "   it has a seiling that never goes above no matter what the given value is\n",
    "   Takes a number that is under study, and the ceiling value:k_huber\n",
    "   returns a value that processes the given number but never exceeds the seiling\n",
    "   '''\n",
    "   k_huber = 3\n",
    "   if abs(x)<k_huber:\n",
    "        return x\n",
    "   else:\n",
    "        return sign(x)*k_huber\n",
    "\n",
    "# Biweight function\n",
    "def get_biweight(x,ck=5,k=1.75):\n",
    "   '''\n",
    "   A function that is used to assign weights based on a distance measure.\n",
    "   The values of the constants: ck and k describe how the assigned weight changes\n",
    "   Takes a number and the constants values\n",
    "   Returns the weight that the number should carry\n",
    "   '''\n",
    "   ck = 4.685\n",
    "   k = 3\n",
    "   if abs(x)<k:\n",
    "      return ck*(1-(1-(x/k)**2)**3)\n",
    "   else:\n",
    "      return ck\n",
    "\n",
    "# Estimating sigma_hat from the privious sigma\n",
    "def get_sigma_hat(k,ck,previous_sigma,observed,predicted,lmbda,multiplicative_error=True,integers = False):\n",
    "    '''\n",
    "    Estimates sigma_hat on each step by taking previous sigma into account in addition to the observed and predicted values\n",
    "    lmbda is the smoothing parameter, estimated through the optimization process\n",
    "    Based on whether the error is multiplicative of additive, calculations are different\n",
    "    The values of k and ck affect the get_biweight() function.\n",
    "    Takes observed value,predicted value, previous sigma, lmbda and the error specification\n",
    "    Returns an estimate of the current sigma\n",
    "    '''\n",
    "    # To avoid division on zero, especially when dealing with anomalous values.\n",
    "    epsilon = 1e-8 \n",
    "    if integers:\n",
    "      epsilon_sigma = 5e-2\n",
    "    else:\n",
    "      epsilon_sigma = 0\n",
    "    \n",
    "    if multiplicative_error:\n",
    "      sigma_hat_squared = lmbda * get_biweight((observed-predicted)/(predicted * previous_sigma+epsilon),ck=ck,k=k) * (previous_sigma)**2 + (1-lmbda) * (previous_sigma)**2\n",
    "      if sigma_hat_squared<0:\n",
    "         print('Negative value: ',sigma_hat_squared,'\\n',\n",
    "               'lmbda: ',lmbda,' previous_sigma: ',previous_sigma,'\\n',\n",
    "               'observed: ',observed,' .predicted: ',predicted,'\\n',\n",
    "               'get_biweight: ',get_biweight((observed-predicted)/(predicted * previous_sigma+epsilon),ck=ck,k=k),'\\n')\n",
    "      sigma_hat = math.sqrt(sigma_hat_squared)\n",
    "      return max(sigma_hat,epsilon_sigma)\n",
    "        \n",
    "    else:\n",
    "      sigma_hat_squared = lmbda * get_biweight((observed-predicted)/(previous_sigma+epsilon),ck = ck,k=k) * (previous_sigma)**2 + (1-lmbda) * (previous_sigma)**2\n",
    "      if sigma_hat_squared<0:\n",
    "         print('Negative value: ',sigma_hat_squared,'\\n',\n",
    "               'lmbda: ',lmbda,' previous_sigma: ',previous_sigma,'\\n',\n",
    "               'observed: ',observed,' .predicted: ',predicted,'\\n',\n",
    "               'get_biweight: ',get_biweight((observed-predicted)/(predicted * previous_sigma+epsilon),ck=ck,k=k),'\\n')\n",
    "      sigma_hat = math.sqrt(sigma_hat_squared)\n",
    "      return max(sigma_hat,epsilon_sigma)\n",
    "    \n",
    "\n",
    "\n",
    "# Clean observations:\n",
    "def get_clean_observation(k,observed ,predicted, sigma_hat,multiplicative_error=True):\n",
    "    '''\n",
    "    Clean observations are observations whose values are adjusted to limit the influece of extreme data points.\n",
    "    Observed values and predictions in addition to the sigma_hat are necessary to set the threshold of where to start \"cleaning\"\n",
    "    The clean observations are used to obtain the predicted values for the time series and not the actual observations\n",
    "    The value of k influences the huber() function\n",
    "    Based on whether the error is multiplicative or additive, the calculations differ\n",
    "    Takes observed ,predicted, sigma_hat and the error specification\n",
    "    Returns an observation that is the same as the actual observation if its value is plausible and a clean adjusted \n",
    "    observation if its value is extreme.\n",
    "    '''\n",
    "    # To avoid division on zero when dealing with anomalies\n",
    "    epsilon = 1e-8 \n",
    "    if multiplicative_error:\n",
    "      clean_observation = (1+ huber((observed-predicted)/(predicted*sigma_hat+epsilon))*sigma_hat)*predicted\n",
    "      return clean_observation\n",
    "    else:\n",
    "      clean_observation = huber((observed-predicted)/(sigma_hat+epsilon))*sigma_hat + predicted\n",
    "      return clean_observation\n",
    "    \n",
    "\n",
    "# MAD Median Absolute Deviance\n",
    "def get_MAD(errors):\n",
    "  '''\n",
    "  This is an alternative to the mean() for when doing robust calculations against outliers\n",
    "  Because outliers can have a greater effect on means, medians represent a much more robust alternative\n",
    "  Takes errors: if errors are additive: (difference between observed and predicted values)\n",
    "                if errors are multiplicative: (ratio between observed and predicted values)\n",
    "  Returns the Median Absolute Deviation MAD\n",
    "  '''\n",
    "  # Calculate the absolute deviations from the median\n",
    "  med = np.median(errors)\n",
    "  absolute_error = np.abs(errors-med)\n",
    "  # Calculate the MAD\n",
    "  MAD = 1.4826 * np.median(absolute_error)\n",
    "  return MAD\n",
    "\n",
    "\n",
    "def get_t_tilde(t, m):\n",
    "  '''\n",
    "  To compute the right moment of the seasonal cycle\n",
    "  Takes the index of an observation and the length of one season.\n",
    "  Returns the place of this observation is the seasonal cycle\n",
    "  '''\n",
    "  t_tilde = (t % m)\n",
    "  return t_tilde\n",
    "\n",
    "\n",
    "def tau_squared(e,ck,k):\n",
    "   '''\n",
    "   A robust alternative of the sum of squares\n",
    "   It is used in the target function of the optimization process as the quantity to optimize\n",
    "   as it calculates how distant the observations are from a supposed center which is here MAD\n",
    "   Takes errors: if errors are additive: (difference between observed and predicted values)\n",
    "                 if errors are multiplicative: (ratio between observed and predicted values)\n",
    "   Returns the quantity of tau^2\n",
    "   '''\n",
    "   # To avoid division on zero especially when dealing with anomalous values\n",
    "   epsilon = 1e-6\n",
    "   s_T = get_MAD(e)\n",
    "   \n",
    "   TT = len(e)\n",
    "   ratio  = (s_T**2/TT)\n",
    "   summ = 0\n",
    "   \n",
    "   for i in range(len(e)):\n",
    "      x=e[i]/(s_T+epsilon)\n",
    "      summ = summ + get_biweight((e[i]/(s_T+epsilon)),ck=ck,k=k)\n",
    "   tau_2 = ratio * summ\n",
    "   if tau_2<0:\n",
    "      print('Negative tau_squared value encountered', tau_2)\n",
    "      raise ValueError('Negative tau_squared value encountered')\n",
    "   return tau_2\n",
    "\n",
    "\n",
    "def rob_lik_A (observed,predicted,ck,k):\n",
    "   '''\n",
    "   Getting the robust likelihood for the additive error case\n",
    "   Here, tau_squared() function is utilized\n",
    "   '''\n",
    "   e = observed - predicted\n",
    "   TT = len(e)\n",
    "   tau2 = tau_squared(e,ck=ck,k=k)\n",
    "   if tau2<0:\n",
    "      print('Encountered a negative value of tau_squared',tau2)\n",
    "   roblik_A = (-TT/2) * math.log(tau2)\n",
    "   return roblik_A\n",
    "\n",
    "def get_multiplicative_error(observed,predicted):\n",
    "   '''\n",
    "   For the multiplicative case, errors are not the difference between observed and predicted.\n",
    "   Here, error is the ratio between the difference and the predicted\n",
    "   '''\n",
    "   epsilon = 1e-8 \n",
    "   e = np.zeros(len(observed))\n",
    "   for i in range(len(observed)):\n",
    "    e[i] = (observed[i]-predicted[i])/(predicted[i]+epsilon)\n",
    "   return e\n",
    "\n",
    "def rob_lik_M(observed,predicted,ck,k):\n",
    "   '''\n",
    "   Calculating the robust likelihood for the multiplicative case\n",
    "   Here, tau_squared() function is utilized\n",
    "   '''\n",
    "   e = get_multiplicative_error(observed,predicted)\n",
    "   TT = len(e)\n",
    "   tau2 = tau_squared(e,ck=ck,k=k)\n",
    "   if tau2<0:\n",
    "      print('Encountered a negative value of tau_squared',tau2)\n",
    "   roblik_M = (-TT/2) * math.log(tau2)\n",
    "   return roblik_M\n",
    "\n",
    "\n",
    "def get_startup_period(seasonality,y,m):\n",
    "   '''\n",
    "   The startup period is the number of observations at the beginning of the series that will be used\n",
    "   to obtain all initial vales from. It is estimated differently based on whether we have a seasonal component or not\n",
    "   '''\n",
    "   if seasonality :\n",
    "      startup_period = max(5*m,10)\n",
    "   else:\n",
    "      startup_period = 10\n",
    "   if len(y)<startup_period:\n",
    "      warnings.warn(\"There are too few data points to obtain robust starting values.\", FutureWarning)\n",
    "      startup_period = len(y)\n",
    "   return startup_period\n",
    "\n",
    "def get_b_0(startup_period,y):\n",
    "    # b_0_hat : Getting the median of i for the medians of j\n",
    "   amounts_i =[]\n",
    "   for i in range(startup_period):\n",
    "      amounts_j = []\n",
    "      for j in range(startup_period):\n",
    "         if i != j:\n",
    "            amounts_j.append((y[i]-y[j])/(i-j))\n",
    "      median_j = statistics.median(amounts_j)\n",
    "      amounts_i.append(median_j)\n",
    "   b_0_hat = statistics.median(amounts_i)\n",
    "   return b_0_hat\n",
    "\n",
    "def get_l_o(y,startup_period,b_0_hat):\n",
    "   # L_0_hat : \n",
    "   startup_observations = y[:startup_period]\n",
    "   startup_index = range(startup_period)\n",
    "   l_0_hat = statistics.median([startup_observations[i]-b_0_hat*i for i in startup_index])\n",
    "   return l_0_hat\n",
    "\n",
    "def get_s_0(y,multiplicative_seasonality,m,startup_period,b_0_hat,l_0_hat):\n",
    "   # s_0_hat : Getting the seasonal starting values for all seasons\n",
    "   startup_observations = y[:startup_period]\n",
    "   startup_index = range(startup_period)\n",
    "   epsilon = 1e-6 # To avoid deviding by zero \n",
    "\n",
    "   if multiplicative_seasonality:\n",
    "      for q in range(m):\n",
    "         globals()[f'S_{q}'] = []\n",
    "         for i in range(q, startup_period, m):\n",
    "            ratio = startup_observations[i] /((startup_index[i]+1)*b_0_hat +l_0_hat+epsilon )\n",
    "            globals()[f'S_{q}'].append(ratio)\n",
    "         globals()[f'S_{q}'] = statistics.median(globals()[f'S_{q}'])\n",
    "\n",
    "   else:\n",
    "      for q in range(m):\n",
    "         globals()[f'S_{q}'] = []\n",
    "         for i in range(q, startup_period, m):\n",
    "            difference = startup_observations[i] - ((startup_index[i]+1)*b_0_hat +l_0_hat )\n",
    "            globals()[f'S_{q}'].append(difference)\n",
    "         globals()[f'S_{q}'] = statistics.median(globals()[f'S_{q}'])\n",
    "\n",
    "   seasonal_components = [globals()[f'S_{q}'] for q in range(m)]\n",
    "   return seasonal_components\n",
    "\n",
    "def get_sigma_0(y,seasonality,multiplicative_error,multiplicative_seasonality,startup_period,m,l_0_hat,b_0_hat,integers):\n",
    "   # sigma_0_hat\n",
    "   startup_observations = y[:startup_period]\n",
    "   epsilon = 1e-6 # To avoid deviding by zero \n",
    "\n",
    "   if integers:\n",
    "      epsilon_sigma = 5e-2\n",
    "   else:\n",
    "      epsilon_sigma = 0\n",
    "\n",
    "   if seasonality:\n",
    "      if multiplicative_error:\n",
    "         if not multiplicative_seasonality:\n",
    "            error_components =[]\n",
    "            for t in range(startup_period):\n",
    "               \n",
    "               t_tilde = get_t_tilde(t=t,m=m)\n",
    "               called_seasonality = globals()[f'S_{t_tilde}']\n",
    "               error_components.append((startup_observations[t]-l_0_hat-b_0_hat*(t)-called_seasonality)/(l_0_hat-b_0_hat*(t)-called_seasonality+epsilon))\n",
    "            sigma_o_hat = get_MAD(error_components)\n",
    "            sigma_o_hat = max(sigma_o_hat,epsilon_sigma)\n",
    "         else:\n",
    "            error_components =[]\n",
    "            for t in range(startup_period):\n",
    "               \n",
    "               t_tilde = get_t_tilde(t=t,m=m)\n",
    "               called_seasonality = globals()[f'S_{t_tilde}']\n",
    "               error_components.append((startup_observations[t]-(l_0_hat-b_0_hat*(t))/called_seasonality+epsilon)/((l_0_hat-b_0_hat*(t))/called_seasonality+epsilon))\n",
    "            sigma_o_hat = get_MAD(error_components)\n",
    "            sigma_o_hat = max(sigma_o_hat,epsilon_sigma)\n",
    "\n",
    "\n",
    "      else:\n",
    "         if not multiplicative_seasonality:\n",
    "            error_components =[]\n",
    "            for t in range(startup_period):\n",
    "               \n",
    "               t_tilde = get_t_tilde(t=t,m=m)\n",
    "               called_seasonality = globals()[f'S_{t_tilde}']\n",
    "               error_components.append(startup_observations[t]-l_0_hat-b_0_hat*(t)-called_seasonality)\n",
    "            sigma_o_hat = get_MAD(error_components)\n",
    "            sigma_o_hat = max(sigma_o_hat,epsilon_sigma)\n",
    "         else:\n",
    "            error_components =[]\n",
    "            for t in range(startup_period):\n",
    "               \n",
    "               t_tilde = get_t_tilde(t=t,m=m)\n",
    "               called_seasonality = globals()[f'S_{t_tilde}']\n",
    "               error_components.append(startup_observations[t]-(l_0_hat-b_0_hat*(t))/called_seasonality+epsilon)\n",
    "            sigma_o_hat = get_MAD(error_components) \n",
    "            sigma_o_hat = max(sigma_o_hat,epsilon_sigma)\n",
    "      return sigma_o_hat\n",
    "   else :\n",
    "      error_components =[]\n",
    "      for t in range(startup_period):\n",
    "         error_components.append((startup_observations[t]-l_0_hat-b_0_hat*(t))/(l_0_hat-b_0_hat*(t)+epsilon))\n",
    "      sigma_o_hat = get_MAD(error_components)\n",
    "      sigma_o_hat = max(sigma_o_hat,epsilon_sigma)\n",
    "      return sigma_o_hat\n",
    "   \n",
    "def get_starting_values(y,integers,m=None,trend = True ,seasonality = True,multiplicative_seasonality=True,multiplicative_error=True):\n",
    "   '''\n",
    "   Because the process is recursive, choosing the initial values of the parameters matters.\n",
    "   Choosing the initial values depends on some startup values : startup_period.\n",
    "   In a robust way, we estimate the initial values for the effects (NOT THE SMOOTHING PARAMETERS) which are: \n",
    "   level, trend, seasonality (one seasonality parameter for each element of the cycle m)\n",
    "   Initial parameters are obtained differently for when errors are multiplicative or additive\n",
    "   Initial parameters are obtained differently for when seasonality is multiplicative or additive\n",
    "   Returns a vector of the obtained initial parameters\n",
    "   '''\n",
    "   \n",
    "\n",
    "   \n",
    "   if seasonality and m==None:\n",
    "      raise ValueError(\"Please specify the number of seasons, or set seasonality to False\")\n",
    "   if not seasonality and m!=None :\n",
    "      raise ValueError(\"Seasonality is set to False and m is provided. Either set seasonality to True or m to None\")\n",
    "\n",
    "   startup_period = get_startup_period(y=y,seasonality=seasonality,m=m)\n",
    "   if not trend:\n",
    "      b_0_hat = 0\n",
    "   else :\n",
    "      b_0_hat = get_b_0(startup_period=startup_period,y=y)\n",
    "\n",
    "   l_0_hat = get_l_o(y=y,startup_period=startup_period,b_0_hat=b_0_hat)\n",
    "\n",
    "   if seasonality:\n",
    "      seasonal_components = get_s_0(y=y,multiplicative_seasonality=multiplicative_seasonality,m=m,\n",
    "                              startup_period=startup_period,b_0_hat=b_0_hat,l_0_hat=l_0_hat)\n",
    "   else :\n",
    "      if multiplicative_seasonality:\n",
    "         seasonal_components = 1\n",
    "      else:\n",
    "         seasonal_components = 0\n",
    "\n",
    "   sigma_o_hat = get_sigma_0(y=y,seasonality=seasonality,multiplicative_error=multiplicative_error,multiplicative_seasonality=multiplicative_seasonality,\n",
    "                             startup_period=startup_period,m=m,l_0_hat=l_0_hat,b_0_hat=b_0_hat,integers=integers)\n",
    "\n",
    "   return {'l_0_hat':l_0_hat,'b_0_hat': b_0_hat,'seasonal_components': seasonal_components,'sigma_o_hat': sigma_o_hat}     \n",
    "\n",
    "\n",
    "def forecast_equation(y,alpha,lmbda,integers,beta = None, gamma = None,m = None,seasonality=True,trend=True,\n",
    "                      ck=4.12, k=3,multiplicative_error=True,multiplicative_seasonality=True):\n",
    "    '''\n",
    "    Takes smoothing parameters, type of required model and the time series.\n",
    "    Returns a dictionary with the observaions, initial predictions, clean observations\n",
    "    Has three stages: 1. Initializing variables to store the evolving parameters, getting the starting initial values\n",
    "                      2. Updating the parameters as we go through the observations and getting initial predictions\n",
    "                         Those initial predictions will be used later to obtain the clean observations.\n",
    "                      3. Getting clean observations using the log we have for all sigma values and the initial predictions\n",
    "    '''\n",
    "    if seasonality:\n",
    "        if gamma == None:\n",
    "            raise ValueError(\"If seasonality is specified, gamma value is required \")\n",
    "    if trend:\n",
    "        if beta == None:\n",
    "            raise ValueError(\"If trend is specified, beta value is required \")\n",
    " \n",
    "    ################\n",
    "    #..First step...\n",
    "    ################\n",
    "    # Get initial value to initiate the recursive algorithm and feed them to the value log\n",
    "    starting_values = get_starting_values(y=y,trend=trend,seasonality=seasonality, m=m, multiplicative_seasonality=multiplicative_seasonality,\n",
    "                                            multiplicative_error=multiplicative_error,integers=integers)\n",
    "    epsilon = 1e-8 \n",
    "    #\n",
    "    # Initiating variables to keep the evolving parameters\n",
    "    l= np.zeros(len(y))\n",
    "    b= np.zeros(len(y))\n",
    "    sigma = np.zeros(len(y))\n",
    "    y_hat = np.zeros(len(y)+1)\n",
    "    if m != None :\n",
    "        s=np.zeros((m,len(y)//m+1))\n",
    "    clean_observations = np.zeros(len(y))\n",
    "    #\n",
    "    # Prepare starting values\n",
    "    if seasonality:\n",
    "        for season in range(m):\n",
    "            s[season,0]=starting_values['seasonal_components'][season]\n",
    "    else:\n",
    "        if multiplicative_seasonality:\n",
    "            s = 1\n",
    "        else:\n",
    "            s = 0\n",
    "    #\n",
    "    l[0] = starting_values['l_0_hat']\n",
    "    if trend:\n",
    "        b[0] = starting_values['b_0_hat']\n",
    "    else:\n",
    "        b = 0\n",
    "    sigma[0] = starting_values['sigma_o_hat']\n",
    "    h = 1   # one step ahead\n",
    "    #\n",
    "    ################\n",
    "    #..Second step..\n",
    "    ################\n",
    "    #\n",
    "    # Updating parameters and getting initial predictions (Not based on clean observations)\n",
    "    if trend:\n",
    "        if seasonality:\n",
    "            if  not multiplicative_seasonality:\n",
    "                # separaing initial values from the values of observation 0\n",
    "                l_0 = l[0]\n",
    "                b_0 = b[0]\n",
    "                s_0 = s[:,0]\n",
    "                sigma_0 = sigma[0]\n",
    "\n",
    "                # Forecasting equation \n",
    "                y_hat[0] = l_0 +   b_0 +   s_0[0]\n",
    "                \n",
    "                sigma_hat = get_sigma_hat(previous_sigma = sigma_0,observed = y[0] , \n",
    "                                  predicted = y_hat[0] ,  multiplicative_error=multiplicative_error,ck=ck,k=k,lmbda=lmbda,integers=integers)\n",
    "                sigma[0] = sigma_hat\n",
    "                clean_observation = get_clean_observation(observed = y[0] ,predicted = y_hat[0] ,\n",
    "                                                sigma_hat = sigma_hat , multiplicative_error=multiplicative_error,k=k)\n",
    "                clean_observations[0] = clean_observation\n",
    "\n",
    "                l[0] = alpha * (clean_observations[0] - s_0[0]) + (1-alpha)*(l_0 +   b_0)\n",
    "                b[0] = beta * (l[0] - l_0) + (1-beta) *   b_0\n",
    "                s[0,0] = gamma * (clean_observations[0] - l_0 -   b_0)+ (1-gamma) *s_0[0]\n",
    "                y_hat[1] = l[0] +   b[0] +   s[(1) % m ,max(0,((1) // m)-1)]\n",
    "\n",
    "\n",
    "                for t in range(1,len(y)):\n",
    "                    sigma_hat = get_sigma_hat(previous_sigma = sigma[max(0,t-1)],observed = y[t] , \n",
    "                                  predicted = y_hat[t] ,  multiplicative_error=multiplicative_error,ck=ck,k=k,lmbda=lmbda,integers=integers)\n",
    "                    sigma[t] = sigma_hat\n",
    "                    clean_observation = get_clean_observation(observed = y[t] ,predicted = y_hat[t] ,\n",
    "                                                   sigma_hat = sigma_hat , multiplicative_error=multiplicative_error,k=k)\n",
    "                    clean_observations[t] = clean_observation\n",
    "                    season_step = t % m \n",
    "                    season_cycle = t // m\n",
    "                    l[t] = alpha * (clean_observations[t] - s[season_step,max(0,season_cycle-1)]) + (1-alpha)*(l[max(0,t-1)] +   b[max(0,t-1)])\n",
    "                    b[t] = beta * (l[t] - l[max(0,t-1)]) + (1-beta) *   b[max(0,t-1)]\n",
    "                    s[season_step,t // m ] = gamma * (clean_observations[t] - l[max(0,t-1)] -   b[max(0,t-1)])+ (1-gamma) *s[season_step,max(0,season_cycle-1)]\n",
    "                    y_hat[t+h] = l[t] +   b[t] +   s[(t+1) % m ,((t+1) // m)-1]\n",
    "                    #\n",
    "            elif   multiplicative_seasonality:\n",
    "                # separaing initial values from the values of observation 0\n",
    "                l_0 = l[0]\n",
    "                b_0 = b[0]\n",
    "                s_0 = s[:,0]\n",
    "                sigma_0 = sigma[0]\n",
    "\n",
    "                # Forecasting equation\n",
    "                y_hat[0] = (l_0 +   b_0) *   s_0[0]\n",
    "                sigma_hat = get_sigma_hat(previous_sigma = sigma_0,observed = y[0] , \n",
    "                                  predicted = y_hat[0] ,  multiplicative_error=multiplicative_error,ck=ck,k=k,lmbda=lmbda,integers=integers)\n",
    "                sigma[0] = sigma_hat\n",
    "\n",
    "                clean_observation = get_clean_observation(observed = y[0] ,predicted = y_hat[0] ,\n",
    "                                                sigma_hat = sigma_hat , multiplicative_error=multiplicative_error,k=k)\n",
    "                \n",
    "                clean_observations[0] = clean_observation\n",
    "\n",
    "                l[0] = alpha * (clean_observations[0]/(s_0[0] + epsilon)) + (1-alpha)*(l_0 +   b_0)\n",
    "                b[0] = beta * (l[0] - l_0) + (1-beta) *   b_0\n",
    "                s[0,0] = gamma * (clean_observations[0]/(l_0 +   b_0+epsilon)) + (1-gamma) *s_0[0]\n",
    "                y_hat[1] = (l[0] +   b[0]) *s[(1) % m ,((1) // m)-1]\n",
    "\n",
    "                for t in range(1,len(y)):\n",
    "                    sigma_hat = get_sigma_hat(previous_sigma = sigma[max(0,t-1)],observed = y[t] , \n",
    "                                  predicted = y_hat[t] ,  multiplicative_error=multiplicative_error,ck=ck,k=k,lmbda=lmbda,integers=integers)\n",
    "                    sigma[t] = sigma_hat\n",
    "                    clean_observation = get_clean_observation(observed = y[t] ,predicted = y_hat[t] ,\n",
    "                                                   sigma_hat = sigma_hat , multiplicative_error=multiplicative_error,k=k)\n",
    "                    clean_observations[t] = clean_observation\n",
    "                    season_step = t % m \n",
    "                    season_cycle = t // m\n",
    "                    l[t] = alpha * (clean_observations[t]/(s[season_step,max(0,season_cycle-1)]+epsilon)) + (1-alpha)*(l[max(0,t-1)] +   b[max(0,t-1)])\n",
    "                    b[t] = beta * (l[t] - l[max(0,t-1)]) + (1-beta) *   b[max(0,t-1)]\n",
    "                    s[season_step,t // m ] = gamma * (clean_observations[t]/(l[max(0,t-1)] +   b[max(0,t-1)]+epsilon)) + (1-gamma) *s[season_step,max(0,season_cycle-1)]\n",
    "                    y_hat[t+1] = (l[t] +   b[t]) *s[(t+1) % m ,max(0,((t+1) // m)-1)]\n",
    "        else:\n",
    "            # separaing initial values from the values of observation 0\n",
    "            l_0 = l[0]\n",
    "            b_0 = b[0]\n",
    "            sigma_0 = sigma[0]\n",
    "\n",
    "            # Forecasting equation \n",
    "            y_hat[0] = l_0 +   b_0\n",
    "\n",
    "            sigma_hat = get_sigma_hat(previous_sigma = sigma_0,observed = y[0] , \n",
    "                                predicted = y_hat[0] ,  multiplicative_error=multiplicative_error,ck=ck,k=k,lmbda=lmbda,integers=integers)\n",
    "            sigma[0] = sigma_hat\n",
    "            clean_observation = get_clean_observation(observed = y[0] ,predicted = y_hat[0] ,\n",
    "                                            sigma_hat = sigma_hat , multiplicative_error=multiplicative_error,k=k)\n",
    "            clean_observations[0] = clean_observation\n",
    "\n",
    "            l[0] = alpha * (clean_observations[0] ) + (1-alpha)*(l_0 +   b_0)\n",
    "            b[0] = beta * (l[0] - l_0) + (1-beta) *   b_0\n",
    "\n",
    "            y_hat[1] = l[0] +   b[0]\n",
    "\n",
    "            for t in range(1,len(y)):\n",
    "                sigma_hat = get_sigma_hat(previous_sigma = sigma[max(0,t-1)],observed = y[t] , \n",
    "                                  predicted = y_hat[t] ,  multiplicative_error=multiplicative_error,ck=ck,k=k,lmbda=lmbda,integers=integers)\n",
    "                sigma[t] = sigma_hat\n",
    "                clean_observation = get_clean_observation(observed = y[t] ,predicted = y_hat[t] ,\n",
    "                                                   sigma_hat = sigma_hat , multiplicative_error=multiplicative_error,k=k)\n",
    "                clean_observations[t] = clean_observation\n",
    "                l[t] = alpha * (clean_observations[t] ) + (1-alpha)*(l[max(0,t-1)] +   b[max(0,t-1)])\n",
    "                b[t] = beta * (l[t] - l[max(0,t-1)]) + (1-beta) *   b[max(0,t-1)]\n",
    "                y_hat[t+h] = l[t] +   b[t]\n",
    "    else:\n",
    "        if seasonality:\n",
    "            if  not multiplicative_seasonality:\n",
    "\n",
    "                # separaing initial values from the values of observation 0\n",
    "                l_0 = l[0]\n",
    "                s_0 = s[:,0]\n",
    "                sigma_0 = sigma[0]\n",
    "\n",
    "                # Forecasting equation \n",
    "                y_hat[0] = l_0 +   s_0[0]\n",
    "                \n",
    "                sigma_hat = get_sigma_hat(previous_sigma = sigma_0,observed = y[0] , \n",
    "                                  predicted = y_hat[0] ,  multiplicative_error=multiplicative_error,ck=ck,k=k,lmbda=lmbda,integers=integers)\n",
    "                sigma[0] = sigma_hat\n",
    "                clean_observation = get_clean_observation(observed = y[0] ,predicted = y_hat[0] ,\n",
    "                                                sigma_hat = sigma_hat , multiplicative_error=multiplicative_error,k=k)\n",
    "                clean_observations[0] = clean_observation\n",
    "\n",
    "                l[0] = alpha * (clean_observations[0] - s_0[0]) + (1-alpha)*(l_0 )\n",
    "                s[0,0] = gamma * (clean_observations[0] - l_0 )+ (1-gamma) *s_0[0]\n",
    "                y_hat[1] = l[0] +  s[(1) % m ,max(0,((1) // m)-1)]\n",
    "\n",
    "                for t in range(1,len(y)):\n",
    "                    sigma_hat = get_sigma_hat(previous_sigma = sigma[max(0,t-1)],observed = y[t] , \n",
    "                                  predicted = y_hat[t] ,  multiplicative_error=multiplicative_error,ck=ck,k=k,lmbda=lmbda,integers=integers)\n",
    "                    sigma[t] = sigma_hat\n",
    "                    clean_observation = get_clean_observation(observed = y[t] ,predicted = y_hat[t] ,\n",
    "                                                   sigma_hat = sigma_hat , multiplicative_error=multiplicative_error,k=k)\n",
    "                    clean_observations[t] = clean_observation\n",
    "                    season_step = t % m \n",
    "                    season_cycle = t // m\n",
    "                    l[t] = alpha * (clean_observations[t] - s[season_step,max(0,season_cycle-1)]) + (1-alpha)*(l[max(0,t-1)])\n",
    "                    s[season_step,t // m ] = gamma * (clean_observations[t] - l[max(0,t-1)] )+ (1-gamma)* s[season_step,max(0,season_cycle-1)]\n",
    "                    y_hat[t+h] = l[t] +    s[(t+1) % m ,max(0,((t+1) // m)-1)]\n",
    "                    #\n",
    "            elif   multiplicative_seasonality:\n",
    "                # separaing initial values from the values of observation 0\n",
    "                l_0 = l[0]\n",
    "                s_0 = s[:,0]\n",
    "                sigma_0 = sigma[0]\n",
    "\n",
    "                # Forecasting equation\n",
    "                y_hat[0] = l_0  *   s_0[0]\n",
    "                sigma_hat = get_sigma_hat(previous_sigma = sigma_0,observed = y[0] , \n",
    "                                  predicted = y_hat[0] ,  multiplicative_error=multiplicative_error,ck=ck,k=k,lmbda=lmbda,integers=integers)\n",
    "                sigma[0] = sigma_hat\n",
    "\n",
    "                clean_observation = get_clean_observation(observed = y[0] ,predicted = y_hat[0] ,\n",
    "                                                sigma_hat = sigma_hat , multiplicative_error=multiplicative_error,k=k)\n",
    "                \n",
    "                clean_observations[0] = clean_observation\n",
    "\n",
    "                l[0] = alpha * (clean_observations[0]/(s_0[0] + epsilon)) + (1-alpha)*(l_0 )\n",
    "                s[0,0] = gamma * (clean_observations[0]/(l_0 +epsilon)) + (1-gamma) *s_0[0]\n",
    "                y_hat[1] = l[0]  *s[(1) % m ,((1) // m)-1]\n",
    "\n",
    "                for t in range(1,len(y)):\n",
    "                    sigma_hat = get_sigma_hat(previous_sigma = sigma[max(0,t-1)],observed = y[t] , \n",
    "                                  predicted = y_hat[t] ,  multiplicative_error=multiplicative_error,ck=ck,k=k,lmbda=lmbda,integers=integers)\n",
    "                    sigma[t] = sigma_hat\n",
    "                    clean_observation = get_clean_observation(observed = y[t] ,predicted = y_hat[t] ,\n",
    "                                                   sigma_hat = sigma_hat , multiplicative_error=multiplicative_error,k=k)\n",
    "                    clean_observations[t] = clean_observation\n",
    "                    season_step = t % m \n",
    "                    season_cycle = t // m\n",
    "                    l[t] = alpha * (clean_observations[t]/(s[season_step,max(0,season_cycle-1)]+epsilon)) + (1-alpha)*(l[max(0,t-1)])\n",
    "                    s[season_step,t // m ] = gamma * (clean_observations[t]/(l[max(0,t-1)] + epsilon)) + (1-gamma)* s[season_step,max(0,season_cycle-1)]\n",
    "                    y_hat[t+1] = (l[t] ) * s[(t+1) % m ,max(0,((t+1) // m)-1)]\n",
    "        else:\n",
    "            # separaing initial values from the values of observation 0\n",
    "            l_0 = l[0]\n",
    "            sigma_0 = sigma[0]\n",
    "\n",
    "            # Forecasting equation \n",
    "            y_hat[0] = l_0 \n",
    "\n",
    "            sigma_hat = get_sigma_hat(previous_sigma = sigma_0,observed = y[0] , \n",
    "                                predicted = y_hat[0] ,  multiplicative_error=multiplicative_error,ck=ck,k=k,lmbda=lmbda,integers=integers)\n",
    "            sigma[0] = sigma_hat\n",
    "            clean_observation = get_clean_observation(observed = y[0] ,predicted = y_hat[0] ,\n",
    "                                            sigma_hat = sigma_hat , multiplicative_error=multiplicative_error,k=k)\n",
    "            clean_observations[0] = clean_observation\n",
    "\n",
    "            l[0] = alpha * (clean_observations[0] ) + (1-alpha)*(l_0 )\n",
    "\n",
    "            y_hat[1] = l[0] \n",
    "\n",
    "            for t in range(1,len(y)):\n",
    "                sigma_hat = get_sigma_hat(previous_sigma = sigma[max(0,t-1)],observed = y[t] , \n",
    "                                  predicted = y_hat[t] ,  multiplicative_error=multiplicative_error,ck=ck,k=k,lmbda=lmbda,integers=integers)\n",
    "                sigma[t] = sigma_hat\n",
    "                clean_observation = get_clean_observation(observed = y[t] ,predicted = y_hat[t] ,\n",
    "                                                   sigma_hat = sigma_hat , multiplicative_error=multiplicative_error,k=k)\n",
    "                clean_observations[t] = clean_observation\n",
    "                l[t] = alpha * (clean_observations[t] ) + (1-alpha)*(l[max(0,t-1)] )\n",
    "                y_hat[t+h] = l[t]\n",
    "                #\n",
    "    y_hat = y_hat[:-1] # Taking out the last prediction (corresponds to non-existing observation)\n",
    "    #\n",
    "    ################\n",
    "    #..Third step...\n",
    "    ################\n",
    "    #\n",
    "    # Getting cleaned observations.\n",
    "    # Alternating between updating sigma_hat and a clean observation for every raw observation.\n",
    "    return {\"predicted\":y_hat,\"observed\" : y, \"cleaned\" :clean_observations,\"sigma\":sigma,\"l\":l,\"b\":b,\"s\":s}\n",
    "\n",
    "\n",
    "def get_robust_likelihood(params, y, m,trend,seasonality,multiplicative_error, multiplicative_seasonality,lmbda,integers,ck=4.685, k=3):\n",
    "    '''\n",
    "    Fits a smoothing model, obtains predictions and ,thereafter, residuals likelihood\n",
    "    uses the obtained likelihood to optimize the smoothing parameters.\n",
    "    Those optimized parameters will be later used to obtain clean observations on the following run\n",
    "    '''\n",
    "\n",
    "    #\n",
    "    if trend and seasonality:\n",
    "        lmbda, alpha, beta,  gamma  = params\n",
    "        forecasting = forecast_equation(multiplicative_error=multiplicative_error,multiplicative_seasonality=multiplicative_seasonality,\n",
    "                                        y=y,m=m,alpha=alpha,beta=beta, gamma=gamma ,lmbda = lmbda,ck=ck,k=k,seasonality=seasonality,\n",
    "                                        trend=trend,integers=integers)\n",
    "\n",
    "    # \n",
    "    if trend and not seasonality:\n",
    "        lmbda, alpha, beta  = params\n",
    "        forecasting = forecast_equation(multiplicative_error=multiplicative_error,multiplicative_seasonality=multiplicative_seasonality,\n",
    "                                        y=y,m=m,alpha=alpha,beta=beta, ck=ck,k=k,lmbda =lmbda ,seasonality=seasonality,\n",
    "                                        trend=trend,integers=integers)\n",
    "\n",
    "    #\n",
    "    if not trend and seasonality:\n",
    "        lmbda, alpha, gamma  = params\n",
    "        forecasting = forecast_equation(multiplicative_error=multiplicative_error,multiplicative_seasonality=multiplicative_seasonality,\n",
    "                                        y=y,m=m,alpha=alpha,ck=ck,k=k, lmbda = lmbda, seasonality=seasonality,gamma=gamma,\n",
    "                                        trend=trend,integers=integers)\n",
    "    #\n",
    "    if not trend and not seasonality:\n",
    "        lmbda, alpha = params\n",
    "        forecasting = forecast_equation(multiplicative_error=multiplicative_error,multiplicative_seasonality=multiplicative_seasonality,\n",
    "                                        y=y,m=m,alpha=alpha,ck=ck,k=k,lmbda = lmbda,seasonality=seasonality,\n",
    "                                        trend=trend,integers=integers)\n",
    "\n",
    "    predicted = forecasting['predicted']\n",
    "    observed = forecasting['observed']\n",
    "        \n",
    "    # Residuals are the difference between observed_1 and predicted_2 if error is additive\n",
    "    if not multiplicative_error:\n",
    "        robust_likelihood = rob_lik_A(observed=observed,predicted=predicted,ck=ck,k=k)\n",
    "    # Residuals are the ratio between the difference  and predicted_2 if error is multiplicative\n",
    "    else:\n",
    "        robust_likelihood = rob_lik_M(observed=observed,predicted=predicted,ck=ck,k=k)\n",
    "\n",
    "        \n",
    "    return - robust_likelihood\n",
    "\n",
    "\n",
    "def get_optimized_parameters(y, m,integers,seasonality=True,trend=True,multiplicative_error = True,multiplicative_seasonality=True,ck=4.685,k=3,\n",
    "                              method='Nelder-Mead'):  # Ck = 4.685 in Tukey's function | and k = 3 \n",
    "    '''\n",
    "    Performs optimization to find the best values of the smoothing parameters \n",
    "    Returns optimal parameters if success, or raises an error if the optimization failed.\n",
    "    '''\n",
    "\n",
    "    if trend and seasonality:\n",
    "        initial_guess = [0.5,0.5,0.5,0.5 ]\n",
    "        if len(y)<= len(initial_guess):\n",
    "            raise ValueError('Too few observations were given, model cannot be properly estimated')\n",
    "        up = 0.999\n",
    "        down = 0.001\n",
    "        bounds = [ (down, up), (down, up),(down, up),(down, up)]\n",
    "        args = (y, m,trend,seasonality,multiplicative_error,multiplicative_seasonality,ck,k,integers)\n",
    "\n",
    "    if trend and not seasonality:\n",
    "        initial_guess = [0.5,0.5,0.5 ]\n",
    "        if len(y)<= len(initial_guess):\n",
    "            raise ValueError('Too few observations were given, model cannot be properly estimated')\n",
    "        up = 0.999\n",
    "        down = 0.001\n",
    "        bounds = [  (down, up), (down, up), (down, up)]\n",
    "        args = (y, m,trend,seasonality  ,multiplicative_error,multiplicative_seasonality,ck,k,integers)\n",
    "\n",
    "    if not trend and seasonality:\n",
    "        initial_guess = [0.5,0.5,0.5]\n",
    "        if len(y)<= len(initial_guess):\n",
    "            raise ValueError('Too few observations were given, model cannot be properly estimated')\n",
    "        up = 0.999\n",
    "        down = 0.001\n",
    "        bounds = [(down, up), (down, up),(down, up)]\n",
    "        args = (y, m,trend,seasonality  ,multiplicative_error,multiplicative_seasonality,ck,k,integers)\n",
    "\n",
    "    if not trend and not seasonality:\n",
    "        initial_guess = [0.5,0.5]\n",
    "        if len(y)<= len(initial_guess):\n",
    "            raise ValueError('Too few observations were given, model cannot be properly estimated')\n",
    "        up = 0.999\n",
    "        down = 0.001\n",
    "        bounds = [(down, up),(down, up)]\n",
    "        args = (y, m,trend,seasonality  ,multiplicative_error,multiplicative_seasonality,ck,k,integers)\n",
    "\n",
    "    result = minimize(fun=get_robust_likelihood, args=args, \n",
    "                      x0=initial_guess, method=method, bounds = bounds ,options={'maxiter': 10000})\n",
    "    optimum = {\"lmbda\":None,\"alpha\":None,\"beta\":None, \n",
    "                       \"gamma\":None}\n",
    "    if result.success:\n",
    "        if trend and seasonality:\n",
    "            optimum['lmbda'] =result.x[0]\n",
    "            optimum['alpha'] = result.x[1]\n",
    "            optimum['beta'] = result.x[2]\n",
    " \n",
    "            optimum['gamma'] = result.x[3]\n",
    "        if trend and not seasonality:\n",
    "            optimum['lmbda'] =result.x[0]\n",
    "            optimum['alpha'] = result.x[1]\n",
    "            optimum['beta'] = result.x[2]\n",
    " \n",
    "\n",
    "\n",
    "        if not trend and seasonality:\n",
    "            optimum['lmbda'] =result.x[0]\n",
    "            optimum['alpha'] = result.x[1]\n",
    "            optimum['gamma'] = result.x[2]\n",
    "\n",
    "        if not trend and not seasonality:\n",
    "            optimum['lmbda'] =result.x[0]\n",
    "            optimum['alpha'] = result.x[1]\n",
    "\n",
    "        return  optimum # Return the optimized parameters\n",
    "    else:\n",
    "        raise ValueError(\"Optimization failed: \" + result.message)\n",
    "    \n",
    "\n",
    "def fit_model(seasonality,trend,multiplicative_error,multiplicative_seasonality,y,m,integers,ck=4.685, k=3,initial_guess = [0.8,0.4,0.8,0.4],method='Nelder-Mead'):\n",
    "    '''\n",
    "    A function that fits the exponential smoothing based on the cleaned observations and returns robust likelihood\n",
    "    This represents the second run of the algorithm. The first run was only to obtain the clean observations.\n",
    "    This second run aims to get the robust likelihood that is not affected by the presence of outliers.\n",
    "    This likelihood will be used to robustly compare between different models, with out the influence of outliers.\n",
    "    The steps:\n",
    "        1. Gets initial smoothing parameters that are suitable for the raw observations to start the algorithm\n",
    "        2. fits a model using the initial parameters on the raw observations\n",
    "        3. gets clean observations from the first run\n",
    "        4. gets secondary smoothig parameters that are suitable for the cleaned observations\n",
    "        5. fits a model using the secondary parameters and the clean observations to obtain predictions\n",
    "        6. calculates residuals and likelihood based on the difference between clean observations and their predictions\n",
    "    '''\n",
    "    if seasonality and trend:\n",
    "        initial_smoothing_parameters = get_optimized_parameters(y = y, m = m,trend=trend, seasonality=seasonality,multiplicative_error = multiplicative_error,\n",
    "                                                            multiplicative_seasonality=multiplicative_seasonality,ck=ck,k=k,\n",
    "                                                            method=method,integers=integers)\n",
    "        alpha = initial_smoothing_parameters['alpha']\n",
    "        beta = initial_smoothing_parameters['beta']\n",
    " \n",
    "        gamma = initial_smoothing_parameters['gamma']\n",
    "        lmbda = initial_smoothing_parameters['lmbda']\n",
    "        opt_parameters = {'alpha':alpha,'beta':beta, 'gamma':gamma,'lmbda':lmbda}\n",
    "\n",
    "        # First casting phase with raw observations to obtain clean observations\n",
    "        forecasting = forecast_equation(trend=trend, seasonality=seasonality,multiplicative_error=multiplicative_error,multiplicative_seasonality=multiplicative_seasonality,\n",
    "                                        y=y,m=m,alpha=alpha,beta=beta, lmbda = lmbda, gamma=gamma,ck=ck,k=k,integers=integers)\n",
    "        predicted = forecasting['predicted']\n",
    "        cleaned = forecasting['cleaned']\n",
    "        observed = forecasting['observed']\n",
    "        \n",
    "        \n",
    "    if not seasonality and trend:\n",
    "        initial_smoothing_parameters = get_optimized_parameters(y = y, m = m,trend=trend, seasonality=seasonality,multiplicative_error = multiplicative_error,\n",
    "                                                    multiplicative_seasonality=multiplicative_seasonality,ck=ck,k=k,\n",
    "                                                    method=method,integers=integers)\n",
    "        alpha = initial_smoothing_parameters['alpha']\n",
    "        beta = initial_smoothing_parameters['beta']\n",
    " \n",
    "        gamma = None\n",
    "        lmbda = initial_smoothing_parameters['lmbda']\n",
    "        opt_parameters = {'alpha':alpha,'beta':beta, 'gamma':gamma,'lmbda':lmbda}\n",
    "\n",
    "        # First casting phase with raw observations to obtain clean observations\n",
    "        forecasting = forecast_equation(trend=trend, seasonality=seasonality,multiplicative_error=multiplicative_error,multiplicative_seasonality=multiplicative_seasonality,\n",
    "                                        y=y,m=m,alpha=alpha,beta=beta, lmbda = lmbda , gamma=gamma,ck=ck,k=k,integers=integers)\n",
    "        predicted = forecasting['predicted']\n",
    "        cleaned = forecasting['cleaned']\n",
    "        observed = forecasting['observed']\n",
    "        \n",
    "    if seasonality and not trend:\n",
    "        initial_smoothing_parameters = get_optimized_parameters(y = y, m = m,trend=trend, seasonality=seasonality,multiplicative_error = multiplicative_error,\n",
    "                                                    multiplicative_seasonality=multiplicative_seasonality,ck=ck,k=k,\n",
    "                                                    method=method,integers=integers)\n",
    "        alpha = initial_smoothing_parameters['alpha']\n",
    "        beta = None\n",
    " \n",
    "        gamma = initial_smoothing_parameters['gamma']\n",
    "        lmbda = initial_smoothing_parameters['lmbda']\n",
    "\n",
    "        opt_parameters = {'alpha':alpha,'beta':beta, 'gamma':gamma,'lmbda':lmbda}\n",
    "\n",
    "        # First casting phase with raw observations to obtain clean observations\n",
    "        forecasting = forecast_equation(trend=trend, seasonality=seasonality,multiplicative_error=multiplicative_error,multiplicative_seasonality=multiplicative_seasonality,\n",
    "                                        y=y,m=m,alpha=alpha,beta=beta, lmbda = lmbda, gamma=gamma,ck=ck,k=k,integers=integers)\n",
    "        predicted = forecasting['predicted']\n",
    "        cleaned = forecasting['cleaned']\n",
    "        observed = forecasting['observed']\n",
    "        \n",
    "    if not seasonality and not trend:\n",
    "        initial_smoothing_parameters = get_optimized_parameters(y = y, m = m,trend=trend, seasonality=seasonality,multiplicative_error = multiplicative_error,\n",
    "                                                    multiplicative_seasonality=multiplicative_seasonality,ck=ck,k=k,\n",
    "                                                    method=method,integers=integers)\n",
    "        alpha = initial_smoothing_parameters['alpha']\n",
    "        beta = None\n",
    " \n",
    "        gamma = None\n",
    "        lmbda = initial_smoothing_parameters['lmbda']\n",
    "\n",
    "        opt_parameters = {'alpha':alpha,'beta':beta, 'gamma':gamma,'lmbda':lmbda}\n",
    "\n",
    "        # First casting phase with raw observations to obtain clean observations\n",
    "        forecasting = forecast_equation(trend=trend, seasonality=seasonality,multiplicative_error=multiplicative_error,multiplicative_seasonality=multiplicative_seasonality,\n",
    "                                        y=y,m=m,alpha=alpha,beta=beta,  lmbda = lmbda , gamma=gamma,ck=ck,k=k,integers=integers)\n",
    "        predicted = forecasting['predicted']\n",
    "        cleaned = forecasting['cleaned']\n",
    "        observed = forecasting['observed']\n",
    "\n",
    "    sigma = forecasting['sigma']\n",
    "    l = forecasting['l']\n",
    "    b = forecasting['b']\n",
    "    s = forecasting['s']\n",
    "    # Residuals are the difference between observed_1 and predicted_2 if error is additive\n",
    "    if not multiplicative_error:\n",
    "        residuals = observed - predicted\n",
    "        robust_likelihood = rob_lik_A(observed=observed,predicted=predicted,ck=ck,k=k)\n",
    "    # Residuals are the ratio between the difference  and predicted_2 if error is multiplicative\n",
    "    else:\n",
    "        residuals = get_multiplicative_error(observed=observed,predicted=predicted)\n",
    "        robust_likelihood = rob_lik_M(observed=observed,predicted=predicted,ck=ck,k=k)\n",
    "        \n",
    "    return {\"robust_likelihood\": robust_likelihood,\"observed\":observed,\"cleaned\":cleaned,\n",
    "            \"predicted\":predicted,\"residuals\":residuals,\"sigma\":sigma,\n",
    "            \"l\":l,\"b\":b,\"s\":s,'opt_parameters':opt_parameters}\n",
    "\n",
    "\n",
    "def get_rob_IC(y,m,trend,seasonality,multiplicative_error,multiplicative_seasonality,integers,method='Nelder-Mead',ck=4.685, k=3):\n",
    "    '''\n",
    "    Takes model specifications and runs an optimization process to obtain optimal parameters using the specified method\n",
    "    then it computes the robust likelihood based on the model with optimal parameters\n",
    "    from the obtained robust likelihood, it computes robust information criterion.\n",
    "    '''\n",
    "    robust_likelihood_model = fit_model(multiplicative_error = multiplicative_error,multiplicative_seasonality = multiplicative_seasonality,\n",
    "                                  y = y,m = m,ck = ck,k =k,trend=trend,seasonality=seasonality,integers=integers)\n",
    "    opt_parameters = robust_likelihood_model['opt_parameters']\n",
    "    robust_likelihood = robust_likelihood_model['robust_likelihood']\n",
    "    p = len([ i for i in opt_parameters.keys() if opt_parameters[i] != None])\n",
    "    n = len(y)\n",
    "    robAIC = -2 * robust_likelihood + 2*p\n",
    "    robBIC = -2 * robust_likelihood + math.log(n) * p\n",
    "    robAICc = -2 * robust_likelihood + 2 * ((p * n)/(n - p -1))\n",
    "    return {\"n_parameters\":p,\"n_observations\":n,'robust_likelihood':robust_likelihood,\"robAIC\":robAIC,\"robBIC\":robBIC,\"robAICc\":robAICc,\"opt_parameters\":opt_parameters,'method':method}\n",
    "\n",
    "\n",
    "def find_best_model(y,m,integers,method='Nelder-Mead',ck=4.685, k=3,IC='robAICc'):\n",
    "    '''\n",
    "    Takes into account the combinations of additive and multiplicative error and seasonalities.\n",
    "    obtain the robust Information criterion for each model\n",
    "    picks the model with the best (lowest) AICc as the best model and returns its information\n",
    "    '''\n",
    "    multiplicative_error_multiplicative_seasonality_trend = get_rob_IC(trend=True,seasonality=True,y =y,m =m,multiplicative_error=True,\n",
    "                                                                 multiplicative_seasonality=True , method=method,ck=ck,k=k,integers=integers)\n",
    "    multiplicative_error_additive_seasonality_trend = get_rob_IC(trend=True,seasonality=True,y =y,m =m,multiplicative_error=True,\n",
    "                                                           multiplicative_seasonality=False, method=method,ck=ck,k=k,integers=integers)\n",
    "    additive_error_multiplicative_seasonality_trend = get_rob_IC(trend=True,seasonality=True,y =y,m =m,multiplicative_error=False,\n",
    "                                                           multiplicative_seasonality=True, method=method,ck=ck,k=k,integers=integers)\n",
    "    additive_error_additive_seasonality_trend = get_rob_IC(trend=True,seasonality=True,y =y,m =m,multiplicative_error=False,\n",
    "                                                     multiplicative_seasonality=False ,method=method,ck=ck,k=k,integers=integers)\n",
    "    multiplicative_error_multiplicative_seasonality = get_rob_IC(trend=False,seasonality=True,y =y,m =m,multiplicative_error=True,\n",
    "                                                                multiplicative_seasonality=True , method=method,ck=ck,k=k,\n",
    "                                                                integers=integers)\n",
    "    multiplicative_error_additive_seasonality = get_rob_IC(trend=False,seasonality=True,y =y,m =m,multiplicative_error=True,\n",
    "                                                           multiplicative_seasonality=False, method=method,ck=ck,k=k,integers=integers)\n",
    "    additive_error_multiplicative_seasonality = get_rob_IC(trend=False,seasonality=True,y =y,m =m,multiplicative_error=False,\n",
    "                                                           multiplicative_seasonality=True, method=method,ck=ck,k=k,integers=integers)\n",
    "    additive_error_additive_seasonality = get_rob_IC(trend=False,seasonality=True,y =y,m =m,multiplicative_error=False,\n",
    "                                                     multiplicative_seasonality=False ,method=method,ck=ck,k=k,integers=integers)\n",
    "    \n",
    "    additive_error_trend = get_rob_IC(trend=True,seasonality=False,y =y,m =None,multiplicative_error=False,\n",
    "                                                     multiplicative_seasonality=False ,method=method,ck=ck,k=k,integers=integers)\n",
    "    multiplicative_error_trend = get_rob_IC(trend=True,seasonality=False,y =y,m =None,multiplicative_error=True,\n",
    "                                                     multiplicative_seasonality=False ,method=method,ck=ck,k=k,integers=integers)\n",
    "    additive_error = get_rob_IC(trend=False,seasonality=False,y =y,m =None,multiplicative_error=False,\n",
    "                                                     multiplicative_seasonality=False ,method=method,ck=ck,k=k,integers=integers)\n",
    "    mmultiplicative_error = get_rob_IC(trend=False,seasonality=False,y =y,m =None,multiplicative_error=True,\n",
    "                                                     multiplicative_seasonality=False ,method=method,ck=ck,k=k,integers=integers)\n",
    "    \n",
    "    compare_robAICc = {\"multiplicative_error_multiplicative_seasonality_trend\":multiplicative_error_multiplicative_seasonality_trend[IC],\n",
    "         \"multiplicative_error_additive_seasonality_trend\":multiplicative_error_additive_seasonality_trend[IC],\n",
    "         \"additive_error_multiplicative_seasonality_trend\":additive_error_multiplicative_seasonality_trend[IC],\n",
    "         \"additive_error_additive_seasonality_trend\":additive_error_additive_seasonality_trend[IC],\n",
    "         \"multiplicative_error_multiplicative_seasonality\":multiplicative_error_multiplicative_seasonality[IC],\n",
    "         \"multiplicative_error_additive_seasonality\":multiplicative_error_additive_seasonality[IC],\n",
    "         \"additive_error_multiplicative_seasonality\":additive_error_multiplicative_seasonality[IC],\n",
    "         \"additive_error_additive_seasonality\":additive_error_additive_seasonality[IC],\n",
    "         \"additive_error_trend\":additive_error_trend[IC],\n",
    "         \"multiplicative_error_trend\":multiplicative_error_trend[IC],\n",
    "         \"additive_error\":additive_error[IC],\n",
    "         \"mmultiplicative_error\":mmultiplicative_error[IC]}\n",
    "    \n",
    "    compare_model = {\"multiplicative_error_multiplicative_seasonality_trend\":multiplicative_error_multiplicative_seasonality_trend,\n",
    "         \"multiplicative_error_additive_seasonality_trend\":multiplicative_error_additive_seasonality_trend,\n",
    "         \"additive_error_multiplicative_seasonality_trend\":additive_error_multiplicative_seasonality_trend,\n",
    "         \"additive_error_additive_seasonality_trend\":additive_error_additive_seasonality_trend,\n",
    "         \"multiplicative_error_multiplicative_seasonality\":multiplicative_error_multiplicative_seasonality,\n",
    "         \"multiplicative_error_additive_seasonality\":multiplicative_error_additive_seasonality,\n",
    "         \"additive_error_multiplicative_seasonality\":additive_error_multiplicative_seasonality,\n",
    "         \"additive_error_additive_seasonality\":additive_error_additive_seasonality,\n",
    "         \"additive_error_trend\":additive_error_trend,\n",
    "         \"multiplicative_error_trend\":multiplicative_error_trend,\n",
    "         \"additive_error\":additive_error,\n",
    "         \"mmultiplicative_error\":mmultiplicative_error}\n",
    "    best_model_name = str(min(compare_robAICc,key=compare_robAICc.get))\n",
    "    best_model = compare_model[best_model_name]\n",
    "\n",
    "    if best_model_name.find(\"multiplicative_error\") == -1:\n",
    "        multiplicative_error = False\n",
    "    else:\n",
    "        multiplicative_error = True\n",
    "    \n",
    "    if best_model_name.find(\"multiplicative_seasonality\") == -1:\n",
    "        multiplicative_seasonality = False\n",
    "    else:\n",
    "        multiplicative_seasonality = True\n",
    "    if best_model_name.find(\"seasonality\") == -1:\n",
    "        seasonality = False\n",
    "    else:\n",
    "        seasonality = True\n",
    "\n",
    "    if best_model_name.find(\"trend\")== -1:\n",
    "        trend = False\n",
    "    else:\n",
    "        trend = True\n",
    "\n",
    "    optimum = best_model['opt_parameters']\n",
    "\n",
    "    robAIC = best_model['robAIC']\n",
    "    robBIC = best_model['robBIC']\n",
    "    robAICc = best_model['robAICc']\n",
    "    p = best_model['n_parameters']\n",
    "    n = best_model['n_observations']\n",
    "    robust_likelihood = best_model['robust_likelihood']\n",
    "\n",
    "\n",
    "    return {\"best_model\":best_model_name,'robust_likelihood':robust_likelihood,\n",
    "            \"n_parameters\":p,'n_observations':n,\"robAIC\":robAIC,\"robBIC\":robBIC,\n",
    "            \"robAICc\":robAICc,\"optimal_parameters\":optimum,\"method\":method,\n",
    "            'multiplicative_error':multiplicative_error,'multiplicative_seasonality':multiplicative_seasonality,\n",
    "            \"seasonality\":seasonality,\"trend\":trend}\n",
    "\n",
    "\n",
    "def get_confidence_interval(observed,predicted,sigma,multiplicative_error,a):\n",
    "    '''\n",
    "    A function that computes confidence interval around expected values.\n",
    "    Takes observed vector, predicted vector, sigma vector, a confidence level and \n",
    "    whether the error is multiplicative or additive.\n",
    "    Returns confidence level's information and anomalies information that are outside the \n",
    "    computed confidence interval\n",
    "    '''\n",
    "    epsilon = 1e-8\n",
    "    # Calculate the cumulative probability\n",
    "    cumulative_probability = 1 - a / 2\n",
    "    # Get the z-score corresponding to the cumulative probability\n",
    "    q = stats.norm.ppf(cumulative_probability)\n",
    "    prediction_interval = []\n",
    "    lower_bound = []\n",
    "    upper_bound = []\n",
    "    each_certainty = {}\n",
    "    anomalies_indecies = {}\n",
    "\n",
    "    if not multiplicative_error:\n",
    "        for i in range(len(observed)):\n",
    "            lower = predicted[i] - q * sigma[i]\n",
    "            upper = predicted[i] + q * sigma[i]\n",
    "            entry = [lower,upper]\n",
    "            prediction_interval.append(entry)\n",
    "            lower_bound.append(lower)\n",
    "            upper_bound.append(upper)\n",
    "            \n",
    "            if observed[i]>upper :\n",
    "                certainty = (abs(observed[i]-predicted[i])/(sigma[i]+epsilon))*10\n",
    "                each_certainty[i]=certainty\n",
    "                certainty = '( ' + str(round(certainty,0)) + '%' + ' )'\n",
    "                anomalies_indecies[i]=\"too high\" + certainty\n",
    "                \n",
    "            if observed[i]<lower:\n",
    "                certainty = (abs(observed[i]-predicted[i])/(sigma[i]+epsilon))*10\n",
    "                each_certainty[i]=certainty\n",
    "                certainty = '( ' + str(round(certainty,0)) + '%' + ' )'\n",
    "                anomalies_indecies[i]=\"too low\" + certainty\n",
    "    \n",
    "    else:\n",
    "        for i in range(len(observed)):\n",
    "            lower = predicted[i] * ( 1 - q * sigma[i])\n",
    "            upper = predicted[i] * ( 1 + q * sigma[i])\n",
    "            entry = [lower,upper]\n",
    "            prediction_interval.append(entry)\n",
    "            lower_bound.append(lower)\n",
    "            upper_bound.append(upper)\n",
    "            \n",
    "            if observed[i]>upper :\n",
    "                certainty = (abs((observed[i]-predicted[i])/(predicted[i]+epsilon))/(sigma[i]+epsilon))*10\n",
    "                each_certainty[i]=certainty\n",
    "                certainty = '( ' + str(round(certainty,0)) + '%' + ' )'\n",
    "                anomalies_indecies[i]=\"too high\" + certainty\n",
    "            if observed[i]<lower:\n",
    "                certainty = (abs((observed[i]-predicted[i])/(epsilon+predicted[i]))/(epsilon+sigma[i]))*10\n",
    "                each_certainty[i]=certainty\n",
    "                certainty = '( ' + str(round(certainty,0)) + '%' + ' )'\n",
    "                anomalies_indecies[i]=\"too low\" + certainty\n",
    "    return {'prediction_interval':prediction_interval,'lower_bound':lower_bound,'upper_bound':upper_bound,'anomalies_indecies':anomalies_indecies,'certainty':each_certainty}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_prediction_intervals(y,m,integers,method='Nelder-Mead',a = 0.05,ck=4.685, k=3,\n",
    "                             multiplicative_error = None,seasonality=None,trend = None,multiplicative_seasonality = None,IC='robAICc'):\n",
    "    '''\n",
    "    Finds the best model in terms of additive or multiplicative seasonality and error based on the AICc\n",
    "    takes predictions from that model\n",
    "    uses the error scale term to obtain a confidence interval around each prediction\n",
    "    returns the best model, a summary, and the prediction intervals\n",
    "    '''\n",
    "    if multiplicative_error != None or seasonality != None or trend != None or multiplicative_seasonality != None :\n",
    "        best_model = {\"multiplicative_error\":multiplicative_error,\"seasonality\":seasonality,\"trend\":trend,\n",
    "                      \"multiplicative_seasonality\":multiplicative_seasonality}\n",
    "        \n",
    "    else:\n",
    "        # Identifying the best model giving the data we have\n",
    "        best_model = find_best_model(y=y,m=m,method=method,ck=ck,k=k,IC=IC,integers=integers)\n",
    "        multiplicative_error = best_model['multiplicative_error']\n",
    "        seasonality = best_model['seasonality']\n",
    "        trend = best_model['trend']\n",
    "        multiplicative_seasonality = best_model['multiplicative_seasonality']\n",
    "        \n",
    "    if not seasonality:\n",
    "        m = None\n",
    "    # Fitting the best model\n",
    "    fitted_model = fit_model(multiplicative_error=multiplicative_error,multiplicative_seasonality = multiplicative_seasonality,\n",
    "                             y = y,m = m,ck=ck,k=k,trend=trend,seasonality=seasonality,integers=integers)\n",
    "    observed = fitted_model['observed']\n",
    "    predicted = fitted_model['predicted']\n",
    "    sigma = fitted_model['sigma']\n",
    "\n",
    "    confidence_interval = get_confidence_interval(observed=observed,predicted=predicted,sigma=sigma,multiplicative_error=multiplicative_error,a=a)\n",
    "    prediction_interval = confidence_interval['prediction_interval']\n",
    "    lower_bound = confidence_interval['lower_bound']\n",
    "    upper_bound = confidence_interval['upper_bound']\n",
    "    anomalies_indecies = confidence_interval['anomalies_indecies']\n",
    "    certainty = confidence_interval['certainty']\n",
    "\n",
    "    '''\n",
    "    # Calculate the cumulative probability\n",
    "    cumulative_probability = 1 - a / 2\n",
    "    # Get the z-score corresponding to the cumulative probability\n",
    "    q = stats.norm.ppf(cumulative_probability)\n",
    "    prediction_interval = []\n",
    "    lower_bound = []\n",
    "    upper_bound = []\n",
    "    anomalies_indecies = {}\n",
    "\n",
    "    if not multiplicative_error:\n",
    "        for i in range(len(observed)):\n",
    "            lower = predicted[i] - q * sigma[i]\n",
    "            upper = predicted[i] + q * sigma[i]\n",
    "            entry = [lower,upper]\n",
    "            prediction_interval.append(entry)\n",
    "            lower_bound.append(lower)\n",
    "            upper_bound.append(upper)\n",
    "            \n",
    "            if observed[i]>upper :\n",
    "                certainty = (abs(observed[i]-predicted[i])/sigma[i])*10\n",
    "                certainty = '( ' + str(round(certainty,0)) + '%' + ' )'\n",
    "                anomalies_indecies[i]=\"too high\" + certainty\n",
    "            if observed[i]<lower:\n",
    "                certainty = (abs(observed[i]-predicted[i])/sigma[i])*10\n",
    "                certainty = '( ' + str(round(certainty,0)) + '%' + ' )'\n",
    "                anomalies_indecies[i]=\"too low\" + certainty\n",
    "    \n",
    "    else:\n",
    "        for i in range(len(observed)):\n",
    "            lower = predicted[i] * ( 1 - q * sigma[i])\n",
    "            upper = predicted[i] * ( 1 + q * sigma[i])\n",
    "            entry = [lower,upper]\n",
    "            prediction_interval.append(entry)\n",
    "            lower_bound.append(lower)\n",
    "            upper_bound.append(upper)\n",
    "            \n",
    "            if observed[i]>upper :\n",
    "                certainty = (abs((observed[i]-predicted[i])/predicted[i])/sigma[i])*10\n",
    "                certainty = '( ' + str(round(certainty,0)) + '%' + ' )'\n",
    "                anomalies_indecies[i]=\"too high\" + certainty\n",
    "            if observed[i]<lower:\n",
    "                certainty = (abs((observed[i]-predicted[i])/predicted[i])/sigma[i])*10\n",
    "                certainty = '( ' + str(round(certainty,0)) + '%' + ' )'\n",
    "                anomalies_indecies[i]=\"too low\" + certainty\n",
    "    '''\n",
    "\n",
    "    return {'prediction_interval':prediction_interval,'lower_bound':lower_bound,\n",
    "            'upper_bound':upper_bound, 'best_model_summary':best_model,\n",
    "            'best_model_log':fitted_model,\"anomalies_indecies\":anomalies_indecies,'certainty':certainty}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot(interval,a = 0.05):\n",
    "    '''\n",
    "    Takes the prediction interval object, plots the observed data, the prediction interval and the anomalies\n",
    "    '''\n",
    "\n",
    "    multiplicative_error = interval['best_model_summary']['multiplicative_error']\n",
    "    observed = interval['best_model_log']['observed']\n",
    "    predicted = interval['best_model_log']['predicted']\n",
    "    sigma = interval['best_model_log']['sigma']\n",
    "\n",
    "    confidence_interval = get_confidence_interval(a=a,multiplicative_error=multiplicative_error, sigma=sigma,observed = observed,predicted = predicted)\n",
    "    lower_bound = confidence_interval['lower_bound']\n",
    "    upper_bound = confidence_interval['upper_bound']\n",
    "    anomalies_indecies = confidence_interval['anomalies_indecies']\n",
    "\n",
    "    '''\n",
    "    # Calculate the cumulative probability\n",
    "    cumulative_probability = 1 - a / 2\n",
    "    # Get the z-score corresponding to the cumulative probability\n",
    "    q = stats.norm.ppf(cumulative_probability)\n",
    "    prediction_interval = []\n",
    "    lower_bound = []\n",
    "    upper_bound = []\n",
    "    anomalies_indecies = []\n",
    "\n",
    "    if not multiplicative_error:\n",
    "        for i in range(len(observed)):\n",
    "            lower = predicted[i] - q * sigma[i]\n",
    "            upper = predicted[i] + q * sigma[i]\n",
    "            entry = [lower,upper]\n",
    "            prediction_interval.append(entry)\n",
    "            lower_bound.append(lower)\n",
    "            upper_bound.append(upper)\n",
    "            if observed[i]>upper or observed[i]<lower:\n",
    "                anomalies_indecies.append(i)\n",
    "    \n",
    "    else:\n",
    "        for i in range(len(observed)):\n",
    "            lower = predicted[i] * ( 1 - q * sigma[i])\n",
    "            upper = predicted[i] * ( 1 + q * sigma[i])\n",
    "            entry = [lower,upper]\n",
    "            prediction_interval.append(entry)\n",
    "            lower_bound.append(lower)\n",
    "            upper_bound.append(upper)\n",
    "            if observed[i]>upper or observed[i]<lower:\n",
    "                anomalies_indecies.append(i)'\n",
    "    '''\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'Observed': observed,\n",
    "        'Predicted': predicted,\n",
    "        'Lower Bound': lower_bound,\n",
    "        'Upper Bound': upper_bound})\n",
    "    anomaly_index = anomalies_indecies \n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(df['Observed'], label='Observed', color='blue')\n",
    "    plt.plot(df['Predicted'], label='Predicted', color='green', linestyle='--')\n",
    "\n",
    "    # Fill between for confidence interval\n",
    "    plt.fill_between(df.index, df['Lower Bound'], df['Upper Bound'], color='gray', alpha=0.3, label='Confidence Interval')\n",
    "\n",
    "    # Highlight the anomaly\n",
    "    # Note: Ensure your anomaly_index aligns with how your data's index is structured\n",
    "    plt.scatter(anomaly_index, df.iloc[anomaly_index]['Observed'], color='red', zorder=5, label='Anomaly')\n",
    "    #plt.scatter(anomaly_index, df.loc[anomaly_index,'Observed'], color='red', zorder=5, label='Anomaly')\n",
    "\n",
    "\n",
    "    plt.title('Time Series Forecast')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def get_bidirectional_anomalies(anomalies_dictionary_bidimentional_raw,anomalies_certainty_bidimentional_raw,n_small):\n",
    "    anomalies_indecies_numbers_raw = [i for i in anomalies_dictionary_bidimentional_raw.keys()]\n",
    "    anomalies_certainty_raw = [i for i in anomalies_certainty_bidimentional_raw.values()]\n",
    "    anomalies_indecies_numbers = []\n",
    "    anomalies_indecies_types = []\n",
    "    anomalies_indecies_certainty = []\n",
    "    anomalies_indecies_certainty_total = {}\n",
    "    anomalies_indecies_numbers_total = {}\n",
    "    patchy_skip = int()\n",
    "    for v in anomalies_indecies_numbers_raw:\n",
    "        if patchy_skip == v-1:\n",
    "            patchy_skip = v\n",
    "            continue\n",
    "        if v < n_small:\n",
    "            backward_pass_index = (n_small-1)+(n_small-v-1)\n",
    "            backward_pass_one_before = (n_small-1)+(n_small-(max(v-1,0))-1)\n",
    "            backward_pass_two_before = (n_small-1)+(n_small-(max(v-2,0))-1)\n",
    "            backward_pass_one_after = (n_small-1)+(n_small-(min(v+1,n_small))-1)\n",
    "\n",
    "\n",
    "            if (v+1) in anomalies_indecies_numbers_raw and   backward_pass_one_before in anomalies_indecies_numbers_raw and backward_pass_two_before in anomalies_indecies_numbers_raw and ((str(anomalies_dictionary_bidimentional_raw[v]).find('high') != 0 and str(anomalies_dictionary_bidimentional_raw[v+1]).find('high') != 0) or (str(anomalies_dictionary_bidimentional_raw[v]).find('low') != 0 and str(anomalies_dictionary_bidimentional_raw[v+1]).find('low') != 0)) :\n",
    "                anomalies_indecies_numbers.append(v)\n",
    "                current_certainty = anomalies_certainty_bidimentional_raw[v]\n",
    "                anomalies_indecies_certainty.append(current_certainty)\n",
    "\n",
    "                current_type = str(anomalies_dictionary_bidimentional_raw[v]) +'-patchy'\n",
    "                anomalies_indecies_types.append(current_type)\n",
    "                patchy_skip = v         \n",
    "\n",
    "                \n",
    "            elif  backward_pass_index in anomalies_indecies_numbers_raw:\n",
    "                anomalies_indecies_numbers.append(v)\n",
    "                current_certainty = anomalies_certainty_bidimentional_raw[v]\n",
    "                anomalies_indecies_certainty.append(current_certainty)\n",
    "\n",
    "                current_type = str(anomalies_dictionary_bidimentional_raw[v]) + '-isolated'\n",
    "                anomalies_indecies_types.append(current_type)\n",
    "\n",
    "    for i in range(len(anomalies_indecies_numbers)):\n",
    "        anomalies_indecies_numbers_total[anomalies_indecies_numbers[i]]=anomalies_indecies_types[i]\n",
    "        anomalies_indecies_certainty_total[anomalies_indecies_numbers[i]]=anomalies_indecies_certainty[i]\n",
    "\n",
    "    return {'anomalies_indecies_numbers_total':anomalies_indecies_numbers_total,'anomalies_indecies_numbers':anomalies_indecies_numbers,'anomalies_indecies_types':anomalies_indecies_types,'anomalies_indecies_certainty':anomalies_indecies_certainty,'anomalies_indecies_certainty_total':anomalies_indecies_certainty_total}\n",
    "\n",
    "def reset_interval(data,dates,invoice_ids,results,a=0.05):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import statsmodels.api as sm\n",
    "    import math\n",
    "    import statistics\n",
    "    from scipy.optimize import minimize\n",
    "    import scipy.stats as stats\n",
    "    import matplotlib.pyplot as plt\n",
    "    import warnings\n",
    "    import scipy.stats as stats \n",
    "    import pandas as pd\n",
    "    \n",
    "\n",
    "    multiplicative_error = results['multiplicative_error']\n",
    "\n",
    "    observed = results['observed_bidrectional']\n",
    "    predicted = results['predicted_bidrectional']\n",
    "    sigma = results['sigma_bidrectional']\n",
    "    n_small = results['n']\n",
    "\n",
    "\n",
    "    confidence_interval = get_confidence_interval(observed=observed,predicted=predicted,sigma=sigma,multiplicative_error=multiplicative_error,a=a)\n",
    "    prediction_interval = confidence_interval['prediction_interval']\n",
    "    lower_bound = confidence_interval['lower_bound']\n",
    "    upper_bound = confidence_interval['upper_bound']\n",
    "    anomalies_indecies = confidence_interval['anomalies_indecies']\n",
    "    anomalies_certainty = confidence_interval['certainty']\n",
    "\n",
    "\n",
    "    bidirectional_anomalies = get_bidirectional_anomalies(anomalies_dictionary_bidimentional_raw = anomalies_indecies,anomalies_certainty_bidimentional_raw =anomalies_certainty ,n_small=n_small)\n",
    "    anomalies_indecies_numbers = bidirectional_anomalies['anomalies_indecies_numbers']\n",
    "    anomalies_indecies_types = bidirectional_anomalies['anomalies_indecies_types']\n",
    "    anomalies_indecies_numbers_total = bidirectional_anomalies['anomalies_indecies_numbers_total']\n",
    "    anomalies_indecies_certainty = bidirectional_anomalies['anomalies_indecies_certainty']\n",
    "    anomalies_indecies_certainty_total = bidirectional_anomalies['anomalies_indecies_certainty_total']\n",
    "\n",
    "\n",
    "    anomalies_indecies_numbers = [i for i in anomalies_indecies_numbers_total.keys()]\n",
    "    anomalies_indecies_types = [i for i in anomalies_indecies_numbers_total.values()]\n",
    "    number_of_anomalies = len(anomalies_indecies_numbers)\n",
    "    anomalies_dates = dates[anomalies_indecies_numbers]\n",
    "    anomalies_observed_values = observed[anomalies_indecies_numbers]\n",
    "    anomalies_expected_values = predicted[anomalies_indecies_numbers]\n",
    "    anomalies_invoice_id = invoice_ids[anomalies_indecies_numbers]\n",
    "    anomalies_certainty = [i for i in anomalies_indecies_certainty_total.values()]\n",
    "    out = {'anomalies_indecies_numbers':anomalies_indecies_numbers,'anomalies_indecies_types':anomalies_indecies_types,'number_of_anomalies':number_of_anomalies,'anomalies_dates':anomalies_dates,'anomalies_observed_values':anomalies_observed_values,'anomalies_expected_values':anomalies_expected_values,'anomalies_invoice_id':anomalies_invoice_id,'anomalies_certainty':anomalies_certainty}\n",
    "    return out\n",
    "\n",
    "\n",
    "def exponential_smoothing_nonrobust(data,trend='add', seasonal=None,seasonality=False, multiplicative_seasonality = False, multiplicative_error = True, integers = False, m = None):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import statsmodels.api as sm\n",
    "    from statsmodels.tsa.api import ExponentialSmoothing\n",
    "    import math\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    import statistics\n",
    "\n",
    "\n",
    "    # Sample data: Monthly totals of international airline passengers, 1949 to 1960.\n",
    "    #data = sm.datasets.get_rdataset(\"AirPassengers\").data['value']\n",
    "\n",
    "    # Import the complete simulation data\n",
    "    #path_dummy = \"/Workspace/Users/ashraf.zedane@nl.abnamro.com/data_simulation/History/passengers.csv\"\n",
    "    #data_dummy = pd.read_csv(path_dummy)\n",
    "\n",
    "    #data = np.array(data_dummy).reshape((144,))\n",
    "\n",
    "    time_series = data\n",
    "    time_series_reversed = time_series[::-1] \n",
    "    n_small = time_series.shape[0]\n",
    "    bidirectional_array = np.concatenate((time_series, time_series_reversed[1:]), axis=None)\n",
    "    bidirectional_array_midpoint = bidirectional_array[n_small-1] # it is also the last point of the forward pass\n",
    "\n",
    "    model = ExponentialSmoothing(bidirectional_array, trend=trend, seasonal=seasonal)\n",
    "    fitted_model = model.fit()\n",
    "    time_series = data\n",
    "    time_series_reversed = time_series[::-1] \n",
    "    n_small = time_series.shape[0]\n",
    "    bidirectional_array = np.concatenate((time_series, time_series_reversed[1:]), axis=None)\n",
    "    bidirectional_array_midpoint = bidirectional_array[n_small-1] # it is also the last point of the forward pass\n",
    "\n",
    "    l_0 = fitted_model.params['initial_level']\n",
    "    b_0 = fitted_model.params['initial_trend']\n",
    "    startup_period = get_startup_period(y=bidirectional_array,seasonality = seasonality,m=m)\n",
    "    sigma = np.zeros((len(bidirectional_array),))\n",
    "    sigma_0 = get_sigma_0(y=bidirectional_array,b_0_hat=b_0,l_0_hat=l_0,multiplicative_error=multiplicative_error,multiplicative_seasonality=multiplicative_seasonality,startup_period=startup_period,seasonality = seasonality,m=m,integers=False)\n",
    "    sigma_first = get_sigma_hat(observed = bidirectional_array[0],predicted=fitted_model.fittedvalues[0],previous_sigma=sigma_0,integers=integers,lmbda=0.5,multiplicative_error=multiplicative_error,ck = 4.685,k = 3)\n",
    "    sigma[0] = sigma_first \n",
    "    for i in range(1,len(bidirectional_array)):\n",
    "        sigma_value = get_sigma_hat(observed = bidirectional_array[i],predicted=fitted_model.fittedvalues[i],previous_sigma=sigma[i-1],integers=integers,lmbda=0.9,multiplicative_error=multiplicative_error,ck = 4.685,k = 3)\n",
    "        sigma[i] = sigma_value\n",
    "    confidence_interval = get_confidence_interval(observed=bidirectional_array,predicted=fitted_model.fittedvalues,sigma=sigma,multiplicative_error=multiplicative_error,a=0.0005)\n",
    "    n_small = len(data)\n",
    "\n",
    "    get_bidirectional_anomalies(anomalies_dictionary_bidimentional_raw=confidence_interval['anomalies_indecies'],anomalies_certainty_bidimentional_raw=confidence_interval['certainty'],n_small=n_small).keys()\n",
    "    results = {'multiplicative_error':multiplicative_error,\n",
    "            'observed_bidrectional':bidirectional_array,\n",
    "            'predicted_bidrectional':fitted_model.fittedvalues,\n",
    "            'sigma_bidrectional':sigma,\n",
    "            'n':n_small}\n",
    "    return results\n",
    "\n",
    "\n",
    "def detect_anomalies_from_dataframe(data,target_column = 'Invoice Spend',date_column = 'Date',default_m =1,default_alpha = 0.05,column_to_exclude = ['Date'], path_dump_pkl = '/Workspace/Users/ashraf.zedane@nl.abnamro.com/data_simulation/Reduced_data/v_1/analysis_outcome.pkl', path_anomaly = \"/Workspace/Users/ashraf.zedane@nl.abnamro.com/data_simulation/Reduced_data/v_1/anomalies_log.xlsx\"):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import statsmodels.api as sm\n",
    "    import math\n",
    "    import statistics\n",
    "    from scipy.optimize import minimize\n",
    "    import scipy.stats as stats\n",
    "    import matplotlib.pyplot as plt\n",
    "    import warnings\n",
    "\n",
    "    #if type(self.data) != type(pd.DataFrame()):\n",
    "    #    raise ValueError('This method supports only dataframes')\n",
    "    import pickle\n",
    "    # To keep track of the fitting progress\n",
    "    total_number_of_models = 0\n",
    "    number_of_models = 0\n",
    "    number_of_failed_models = 0\n",
    "    # The dictionary to save everything\n",
    "    analysis_log = {}\n",
    "\n",
    "    # Generating a performance metric summary\n",
    "    anomalies_log = pd.read_excel(path_anomaly)\n",
    "\n",
    "    # Extracting columns info\n",
    "    numeric_columns = [i for i in  data.select_dtypes(include='number').columns]\n",
    "    all_columns = [i for i in  data.columns]\n",
    "    other_columns = []\n",
    "    for i in all_columns:\n",
    "        if i not in numeric_columns and i not in column_to_exclude:\n",
    "            other_columns.append(i)\n",
    "\n",
    "    # Getting the total number of columns to be fitted\n",
    "    for categorical_column in other_columns:\n",
    "        if categorical_column == 'Department (D3)':\n",
    "            analysis_log[categorical_column] = {}\n",
    "            unique_categories = set( data[categorical_column])\n",
    "            unique_categories = list(unique_categories)\n",
    "            for category in unique_categories:\n",
    "                if category not in [j for j in set([i for i in  anomalies_log['department_D3']])]: \n",
    "                    continue\n",
    "                if ('patchy higher' in [i for i in  anomalies_log['anomaly type']]) or  ('patchy lower' in [i for i in  anomalies_log['anomaly type']]) or  ('higher value' in [i for i in  anomalies_log['anomaly type']]) or ('lower value' in [i for i in  anomalies_log['anomaly type']]) or ('extremely higher value' in [i for i in  anomalies_log['anomaly type']]):\n",
    "                    total_number_of_models = total_number_of_models +1\n",
    "\n",
    "                if ('lower frequency' in [i for i in  anomalies_log['anomaly type']]) or ('higher frequency' in [i for i in  anomalies_log['anomaly type']]) :   \n",
    "                    time_frames = ['W','M','Q','Y']\n",
    "                    value_frequency = ['sum','count']\n",
    "                    for time_frame in time_frames:\n",
    "                        for count_sum in value_frequency:\n",
    "                            total_number_of_models = total_number_of_models + 1\n",
    "\n",
    "    # Fitting the actual models\n",
    "    for categorical_column in other_columns:\n",
    "        if categorical_column == 'Department (D3)':\n",
    "            analysis_log[categorical_column] = {}\n",
    "            unique_categories = set( data[categorical_column])\n",
    "            unique_categories = list(unique_categories)\n",
    "\n",
    "            for category in unique_categories:\n",
    "                if category not in [j for j in set([i for i in  anomalies_log['department_D3']])]:\n",
    "                    continue\n",
    "                else:\n",
    "                    print('Analyzing: ',category)\n",
    "                    analysis_log[categorical_column][category]={}\n",
    "                    filtered_department = data[data[categorical_column]==category][[target_column,date_column]]\n",
    "                    invoice_ids = data[data[categorical_column]==category]['InvoiceID']\n",
    "                    filtered_department[date_column] = pd.to_datetime(filtered_department[date_column])\n",
    "                    # Fitting the invoices sequence model\n",
    "                    analysis_log[categorical_column][category]['sequenced']={target_column:np.array(filtered_department[target_column]),\n",
    "                                                                            date_column:np.array(filtered_department[date_column]),\n",
    "                                                                            'ids':np.array(invoice_ids),\n",
    "                                                                            'model_sequenced':None}\n",
    "                    object_sequenced = 0\n",
    "                    number_of_models = number_of_models +1\n",
    "                    print('So far fitted ',round((number_of_models/total_number_of_models)*100,2),\"% of total models\")\n",
    "\n",
    "                    #try:\n",
    "                    bidirectional_info = exponential_smoothing_nonrobust(data = np.array(filtered_department[target_column]),trend='add', seasonal=None,seasonality=False, multiplicative_seasonality = False, multiplicative_error = True, integers = False, m = None)\n",
    "                    bidirectional_anomalies = reset_interval(data=data,dates = np.array(filtered_department[date_column]),a=0.0005,results=bidirectional_info,invoice_ids = np.array(invoice_ids))\n",
    "\n",
    "                    #except:\n",
    "                    #print('A model could not be fitted')\n",
    "                    #number_of_failed_models = number_of_failed_models + 1\n",
    "\n",
    "                    analysis_log[categorical_column][category]['sequenced']['bidirectional_anomalies'] = bidirectional_anomalies\n",
    "                    analysis_log[categorical_column][category]['sequenced']['bidirectional_info'] = bidirectional_info\n",
    "                    analysis_log[categorical_column][category]['sequenced']['data'] = np.array(filtered_department[target_column])\n",
    "                    analysis_log[categorical_column][category]['sequenced']['dates'] = np.array(filtered_department[date_column])\n",
    "                    analysis_log[categorical_column][category]['sequenced']['invoice_ids'] = np.array(invoice_ids)\n",
    "\n",
    "\n",
    "    analysis_log['model numbers']={'total_number_of_models':total_number_of_models, 'number_of_failed_models':number_of_failed_models }\n",
    "    \n",
    "    # Save everything in a dictionary file pkl\n",
    "    # to transfer the results of the analysis from RAM to a file.\n",
    "    #with open(path_dump_pkl, 'wb') as file:\n",
    "        # Use pickle.dump to save the dictionary to the file\n",
    "    #    pickle.dump(analysis_log, file)\n",
    "    #print('A pkl version of the analysis_log was dumped in: ','\\n',\n",
    "    #    path_dump_pkl)        \n",
    "\n",
    "\n",
    "    # Possible values of a (default is 0.05)\n",
    "    a_values = [1/10**i for i in range(50)]\n",
    "    a_values[-1]=0\n",
    "    resulting = {}\n",
    "    epsilon = 1e-6\n",
    "    for a in a_values:\n",
    "        if a == 1:\n",
    "            resulting[a]= {'true_anomalies_detected':40,\n",
    "                            'true_anomalies_undetected':0,\n",
    "                            'total_inserted_anomalies':40,\n",
    "                            'total_detected_anomalies':100,\n",
    "                            'detection_ratio':1,\n",
    "                            'false_positive_rate':0.99}\n",
    "            continue\n",
    "\n",
    "        true_anomalies_detected = 0\n",
    "        true_anomalies_undetected = 0\n",
    "        total_inserted_anomalies = 0\n",
    "        total_detected_anomalies = 0\n",
    "        if len([j for j in set([i for i in anomalies_log['department_D3']])]) != 0:\n",
    "            for department_d3 in [j for j in set([i for i in anomalies_log['department_D3']])]:\n",
    "                if [i for i in anomalies_log[anomalies_log['department_D3']==department_d3]['anomaly type']][0] == 'aggregate increase':\n",
    "                    continue\n",
    "                else:\n",
    "                    print(analysis_log['Department (D3)'][department_d3]['sequenced']['bidirectional_anomalies'])\n",
    "                    if analysis_log['Department (D3)'][department_d3]['sequenced']['bidirectional_anomalies']['number_of_anomalies'] != 0:\n",
    "                        bidirectional_info = analysis_log['Department (D3)'][department_d3]['sequenced']['bidirectional_info']\n",
    "                        dates = analysis_log['Department (D3)'][department_d3]['sequenced']['dates']\n",
    "                        data = analysis_log['Department (D3)'][department_d3]['sequenced']['data']\n",
    "                        invoice_ids = analysis_log['Department (D3)'][department_d3]['sequenced']['invoice_ids']\n",
    "\n",
    "                        new_results = reset_interval(data=data,dates = dates,a=a,results=bidirectional_info,invoice_ids = invoice_ids)\n",
    "\n",
    "                        anomalies_indecies_numbers = new_results['anomalies_indecies_numbers']\n",
    "\n",
    "                        anomalies_invoice_id = new_results['anomalies_invoice_id']\n",
    "\n",
    "                        total_inserted_anomalies = total_inserted_anomalies + anomalies_log[anomalies_log['department_D3']==department_d3].shape[0] \n",
    "                        total_detected_anomalies = total_detected_anomalies + len(anomalies_invoice_id)\n",
    "                        for inserted_anomaly in [i for i in anomalies_log[anomalies_log['department_D3']==department_d3]['invoice_ID']]:\n",
    "                            if inserted_anomaly in anomalies_invoice_id:\n",
    "                                true_anomalies_detected = true_anomalies_detected + 1 \n",
    "                            else:\n",
    "                                true_anomalies_undetected = true_anomalies_undetected + 1 \n",
    "                        falsely_detected_anomalies = total_detected_anomalies - true_anomalies_detected\n",
    "                        false_positive_rate = falsely_detected_anomalies/(total_detected_anomalies +epsilon )\n",
    "        resulting[a]= {'true_anomalies_detected':true_anomalies_detected,\n",
    "                            'true_anomalies_undetected':true_anomalies_undetected,\n",
    "                            'total_inserted_anomalies':total_inserted_anomalies,\n",
    "                            'total_detected_anomalies':total_detected_anomalies,\n",
    "                            'detection_ratio':true_anomalies_detected/(epsilon + total_inserted_anomalies),\n",
    "                            'false_positive_rate':false_positive_rate}\n",
    "\n",
    "    out = {'analysis_log':analysis_log,'anomalies_log':anomalies_log,'resulting':resulting}\n",
    "\n",
    "    # Save everything in a dictionary file pkl\n",
    "    # to transfer the results of the analysis from RAM to a file.\n",
    "    #with open(path_dump_pkl, 'wb') as file:\n",
    "        # Use pickle.dump to save the dictionary to the file\n",
    "    #    pickle.dump(out, file)\n",
    "    #print('A pkl version of the results was dumped in: ','\\n',\n",
    "    #    path_dump_pkl)\n",
    "    return out\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95ed19e0-3d5d-43f7-84e9-04d1713c85aa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Implementing the non-robust algorithm on the simulated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22c97b5a-9964-4316-8bad-ee6375b94ce6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "seeds = [i for i in range(1,10)]\n",
    "path_anomaly = \"/Workspace/Users/ashraf.zedane@nl.abnamro.com/data_simulation/Reduced_data/v_1/anomalies_log.xlsx\"\n",
    "path_dummy = \"/Workspace/Users/ashraf.zedane@nl.abnamro.com/data_simulation/Reduced_data/v_1/dummy.xlsx\"\n",
    "replication_outcome_path = '/Workspace/Users/ashraf.zedane@nl.abnamro.com/data_simulation/Reduced_data/v_1/replication_outcome.pkl'\n",
    "replication_outcome_path_temp = '/Workspace/Users/ashraf.zedane@nl.abnamro.com/data_simulation/Reduced_data/v_1/replication_outcome_temp.pkl'\n",
    "replication_outcome = {}\n",
    "for i in seeds:\n",
    "    simulate_data(seed = seeds,\n",
    "                Commodities_L1_number = 1,\n",
    "                    Departments_L1_number = 2,\n",
    "                    Account_level_number = 1,\n",
    "                    Suppliers_number = 2,\n",
    "                    start_year = 2022,\n",
    "                    end_year = 2023,\n",
    "                    number_of_anomalies = 40,\n",
    "                    path = '/Workspace/Users/ashraf.zedane@nl.abnamro.com/data_simulation/Reduced_data/v_1/',\n",
    "                    data_name_str = 'dummy.xlsx',\n",
    "                    anomalies_log_str = 'anomalies_log.xlsx',\n",
    "                    mu = [100000,300000],sdv = [50000,100000],\n",
    "                    rtrend = [5000,25000])\n",
    "\n",
    "    anomalies_dummy = pd.read_excel(path_anomaly)\n",
    "    # Import the complete simulation data\n",
    "    \n",
    "    data_dummy = pd.read_excel(path_dummy)\n",
    "    df = data_dummy\n",
    "\n",
    "\n",
    "\n",
    "    outcome = detect_anomalies_from_dataframe(data=df,target_column = 'Invoice Spend',date_column = 'Date',default_m =1,default_alpha = 0.05,column_to_exclude = ['Date'], path_dump_pkl = replication_outcome_path, path_anomaly = path_anomaly)\n",
    "    resulting = outcome['resulting']\n",
    "    replication_outcome[i] = resulting\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c39404c-cd8f-4dbc-a14c-9799faf7c32e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Save everything in a dictionary file pkl\n",
    "# to transfer the results of the analysis from RAM to a file.\n",
    "with open('/Workspace/Users/ashraf.zedane@nl.abnamro.com/data_simulation/Reduced_data/v_1/replication_outcome_comp.pkl', 'wb') as file:\n",
    "    # Use pickle.dump to save the dictionary to the file\n",
    "    pickle.dump(replication_outcome, file)\n",
    "print('A pkl version of the replication_outcome was dumped in: ','\\n',\n",
    "    replication_outcome_path)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a044c503-e0b2-449f-87cf-e59032d1669c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Open the file for reading in binary mode\n",
    "with open('/Workspace/Users/ashraf.zedane@nl.abnamro.com/data_simulation/Reduced_data/v_1/replication_outcome_comp.pkl', 'rb') as file:\n",
    "    # Use pickle.load to load the dictionary from the file\n",
    "    loaded_replication_outcome_nonrobust = pickle.load(file)\n",
    "\n",
    "alphas = [i for i in loaded_replication_outcome_nonrobust[1].keys()]\n",
    "detection_nonrobust = []\n",
    "false_positive_nonrobust = []\n",
    "for alpha in alphas:\n",
    "    detection_nonrobust_ratio_temp= []\n",
    "    false_positive_nonrobust_rate_temp = []\n",
    "    for i in loaded_replication_outcome_nonrobust.keys():\n",
    "        detection_nonrobust_ratio_temp.append(loaded_replication_outcome_nonrobust[i][alpha]['detection_ratio'])\n",
    "        false_positive_nonrobust_rate_temp.append(loaded_replication_outcome_nonrobust[i][alpha]['false_positive_rate'])\n",
    "    detection_nonrobust_ratio = np.mean(detection_nonrobust_ratio_temp)\n",
    "    false_positive_nonrobust_rate = np.mean(false_positive_nonrobust_rate_temp)\n",
    "    detection_nonrobust.append(detection_nonrobust_ratio)\n",
    "    false_positive_nonrobust.append(false_positive_nonrobust_rate)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f485de8-3a65-4f6f-b2b2-d0d38de7c090",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Open the file for reading in binary mode\n",
    "with open('/Workspace/Users/ashraf.zedane@nl.abnamro.com/data_simulation/Reduced_data/v_1/replication_outcome.pkl', 'rb') as file:\n",
    "    # Use pickle.load to load the dictionary from the file\n",
    "    loaded_replication_outcome = pickle.load(file)\n",
    "\n",
    "alphas = [i for i in loaded_replication_outcome[1].keys()]\n",
    "detection = []\n",
    "false_positive = []\n",
    "for alpha in alphas:\n",
    "    detection_ratio_temp= []\n",
    "    false_positive_rate_temp = []\n",
    "    for i in loaded_replication_outcome.keys():\n",
    "        detection_ratio_temp.append(loaded_replication_outcome[i][alpha]['detection_ratio'])\n",
    "        false_positive_rate_temp.append(loaded_replication_outcome[i][alpha]['false_positive_rate'])\n",
    "    detection_ratio = np.mean(detection_ratio_temp)\n",
    "    false_positive_rate = np.mean(false_positive_rate_temp)\n",
    "    detection.append(detection_ratio)\n",
    "    false_positive.append(false_positive_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8063dd40-cc1f-435c-9c00-2280b26b6525",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Inspecting the results and showing comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63150ca0-d9ec-4f7f-bfe5-444cedcc7929",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example data\n",
    "fpr1 = false_positive_nonrobust\n",
    "tpr1 = detection_nonrobust\n",
    "alpha1 = alphas\n",
    "\n",
    "fpr2 = false_positive\n",
    "tpr2 = detection\n",
    "alpha2 = alphas\n",
    "\n",
    "# Plotting the ROC curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr1, tpr1, marker='o', linestyle='-', label='Exponential nonrobust')\n",
    "plt.plot(fpr2, tpr2, marker='o', linestyle='-', label='exponential robust')\n",
    "for i ,alpha in enumerate(alpha1):\n",
    "    plt.annotate(f\"{alpha}\",(fpr1[i],tpr1[i]),textcoords = 'offset points',xytext=(0,10),ha ='center',fontsize=6)\n",
    "\n",
    "for i ,alpha in enumerate(alpha2):\n",
    "    plt.annotate(f\"{alpha}\",(fpr2[i],tpr2[i]),textcoords = 'offset points',xytext=(0,10),ha ='center',fontsize=6)\n",
    "\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve Comparison')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "# Display the plot\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Complete_simulation_algorithm_implementation_and_comparison",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
